[{"title":"README","url":"/2022/06/01/README/","content":"\n\n\n\n\n\n&lt;打工是不可能打工的，这辈子都不可能打工的&gt;\n\n\n\n打工是不可能打工的，这辈子都不可能打工的\n打工是不可能打工的，这辈子都不可能打工的\n\n\n\n\n","categories":["README"],"tags":["README"]},{"title":"README","url":"/2022/04/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/Yolo-V1/","content":"\n打工是不可能打工的，这辈子都不可能打工的 打工是不可能打工的，这辈子都不可能打工的 打工是不可能打工的，这辈子都不可能打工的\n\n\nREADME\n","categories":["README"],"tags":["README"]},{"title":"DCGAN：DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DCGAN/","content":"\nDCGAN\n\n\nDCGAN\n","categories":["README"],"tags":["DCGAN"]},{"title":"Fully Convolutional Networks for Semantic Segmentation","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/FCN/","content":"\nFully Convolutional Networks for Semantic Segmentation\n\n\nFully Convolutional Networks for Semantic Segmentation\n","categories":["README"],"tags":["README"]},{"title":"GAN","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GAN/","content":"\nGAN\n\n\nGAN\n","categories":["README"],"tags":["README"]},{"title":"GPT：Generative Pre-Training","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GPT-1/","content":"\nGPT\n\n\nGPT-1\nGPT-2\nGPT-3\n","categories":["README"],"tags":["README"]},{"title":"DenseNet","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DenseNet/","content":"\nDenseNet\n\n\n\n","categories":["README"],"tags":["README"]},{"title":"GPT：Generative Pre-Training","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GPT-2/","content":"\nGPT\n\n\nGPT-1\nGPT-2\nGPT-3\n","categories":["README"],"tags":["README"]},{"title":"GoogLeNet","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GoogLeNet/","content":"\nGoogLeNet\n\n\nGoogLeNet\n","categories":["README"],"tags":["README"]},{"title":"Knowledge Distilling","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Knowledge%20Distilling%20/","content":"\nKnowledge Distilling\n\n\nKnowledge Distilling\n","categories":["README"],"tags":["Knowledge Distilling"]},{"title":"Rich feature hierarchies for accurate object detection and semantic segmentation","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/R-CNN/","content":"\nKnowledge Distilling\n\n\nKnowledge Distilling\n","categories":["README"],"tags":["Knowledge Distilling"]},{"title":"Self-Attention Generative Adversarial Networks","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/SAGAN/","content":"\nSelf-Attention Generative Adversarial Networks\n\n\nSelf-Attention Generative Adversarial Networks\n","categories":["README"],"tags":["Self-Attention Generative Adversarial Networks"]},{"title":"GPT：Generative Pre-Training","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/GPT-3/","content":"\nGPT\n\n\nGPT-1\nGPT-2\nGPT-3\n","categories":["README"],"tags":["README"]},{"title":"Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Switch%20Transformers/","content":"\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n\n\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n","categories":["README"],"tags":["[object Object]"]},{"title":"Transformers are RNNs","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Transformers%20are%20RNNs/","content":"\nTransformers are RNNs\n\n\nTransformers are RNNs\n","categories":["README"],"tags":["Transformers are RNNs"]},{"title":"YOLO9000 - Better, Faster, Stronger","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/YOLO%20v2/","content":"\nYOLO9000: Better, Faster, Stronger\n\n\nYOLO9000: Better, Faster, Stronger\n","categories":["README"],"tags":["[object Object]"]},{"title":"Yolo v1","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/YOLO%20v1/","content":"\nYolo v1\n\n\nYolo-1\n","categories":["README"],"tags":["Yolo v1"]},{"title":"YOLOv3 - An Incremental Improvement","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/YOLO%20v3/","content":"\nYOLOv3: An Incremental Improvement\n\n\nYOLOv3: An Incremental Improvement\n","categories":["README"],"tags":["[object Object]"]},{"title":"YOLOv4 Optimal Speed and Accuracy of Object Detection","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/YOLO%20v4/","content":"\nYOLOv4: Optimal Speed and Accuracy of Object Detection\n\n\nYOLOv4: Optimal Speed and Accuracy of Object Detection\n","categories":["README"],"tags":["[object Object]"]},{"title":"A DEEP LEARNING OBJECT DETECTION METHOD FOR AN EFFICIENT C LUSTERS INITIALIZATION","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/YOLO%20v5/","content":"\nA DEEP LEARNING OBJECT DETECTION METHOD FOR AN EFFICIENT C LUSTERS INITIALIZATION\n\n\nA DEEP LEARNING OBJECT DETECTION METHOD FOR AN EFFICIENT C LUSTERS INITIALIZATION\n","categories":["README"],"tags":["A DEEP LEARNING OBJECT DETECTION METHOD FOR AN EFFICIENT C LUSTERS INITIALIZATION"]},{"title":"Attention Is All You Need","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/YOLO%20v5%20copy/","content":"\nAttention Is All You Need\n\n\n\n","categories":["README"],"tags":["Attention Is All You Need"]},{"title":"Vision transformer","url":"/2022/06/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Vision%20transformer/","content":"\nVision transformer\n\n\nVision transformer\n这玩意是大CNN\n","categories":["README"],"tags":["Vision transformer"]},{"url":"/2022/05/06/Deep%20Learning/Confronting%20the%20Partition%20Function/","content":"\n"},{"title":"GAN","url":"/2022/05/06/Deep%20Learning/Deep%20Generative%20%20Models/","content":"\n\n硬核程度：★★★☆☆☆\n在很多任务中，我们关心某些概率分布并非因为对这些概率分布本身感兴趣，精确推断方法通常需要很大的计算开销, 因此在现实应用中近似推断方法更为常用. 近似推断方法大致可分为两大类: 第一类是采样(sampling)，通过使用随机化方法完成近似; 第二类是使用确定性近似完成近似推断，典型代表为变分推断(variational inference).\n\n\n2014 年, Goodfellow 等提出了一种新的生成式模型, 命名为生成对抗网络 (generative adversarial nets, GAN)  。由于其具有创新性的思想和出色的实验效果, 在机器学习领域迅速刮起了一阵旋风, 大大推动了生成模型研究的发展, 并傕生了很多全新的应用。 生成对抗网络的目标是让网络能够产生估与训练样本具有相同特性的新样本，例如用一个手写数字图片的数据库训练生成对抗网络，让它能产生出新的类似手写数字的图片; 用一个人脸图像的数据库训练生成对抗网络，希望它能产生出与训练样本具有类似风格但不同于训练样本中任何实例的人脸图像，即产生现实世界中并不存在的人的“肖像”。\n生成对抗网络包含丛个神经网络，一个是生成器 (generator)，另一个是判别器 (discriminator)，通过两个网络的博变实现让生成器学会生成新样本的目标。其中, 生成器的任务是在一定的隐变量控制下生成新样本，判别器的任务是对真实训练样本和生成器生 成的“假样本”进行判别。所诎“对抗”, 就是指生成对抗网络在训练过程中, 一方面训练判别器，使之尽可能准确地区分真样本和假样本; 另一方面训练生成器，使之产生的假样本尽量不会被判别器识别出来。通过对这两个格互矛盾的目标交替优化，最终使生成器生成的样本能以假乱真。 图 12-30 示意了一个生成对抗网络的基本结构。其中, 生成器神经网终记作 , 其 中  是网络的隐变量, 任给一个随机向量  生成一个样本,  的先验概率密度为  。真实的训练样本记作 , 它服从概率密度函数  。判别器神经网终记为 , 它以真实样本  或生成样本  为输人, 而输林端通过一个 Sigmoid 函数判断输入 为真实样本 (1) 还是生成样本 (0)。\n\n网绛  和  中的参数都要从数据中学习, 学习的目标是;  &gt;Discriminator outputs likelihood in (0,1) of real image &gt; &gt;Discriminator output for real data x\n\nAlternate between:\n\nGradient ascent on discriminator\n\n\n\nGradient descent on generator\n\n\n\n即，对于判别器来说要使该目标函数最大化，对生成器来说要使判别函数最小化。 我们来分析一下这个目标函数的最优解情况。对固定的生成器，式中的目标函数可以写成  \n\n在  给定的前提下， 与  都可以看作是常数，这样可对  直接求导，即：\n\n对判别器求  最大，最优解需满足  可得最优判别器  为\n\n证明了其可以作为概率密度函数！！！！！\n\n 对于固定的判别器，需要对生成器求  最小。代入 得 $$\n\n进一步推导可得 V(D^{}, G)=-(4)+(p_{} | )+(p_{g} | ) 即 V(D^{}, G)=-(4)+2 (p_{} | p_{g}) $$ 其中， 是  和  的 JS 散度 (Jensen-Shannon divergence)，它是两个概率密度函数以各自为基准的 KL 散度的平均。JS 散度是对称的，也称作对称 KL 散度。 可见, 对生成器最小化  就是最小化  和  的差异，即最小化 ，其最优解是 ，即生成器的概率密度函数与数据的楖率密度函数相同,  。 如何通过用样本训练生成对抗网络求得最优解? 在 GAN 最早的文章中, 作渚提出了 下面的分批随机梯度下降训练算法, 并且证明了如果生成器和判别器具有走㿟的能力 (容 量) 且在算法每一步均寻求给定模型下的最优, 则算法收敛于（唯一的）最优解。\n\nProof. Consider  as a function of  as done in the above criterion. Note that  is convex in . The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained. In other words, if   and  is convex in  for every ，then  if . This is equivalent to computing a gradient descent update for  at the optimal  given the corresponding  is convex in  with a unique global optima as proven in Thm 1 , therefore with sufficiently small updates of  converges to , concluding the proof.\nIn practice, adversarial nets represent a limited family of  distributions via the function , and we optimize  rather than  itself, so the proofs do not apply. However, the excellent performance of multilayer perceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees.\n\nGAN是凸函数证明！！！！！\n\n\n就需要一个度量概率分布的指标一散度。我们熟知的散度有 KL 散度公式如下 （KL 散度有一个缺点就是距离不对称性, 所以说它不是传统真正意义上的距离度量方法）。  和 JS 散度公式如下（JS 散度是改进了 KL 散度的距离不对称性）:  但是其实能将所有我们熟知散度归结到一个大类那就是  - 散度, 具体的定义如 下所示： 定义1：设 和 是 上的两个概率密度函数。则 和 的  散度定义为:  其中, 如果 时, 会有。需要搞清楚一点事的 f-散度依据所选函数不一样，距离的不对称也不一样。 命题3：设 是域 上的严格凸函数，使得 。假设（相当于 ）或，其中。则有， 当且仅当。\n上面列举了一堆的分布度量以及对应的 , 那么一个很自然的问题是这些  的共同特点是什么 呢? 1、它们都是非负实数到实数的映射 ; 2、 ; 3、它们都是凸函数;\n\n\nAlgorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, , is a hyperparameter. We used , the least expensive option, in our experiments.\n\nfor number of training iterations do for  steps do\n\nSample minibatch of  noise samples  from noise prior .\nSample minibatch of  examples  from data generating distribution \nUpdate the discriminator by ascending its stochastic gradient:  end for\nSample minibatch of  noise samples  from noise prior .\nUpdate the generator by descending its stochastic gradient:  end for The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.\n\n\n【GAN 分批随机梯度下降训练算法】 对每一轮训练: { (1) 对判别器进行  步优化 (其中  为需要设置的超参数), 对其中每一步; a) 从生成器隐变量先验密度  中采样生成一扞  个生成样本  b) 从训练样本集中采样一批  个真实样本  c) 对判别器的参数  求目标函数的梯度  用该梯度上升的方向更新判别器参数 , 即  其中  为步长即学习率。 (2) 对生成器进行优化 a) 从生成器的隐变量概率密度  中采样生成一批  个生成样本  b) 对生成器参数  求目标函数的梯度  用该梯度下降的方向更新生成器参数 , 即 \n如此往复迭代训练，直到达到预设训练次数。 以生成对抗网络和变分自编码器为代表的生成模型，能产生与训练样本具有同样特性但又不同于训练样本集中任何实例的新样本。这种能力显示出了极大的应用潜力，改变了机器学习只能用于识别和预测的状况，使学习机器能够在学会“认识事物”的基础上模拟“创造新事物”。\n例如，生成模型可以让机器写出“手写”数字或文字、用机器生成不属于任何人的人像。把生成对抗网络与其他方法相结合，还可以实现用一辐人像生成出保持一定的原人像特征但又赋予其新特征的假造人像，例如把男子人像变成面容相似的女子人像，把风景照片变成某种风格的油画作品，等等。更进一步，人们可以用不同的训练样本集得到对图像的隐变量表示，在隐变量空阃间中对图像进行“语义运算”，用得到的新的隐变量表示生成新图像，由此实现诸如“戴眼镜男子图片一不戴眼镜男子图片十不戴眼镜女子图片=戴眼镜女子图片”的效果。有人甚至尝试用此类技术利用照片把政治人物替换到视频中，生成假的视频。这种应用技术上充满趣味，但也有人开始担心相关技术的发展和应用可能会对媒体和公众舆论产生巨大的影啊，彻底改变人们“眼见为实”的基本假定。\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"卷积神经网络","url":"/2022/05/06/Deep%20Learning/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"\n\n草稿！！！\n\n\n卷积网络\n我们经常一次在多个维度上进行卷积运算。例如, 如果把一张二维的图 像  作为输人, 我们也许也想要使用一个二维的核  :  卷积是可交换的 (commutative), 我们可以等价地写作:  通常, 下面的公式在机器学习库中实现更为简单, 因为  和  的有效取值范围 相对较小。\n卷积运算通过三个重要的思想来帮助改进机器学习系统： 稀疏交互（sparse interactions）、 参数共享（parameter sharing）、 等变表示（equivariant representations）。另外，卷积提供了一种处理大小可变的输入的方法。我们下面依次介绍这些 思想。\n不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似 不变（invariant）。对于平移的不变性是指当我们对输入进行少量平移时，经 过池化函数后的大多数输出并不会发生改\n多通道输入、多卷积核是卷积神经网络中最为常见的形式，\nLeNet\n\nlenet\n\nAlexNet\n\nScreen Shot 2022-05-17 at 8.43.58 PM\n\n\nalexnet\n\nVGG\n\nNiN\n\nGoogLeNet\n网络结构\n\nScreen Shot 2022-05-17 at 9.45.58 PM\n\n四个通道并在一起，模块堆叠起来计算量会越来越大\n1x1卷积层放在前面并不会损失多少信息，还可以减少很多计算量\n\n训练方法\n摘记\nThese classifiers take the form of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the network with a discount weight (the losses of the auxiliary classi- fiers were weighted by 0.3). At inference time, these auxiliary networks are discarded.\nResNet\n\nfunctionclasses\n\n残差网络的有效性\n与lstm的区别\n\n对于同等映射维度不匹配时，zero padding是参数free的，投影法会带来参数。作者比较了这两种方法的优劣。实验证明，投影法会比zero padding表现稍好一些。因为zero padding的部分没有参与残差学习。实验表明，将维度匹配或不匹配的同等映射全用投影法会取得更稍好的结果，但是考虑到不增加复杂度和参数free，不采用这种方法。\n\n\n\nScreen Shot 2022-05-18 at 12.19.22 AM\n\n在训练ResNet之前，让我们观察一下ResNet中不同模块的输入形状是如何变化的。 在之前所有架构中，分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。\nDenseNet\nCNN史上的一个里程碑事件是ResNet模型的出现，ResNet可以训练出更深的CNN模型，从而实现更高的准确度。ResNet模型的核心是通过建立前面层与后面层之间的“短路连接”（shortcuts，skip connection），这有助于训练过程中梯度的反向传播，从而能训练出更深的CNN网络。今天我们要介绍的是DenseNet模型，它的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。本篇文章首先介绍DenseNet的原理以及网路架构，然后讲解DenseNet在Pytorch上的实现。\n。\n一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。 然而，在前向传播中，我们将每个卷积块的输入和输出在通道维上连结。\nA 5-layer dense block with a growth rate of k = 4. Each layer takes all preceding feature-maps as input.\n\n在DenseNet中，作者引入了一个新的超参数，称为k也叫增长系数，主要就是DenseBlock块中每次1x1和3x3卷积后生成的特征图通道数，这些特征图需要cat前一个特征图作为后一层的输入。\n这里要补充一个概念,这里有一个参数,称为增长率,指的是每一层的额外通道数,或者说每层的卷积核,假如输入特征图的channel为,那么第层的channel数就为,因为每一层都接受前面所有层的特征图,所以这个可以很小,通常取12就有不错的结果,我们要注意这个K的实际含义就是这层新提取出的特征.\n稠密网络主要由2部分构成：稠密块（dense block）和过渡层（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。\n\nScreen Shot 2022-05-18 at 12.34.14 AM\n\nA deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change feature-map sizes via convolution and pooling.\nDenseNet通过过渡层不断控制通道数量，每个卷积的输入、输出通道数都没有超过256；反观ResNet，block3中5个卷积层输出通道均为256，block4中5个卷积层输出通道均为512，导致卷积层的参数量大幅增加；此外，在单层参数量最大的全连接层中，DenseNet输入通道数为248，远小于ResNet的512，因此在这一部分也获得了巨大的优势，最终使得DenseNet总体参数量比ResNet有了显著的下降\n更强的梯度流动 参数更少计算效率更高\nDenseNet则使用过渡层来减半高和宽，并减半通道数\n\npreview\n\n同时，CNN 有一些它独特的地方，比如各种定义：\n\nCNN 可以看作是 DNN 的一种简化形式，即这里 Convolution Kernel 中的每一个权值就可以看成是 DNN 中的  ，且与 DNN 一样，会多一个参数 Bias \n一个 Convolution Kernel 在与 Input 不同区域做卷积时，它的参数是固定不变的。放在 DNN 的框架中理解，就是对同一层 Layer 中的神经元而言，它们的  和  是相同的，只是所连接的节点在改变。因此在 CNN 里，这叫做 Shared Weights and Biases\n在 CNN 中，Convolution Kernel 可能是高维的。假如输入是  维的，那么一般 Convolution Kernel 就会选择为  维，也就是与输入的 Depth 一致\n最重要的一点，在 CNN 中，Convolution Kernel 的权值不需要提前设计，而是跟 DNN 一样利用 GD 来优化，我们只需要初始化\n如上面所说，其实 Convolution Kernel 卷积后得到的会是原图的某些特征（如边缘信息），所以在 CNN 中，Convolution Kernel 卷积得到的 Layer 称作 Feature Map\n一般 CNN 中两层之间会含有多个 Convolution Kernel，目的是学习出 Input 的不同特征，对应得到多个 Feature Map。又由于 Convolution Kernel 中的参数是通过 GD 优化得到而非我们设定的，于是初始化就显得格外重要了\n\n对 CNN 还有想要更多了解的可以参考原专栏文章：\n图片数据的识别过程一般认为也是表示学习(Representation Learning)的过程，从接受到 的原始像素特征开始，逐渐提取边缘、角点等底层特征，再到纹理等中层特征，再到头 部、物体部件等高层特征，最后的网络层基于这些学习到的抽象特征表示(Representation) 做分类逻辑的学习。学习到的特征越高层、越准确，就越有利于分类器的分类，从而获得 较好的性能。从表示学习的角度来理解，卷积神经网络通过层层堆叠来逐层提取特征，网 络训练的过程可以看成特征的学习过程，基于学习到的高层抽象特征可以方便地进行分类 任务。\n应用表示学习的思想，训练好的卷积神经网络往往能够学习到较好的特征，这种特征 的提取方法一般是通用的。比如在猫、狗任务上学习到头、脚、身躯等特征的表示，在其 它动物上也能够一定程度上使用。基于这种思想，可以将在任务 A 上训练好的深层神经网 络的前面数个特征提取层迁移到任务 B 上，只需要训练任务 B 的分类逻辑(表现为网络的 最末数层)，即可取得非常好的效果，这种方式是迁移学习的一种，从神经网络角度也称为 网络微调(Fine-tuning)。\nimport torchfrom torch import nndef vgg_block(num_convs, in_channels, out_channels):    layers = []    for _ in range(num_convs):        layers.append(nn.Conv2d(in_channels, out_channels,                                kernel_size=3, padding=1))        layers.append(nn.ReLU())        in_channels = out_channels    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))    return nn.Sequential(*layers)\n参考文献\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"变分推断","url":"/2022/05/06/Deep%20Learning/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD/","content":"\n\n硬核程度：★★★☆☆☆\n在很多任务中，我们关心某些概率分布并非因为对这些概率分布本身感兴趣，精确推断方法通常需要很大的计算开销, 因此在现实应用中近似推断方法更为常用. 近似推断方法大致可分为两大类: 第一类是采样(sampling)，通过使用随机化方法完成近似; 第二类是使用确定性近似完成近似推断，典型代表为变分推断(variational inference).\n\n\n！！！！⚠️\n记得统一符号\n\n精确推断问题可以描述为一个优化问题, 有许多方法正是由此解决了推断的困 难。通过近似这样一个潜在的优化问题，我们往往可以推导出近似推断算法。 为了构造这样一个优化问题, 假设我们有一个包含可见变量  和潜变量  的概 率模型。我们希望计算观察数据的对数概率  。有时候如果边缘化消去  的 操作很费时, 我们会难以计算  。作为替代, 我们可以计算一个  的下界  。这个下界被称为 证据下界 ( evidence lower bound, ELBO )。这个 下界的另一个常用名称是负 变分自由能 (variational free energy )。具体地, 这个证 据下界是这样定义的:  其中  是关于  的一个任意概率分布。 因为  和  之间的距离是由  散度来衡量的, 且  散度总是非负的，我们可以发现  总是小于等于所求的对数概率。当且仅当分布  完全相等 于  时取到等号。\n令人吃惊的是，对于某些分布 ，计算  可以变得相当简单。通过简单的代数运算我们可以把  重写成一个更加简单的形式:  这也给出了证据下界的标准定义:  对于一个选择的合适分布  来说,  是容易计算的。对任意分布  的选择来说,  提供了似然函数的一个下界。越好地近似  的分布 , 得到的下界就 越紧, 换言之, 就是与  更加接近。当  时, 这个近似是完 美的, 也意味着  。 因此我们可以将推断问题看作是找一个分布  使得  最大的过程。精确推断能够在包含分布  的函数族中搜索一个函数, 完美地最大化  在本章中, 我们 将会讲到如何通过近似优化寻找分布  的方法来推导出不同形式的近似推断。我们 可以通过限定分布  的形式或者使用并不彻底的优化方法来使得优化的过程更加高 效 (却更粗略), 但是优化的结果是不完美的, 不求彻底地最大化 , 而只要显著地 提升 。 无论我们选择什么样的分布  始终是一个下界。我们可以通过选择一个更简单或更复杂的计算过程来得到对应的更松或更紧的下界。通过一个不彻底的优化过 程或者将分布  做很强的限定（并且使用一个彻底的优化过程 ) 我们可以获得一个 很差的分布 ，但是降低了计算开销。\n我们介绍的第一个最大化下界  的算法是期望最大化 ( expectation maximization, EM ) 算法。在潜变量模型中，这是一个非常常见的训练算法。在这里我们描述 Neal and Hinton (1999) 所提出的 EM 算法。与大多数我们在本章中介绍的其他算法不同的是，EM 并不是一个近似推断算法，而是一种能够学到近似后验的算法。 EM算法由交替迭代, 直到收玫的两步运算组成:\n\nE 步 (expectation step ) : 令  表示在这一步开始时的参数值。对任何我们 想要训练的 (对所有的或者小批量数据均成立) 索引为  的训练样本 , 令  。通过这个定义, 我们认为  在当前参数  下 定义。如果我们改变 , 那么  将会相应地变化, 但是  还是 不变并且等于  。\nM 步 ( maximization step ): 使用选择的优化算法完全地或者部分地关于  最 大化  这可以被看作通过坐标上升算法来最大化  。在第一步中, 我们更新分布  来 最大化 , 而在另一步中, 我们更新  来最大化  。\n\n基于潜变量模型的随机梯度上升可以被看作是一个  算法的特例，其中  步包括了单次梯度操作。EM 算法的其他变种可以实现多次梯度操作。对一些模型 族来说,  步甚至可以直接推出解析解, 不同于其他方法, 在给定当前  的情况下 直接求出最优解。 尽管E 步采用的是精确推断, 我们仍然可以将  算法视作是某种程度上的近 似推断。具体地说,  步假设一个分布  可以被所有的  值分享。当  步越来越 远离  步中的  时, 这将会导致  和真实的  之间出现差距。幸运的是, 在进人下一个循环时,  步把这种差距又降到了 0 。  算法还包含一些不同的见解。首先, 它包含了学习过程的一个基本框架, 就 是我们通过更新模型参数来提高整个数据集的似然, 其中缺失变量的值是通过后验 分布来估计的。这种特定的性质并非  算法独有的。例如, 使用梯度下降来最大化 对数似然函数的方法也有相同的性质。计算对数似然函数的梯度需要对隐藏单元的 542 第十九章 近似推断 后验分布求期望。EM 算法另一个关键的性质是当我们移动到另一个  时候, 我们 仍然可以使用旧的分布  。在传统机器学习中, 这种特有的性质在推导大  步更新 时候得到了广泛的应用。在深度学习中, 大多数模型太过于复杂以致于在最优大  步更新中很难得到一个简单的解。所以  算法的第二个特质, 更多为其所独有, 较少被使用。\n\n四、总结 总的来说EM算法是变分推断的一个特例, EM算法在M步时, 认为  是一个常数 (在每一个阶段极大似然估计时, 上一步的隐变量是确定的）, 因此计算只需要极大化 分推断比EM算法要复杂的原因。\n期望最大化算法, 或者EM算法, 是寻找具有潜在变量的概率模型的最大似然解的一种通 用的方法 (Dempster et al., 1977; MaLachlan and Krishnan, 1997)。这里, 我们给出一般形式的EM算法, 并且在这个过程中, 会证明9.2节和  节在讨论高斯混合模型时启发式地推导出 的EM算法确实最大化了似然函数 (Csiszàr and Tusnàdy, 1984; Hathaway, 1986; Neal and Hinton, 1999）。我们的讨论也构成了变分推断框架推导的基础。\n考虑一个概率模型，其中我们将所有的观测变量联合起来记作 ，将所有的隐含变量记作  。联合概率分布  由一组参数控制, 记作  。我们的目标是最大化似然函数  这里，我们假设  是离散的，但是当  是连续变量或者离散变量与连续变量的组合时，方法是完全相同的，只需把求和换成适当的积分即可。\n\n我们假设直接最优化  比较困难，但是最优化完整数据的似然函数  就容易得多。接下来，我们引入一个定义在潜在变量上的分布  。我们观察到，对于任意的 ，下面的分解成立  其中，我们定义了  注意,  是概率分布  的一个泛函（关于泛函的讨论, 见附录D）, 并且是参数  的一个 函数。值得仔细研究的是表达式 (9.71) 和 (9.72) 的形式, 特别地, 需要注意, 二者的符号相 反, 并且  包含了  和  的联合概率分布, 而KL  包含了给定  的条件下,  的条件\n\n把推断视作优化问题\n精确推断问题可以描述为一个优化问题，有许多方法正是由此解决了推断的困 难。通过近似这样一个潜在的优化问题，我们往往可以推导出近似推断算法。 为了构造这样一个优化问题，假设我们有一个包含可见变量  和潜变量  的概率模型。我们希望计算观察数据的对数概率  。有时候如果边缘化消去  的操作很费时，我们会难以计算  。作为替代，我们可以计算一个  的下界  。这个下界被称为 证据下界 ( evidence lower bound, ELBO )。这个下界的另一个常用名称是负变分自由能 (variational free energy)。具体地，这个证据下界是这样定义的:  其中  是关于  的一个任意概率分布。 因为  和  之间的距离是由 KL 散度来衡量的, 且 KL 散度总是\nEM 算法：\nEM算法推导\n最小化KL Divergence角度\n$$\n\n两边同时关于求期望，得\n\n$$\n\n注： ， 且  。\n\n\n下图是：EM算法的E步骤的说明。q 分布被设置为当前参数值 θ 旧 下的后验概率分布，这使得下界上移到与 对数似然函数值相同的位置，此时KL散度为零。“EM算法的E步，就是让上式  尽可能地小，以此在M步进行极大似然估计。”\n\n\nE步：⚠️！！！！我搞懂了！！！！看PRML，假设  由  确定 \n\n\nM步：最大化下界  $$\n\n由于与无关所以 ={} {Z} P(Z Y, ^{(t)}) P(Y, Z ) d z $$\n\nM步骤的说明： 分布q(Z)保持固定， 下界 L(q, θ) 关于参数向量 θ 最⼤化， 得到修正值 θ 新 。由于KL散度⾮负，因此这使得对数似然函数ln p(X | θ) 的增量⾄少与下界的增量相等。\n\n\n如果我们将 昌 代入公式 ，我们会看到，在E步骤之后，下界的形式为 昍昍昍\n昍常数\n其中, 常数就是分布  的熵, 因此与  无关。从而在M步骤中, 最大化的量是完整数据对数似然 函数的期望, 正如我们之前在混合高斯模型的情形中看到的那样。注意, 我们进行优化的变 量  只出现在对数运算内部。如果联合概率分布  由指数族分布的成员组成, 或者由指 数族分布成员的乘积组成，那么我们看到对数运算会抵消指数运算, 从而使得  步骤通常比最 大化对应的不完整数据对数似然函数  要容易得多。\nEM算法的计算也咪可以被看做参数空间中的运算, 如图9.14所示。这里, 红色曲线表示（不 完整数据）对数似然函数, 它的最大值是我们想要得到的。我们首先选择某个初始的参数\n\n\n但在隐变量  的组成较为复杂时，  很难求得，因此需要一种“近似推断\"的方法求出 , 使得 , 而变分推断就是其中一种方法  为此我们的目标是  简单来说, 我们就是需要找出隐变量的近似分布。\n总的来说EM算法是变分推断的一个特例, EM算法在M步时, 认为  是一个常数 (在每一个阶段极大似然估计时, 上一步的隐变量是确定的), 因此计算只需要极大化 分推断比EM算法要复杂的原因。\n坐标上升！！！\n\nJensen Inequlity角度EM算法的推导*\n本节从另一个角度Jensen Inequlity来推导。\n 算法能近似实现对观测数据的极大似然估计。下面通过近似求解观测数据的对数似然函数的极大化问题来导出  算法，由此可以清楚地看出  算法的作用。我们面对一个含有隐变量的概率模型，目标是极大化观测数据（不完全数据）  关于参数  的对数似然函数，即极大化  注意到这一极大化的主要困难是式中有末观测数据并有包含和（或积分）的 对数。\n事实上， 算法是通过迭代逐步近似极大化  的。假设在第  次迭代后  的 估计值是  。我们希望新估计值  能使  增加, 即 , 并逐步达到极 大值。为此, 考虑两者的差: \n\n利用 Jensen 不等式 (Jensen inequality) 其中\n\n\n得到其下界:  令  则  即函数  是  的一个下界，可知，  因此，任何可以使  增大的 ，也可以使  增大。为了使  有尽可能大的增长，选择  使  达到极大, 即  现在求  的表达式。省去对  的极大化而言是常数的项, 有  上式等价于 EM算法的一次迭代，即求  函数及其极大化。EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法。下图给出  算法的直观解释。图中上方曲线为 , 下方曲线为  。由已知  为对数似然函数  的下界。由式 , 两个函数在点  处相等。由式 (9.16) 和式 (9.17)，EM算法找到下一个点  使函数  极大 化, 也使函数  极大化。这时由于 , 函数  的增加, 保证对数似然函数  在每次迭代中也是增加的。EM算法在点  ) 重新计算  函数值, 进行下一次迭代。在这个过程中, 对数似然函数  不断增大。从图可以推 断出  算法不能保证找到全局最优值。\n\n\n\n\nEM算法的收敛\n\n只能保证向着极大值靠近，但没保证一定会达到极大值，以下给出了EM算法一定会达到极大值的收敛性证明！！！\n\n由此得到了\nEM 算法：\n输入: 观测变量数据 ，隐变量数据 ，联合分布 ，条件分布 ;\n输出: 模型参数  。 （1）选择参数的初值 , 开始迭代; (2)  步: 记  为第  次迭代参数  的估计值, 在第  次迭代的  步, 计算  这里,  是在给定观测数据  和当前的参数估计  下隐变量数据  的条 件概率分布; (3)  步：求使  极大化的 , 确定第  次迭代的参数的估计值   (4) 重复第 (2) 步和第 (3) 步, 直到收敛。 式 (9.9) 的函数  是  算法的核心, 称为  函数（  function）。\n\n变分推断\n\n第二类是使用确定性近似完成近似推断，典型代表为变分推断(variational inference).\n\n\n！！！！⚠️\n现在，让我们详细讨论变分最优化的概念如何应用于推断问题。假设我们有一个纯粹的贝叶斯模型，其中每个参数都有一个先验概率分布。这个模型也可以有潜在变量以及参数，我们会把所有潜在变量和参数组成的集合记作  。类似地，我们会把所有观测变量的集合记作  。例如，我们可能有  个独立同分布的数据，其中  且  。我们的概率模型确定了联合概率分布 ，我们的目标是找到对后验概率分布  以及模型证据  的近似。与我们关于EM的讨论相同，我们可以将对数边缘概率分解, 即\n注意和以前的区别\n\n 其中我们定义了  这与我们关于EM的讨论的唯一的区别是参数向量  不再出现，因为参数现在是随机变量，被整合到了  中。由于本章中我们主要感兴趣的是连续变量, 因此我们在这个分解的公式中使用了 积分而不是求和。但是, 如果某些变量或者全部的变量都是离散变量, 那么分析过程不变, 只 需根据需要把积分替换为求和即可。\n与之前一样，我们可以通过关于概率分布  的最优化来使下界  达到最大值，这等价于最小化KL散度。如果我们允许任意选择 ，那么下界的最大值出现在KL散度等于零的时刻，此时  等于后验概率分布  。然而，我们假定在需要处理的模型中，对真实的概率分布进行操作是不可行的。\n于是，我们转而考虑概率分布  的一个受限制的类别，然后寻找这个类别中使得KL散度达到最小值的概率分布。我们的目标是充分限制  可以取得的概率分布的类别范围，使得这个范围中的所有概率分布都是可以处理的概率分布。同时，我们还要使得这个范围充分大、充分灵活，从而它能够提供对真实后验概率分布的一个足够好的近似。需要强调的是，施加限制条件的唯一目的是为了计算方便, 并且在这个限制条件下, 我们应该使用尽可能丰富的近似概 率分布。特别地, 对于高度灵活的概率分布来说, 没有 “过拟合”现象。使用灵活的近似仅仅使 得我们更好地近似真实的后验概率分布。 限制近似概率分布的范围的一种方法是使用参数概率分布 , 它由参数集合  控制。 这样, 下界  变成了  的函数, 我们可以利用标准的非线性最优化方法确定参数的最优值。 图10.1给出了这种方法的一个例子, 其中变分分布是一个高斯分布, 并且我们已经关于均值和协 方差进行了最优化。\n这里, 我们考虑另一种方法, 这种方法里，我们限制概率分布  的范围。假设我们将  的 元素划分成若干个互不相交的组, 记作 , 其中  。然后, 我们假定  分布关于这些 分组可以进行分解, 即  需要强调的是，我们关于概率分布没有做更多的假设。特别地，我们没有限制各个因子  的函数形式。变分推断的这个分解的形式对应于物理学中的一个近似框架，叫做平均场理论 (mean field theory)\n在所有具有公式 (10.5) 的形式的概率分布  中, 我们现在寻找下界  最大的概率分布。于是，我们希望对  关于所有的概率分布  进行一个自由形式的（变分）最优化。 我们通过关于每个因子进行最优化来完成整体的最优化过程。为了完成这一点, 我们首先将公 式 (10.5) 代入公式 (10.3) , 然后分离出依赖于一个因子  的项。为了记号的简洁, 我们简单地将  记作 , 这样我们有 $$\n常数常数\n$$ （最后将与  无关的部分写作 const）\n其中, 我们定义了一个新的概率分布 , 形式为 常数 这里, 记号  表示关于定义在所有  上的  概率分布的期望, 即  现在假设我们保持  固定, 关于概率分布  的所有可能的形式最大化公 式 (10.6) 中的  。这很容易做, 因为我们看到公式  是  和  之间 的Kullback-Leibler散度的负值。因此, 最大化公式 (10.6) 等价于最小化Kullback-Leibler散度，且最小值出现在  的位置。于是，我们得到了最优解  的一般的表达式，形式为 常数 很值得花一些时间研究一下解的形式，因为它是变分方法应用的基础。这个解表明，为了得到因子  的最优解的对数, 我们只需考虑所有隐含变量和可见变量上的联合概率分布的对数, 然 后关于所有其他的因子  取期望即可, 其中  。 公式  中的可加性常数通过对概率分布  进行归一化的方式来设定。因此, 如果 我们取两侧的指数, 然后归一化, 我们有  在实际应用中, 我们会发现, 更方便的做法是对公式 (10.9) 进行操作, 然后在必要的时候, 通 过观察的方式恢复出归一化系数。这一点通过下面的例子就会变得逐渐清晰起来。 由公式 (10.9) 给定的方程的集合（其中  ) 表示在概率能够进行分解这一限制 条件下, 下界的最大值满足的一组相容的条件。然而, 这些方程并没有给出一个显式的解, 因 为最优化  的公式  的右侧表达式依赖于关于其他的因子  计算的期望。 于是, 我们会用下面的方式寻找出一个相容的解: 首先, 恰当地初始化所有的因子  然后 在各个因子上进行循环, 每一轮用一个修正后的估计来替换当前因子。这个修正后的估计由公 式 (10.9) 的右侧给出, 计算时使用了当前对于所有其他因子的估计。算法保证收玫, 因为下界 关于每个因子  是一个凸函数（Boyd and Vandenberghe, 2004）。\n\nVAE\n和变分推断一样，将隐变量和参数同时整合到Z里！是一个标准的概率图模型和神经网络结合的一个东西\n\n\n把  中的联合概率分解, 可以得到  第一项是模型对样本的似然函数的期望,\n第二项是变分密度函数与隐含变量的先验密度函数  的 KL 散度。\n变分推断就是最大化证据下界 ELBO，这一最大化一方面使隐念变量解释观察数据的似然函数尽可能大，另一方面使变分函数尽可能靠近隐含变量的先验密度函数(KL散度尽可能小)。\n变分推断是一种一般性的方法，在图 12-27 的概率图模型中，选择不同类型的概率密度函数模型，可以得到不同的具体方法。也有人把变分推断用于模型选择问题。变分自编码器是把变分推䉼与自编码器结合起来，用神经网络来实现概率图模型。\n\n在上面的变分推断中，  是在观测样本下从给定函数族中求解的概率密度函数，因此可以看作是从  到  的条件密度  。下面我们看如何用类似自编码器结构的神经网络来实现  和 ，前者是从输入到隐层的映射 (对应于编码器)，后者是从隐层到输出的映射 (对应于解码器)。与传统神经网络模型不同的是, 这里的映射不是多层感知器和自编码器那样的确定性映射，而是由条件概率定义的随机侏映射。\n我们用类似自编码器中的编码层来实现 , 用解码层来实现  。从概念上，我们可以用最大化 ELBO 作为目标来训练这个特殊的自编码器, 即  &gt;这里,公式右边第一项反映了模型对样本重构的程度，重构误差越小则此项越大; 第二项是要求自编码器的编码层节点尽量符合一个给定的概率密度函数 ，例如通常可以选为元素间独立的多元正态分布  。\n与自编码器相比，变分自编码器多了一个目标，就是要求训练样本在隐层节点的编码符合给定的概率密度函数。从直观上可以这样理解 : 如果不对隐层节点加任何约束，虽然自编码器能够实现把训练样本编码后再解码出接近输人样本的输出，但编码在隐层节点所在的空间中的分布是随意的，如果在隐层空间中任意给定一个向量作为编码，很可能不对应任何有意义的样本;\n变分自编码器通过强制隐节点变量符合给定概率密度函数，约束了编码在隐层空间中的分布，我们按照隐节点概率密度函数采样编码向量，就可以产生出与训练样本相似的新样本。 可以证明，如果我们指定隐层概率密度函数为各项独立的多元正态分布 , 式 (12-55)的目标函数等价于  &gt;其中  是变分自编码器的输出向量。也就是说，变分自编码器的学习目标相当于在普通编码器器最小化重构误差的基础上增加了一个对隐节点的正则化约束。\n但是，式（12-56）的目标函数并不能像自编码器中那样进行训练，原因是自编码器实现的是确定性的映射，无法体现对隐层节点慨率密度函数的约束。\n\n这样做是为了确定一个过程，以使用BP算法，其中把概率不确定抽样分隔开来了\n\n变分自编码器在隐层节点上的运算与普通的自编码器非常不同。如图 12-28 所示, 变 分自编码器并不直接把  向量作为编码层的隐节点输出，而是把编码层和解码层分开，把编码向量  的概率密度函数参数作为编码层神经网络的输出节点，在正态分布情况下就是  和  。解码层神经网络的输人是编码向量 ，它不是从编码层神经网络直接计算出来的，而是用编码层神经网络输出的参数构造概率密度函数，从这个概率密度函数中采样得到的编码向量作为解码层网络的输人，再经过解码层神经网络产生对输入样本的重构。\n作为生成式模型使用时，对于训练好的变分自编码器，得到隐含变量的概率密度函数参数后就不再需要编码层神经网络。我们只需要按照隐含变量概率密度函数采样编码向量，即可用解码器产生新的样本向量。\n由于变分自编码器从啲码层网络到解码层网络之间存在一个采样的步聚，目标函数的梯度无法反向传播到编码层。我们需要消除训练过程中的这一随机因素才可能用梯度下降法进行训练。所谓的方法被称作“再参数化技巧” (reparameterization trick)。\n\n在上面介绍的变分自编码器中，在编码层得到隐层概率密度函数参数  和  后，需要从相应的多元正态分布中采样获得解码层的输人向量  :  其中  和  带下标  以示是在输入样本  下编码层输出的隐变量概率密度参数。再参数化技巧用下面的方法来替代这个采样过程：从单位正态分布中采样一个与隐变量维数相同的随机样本 ， 用隐变量分布的均值  对  进行平移, 再用隐变量分布的方差  改变其尺 度, 即  其中“  ”表示两个维数相同的向量对应位置的元素相乘（即 “按位相乘”, 有时也用“(  \"表示)。\n如果不用再参数化技巧，从编码层得到的楖率密度参数到解码层输入这个环节是随机采样，训练时误差梯度无法传播; 但改为这样的再参数化步骤后，解码层的输入是编码层输出直接进行确定性计算得到的，只是计算中有一个从单位正态分布采样得到的系数  。这个系数保持了解码层输入中需要有的随机性，但它不是需要训练的参数，对于需要训练的参数来说从编码层到解码层都变成了确定性计算，可以通过误差反向传播算法进行梯度下降训练。\n变分自编码器是人们研究和使用最多的生成式模型之一，被成功地应用于产生图像、视频等样本，很多情况下已经能达到几乎以假乱真的效果。除了用于生成新样本，变分自编码器还在很多领域中被用来从高维数据中提取嵌人的低维流形。\nHinton 老仙说， 机器学习的未来在于无监督学习！EM主要是为了解决生成模型，并且是概率生成模型。\nEM算法存在的意义是什么？ - 史博的回答 - 知乎 https://www.zhihu.com/question/40797593/answer/275171156\n1.1 把推断视作优化问题\n1.2 期望最大化\n1.3 最大后验推断和稀疏编码\n1.4 变分推断和变分学习\n许多概率模型很难训练的原因是很难进行推断。在深度学习中, 通常我们有一系列可见变量  和一系列潜变量  。推断困难通常是指难以计算  或其期望。 而这样的操作在一些诸如最大似然学习的任务中往往是必需的。 许多仅含一个隐藏层的简单图模型会定义成易于计算  或其期望的形式, 后验分布都很难处理。对于这些模型而言, 精确推断算法需要指数量级的运行时间。 即使一些只有单层的模型, 如稀疏编码, 也存在着这样的问题。 在本章中, 我们将会介绍几个用来解决这些难以处理的推断问题的技巧。稍后, 在第二十章中, 我们还将描述如何将这些技巧应用到训练其他方法难以奏效的概率 模型中, 如深度信念网络、深度玻尔兹曼机。 在深度学习中难以处理的推断问题通常源于结构化图模型中潜变量之间的相互 作用。读者可以参考图  的几个例子。这些相互作用可能是无向模型的直接相互 作用, 也可能是有向模型中同一个可见变量的共同祖先之间的 “相消解释\"' 作用。\n变分法\n1.5 学成近似推断\n近似推断方法大致可分为两大类: 第一类是采样(sampling)，通过使用随机化方法完成近似; 第二类是使\nhttps://medium.com/geekculture/variational-autoencoder-vae-9b8ce5475f68\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"注意力机制","url":"/2022/05/06/Deep%20Learning/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","content":"\n\n草稿！！！\n\n\n1. Seq2Seq with RNNs\n\nScreen Shot 2022-05-18 at 5.44.40 PM\n\nS_0 由H_t通过全连接前馈网络学到，或者直接设置为0（Pytorch）\n使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。\n这里课件貌似写错了，最初的实现有两种形式，而课件里的图把两种都塞一起了。\n\nseq2seq\n\n机器翻译\nSequence to Sequence with RNNs\nProblem: Input sequence bottlenecked through fixed-sized vector. What ifT=1000?\nIdea: use new context vector at each step of decoder!\n2. Seq2Seq with RNNs and Attention\n\n\n\nThe decoder doesn’t use the fact that h i form an ordered sequence – it just treats them as an unordered set {h i }\nCan use similar architecture given any set of input hidden vectors {h i }!\n3. Image Captioning using spatial features\n\nProblem: Input is \"bottlenecked\" through c - Model needs to encode everything it wants to say within c This is a problem if we want to generate really long descriptions? 100s of words long\nAttention idea: New context vector at every time step. Each context vector will attend to different image regions\n4. Image Captioning with RNNs and Attention\n\nThis entire process is differentiable. model chooses its own attention weights. No attention supervision is required\n\n\nHard attention：(requires reinforcement learning)\n因为为了防止维数过高时QKTQKTQKT的值过大导致softmax函数反向传播时发生梯度消失。那为什么是dkdk而不是dkd_kdk呢？这就是个经验值，从理论上来说，就是还需要让QKTQKTQKT的值适度增加，但不能过度增加，如果是dkd_kdk的话，可能就不增加了。\n5. General attention and self-attention\nGeneral attention\n\nScreen Shot 2022-05-19 at 2.20.24 PM\n\nAttention operation is permutation invariant.\n\nDoesn't care about ordering of the features\nStretch H xW = N into N vectors（in the bottom picture，x should be 9xD instead of 3xD, but for convenience, it's shown like that)\n\nChange  to a scaled simple dot product\n\nLarger dimensions means more terms in the dot product sum.\nSo, the variance of the logits is higher. Large magnitude vectors will produce much higher logits.\nSo, the post-softmax distribution has lower-entropy, assuming logits are IID.\nUltimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others\nDivide by  to reduce effect of large magnitude vectors\n\n\nScreen Shot 2022-05-19 at 2.41.52 PM\n\nMultiple query vectors\n\neach query creates a new output context vector Multiple query vectors\n\n\nNotice that the input vectors are used for both the alignment as well as the attention calculations. - We can add more expressivity to the layer by adding a different FC layer before each of the two steps.\n\nScreen Shot 2022-05-19 at 2.04.12 PM\n\nOutputs: context vectors: y (shape:Dv)\nNow, The input dimensions  and output dimensions  can now change depending on the key and value FC layers.\nSelf attention layer\nWe can calculate the query vectors from the input vectors, therefore, defining a \"self-attention\" layer.\n\nScreen Shot 2022-05-19 at 3.14.35 PM\n\n\n\n\nScreen Shot 2022-05-19 at 3.19.53 PM\n\nProblem:\n\nPermutation equivariant\nPermutation equivariant Self-attention layer doesn’t care about the orders of the inputs!\nHow can we encode ordered sequences like language or spatially ordered image features?\n\n\nScreen Shot 2022-05-19 at 3.23.04 PM\n\n\nFrom Attention Is All You Need :\nThis work use sine and cosine functions of different frequencies:  where pos is the position and  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  to . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  can be represented as a linear function of .\nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n\n\nScreen Shot 2022-05-19 at 3.47.36 PM\n\nreferences: 详解自注意力机制中的位置编码\n\n\n\n\nScreen Shot 2022-05-19 at 4.04.01 PM\n\n\n\nScreen Shot 2022-05-19 at 4.04.19 PM\n\n\nScreen Shot 2022-05-19 at 4.09.59 PM\n\nComparing RNNs to Transformer\nRNNs:\n\n(+) LSTMs work reasonably well for long sequences.\n(-) Expects an ordered sequences of inputs\n(-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done.\n\nTransformer:\n\n(+) Good at long sequences. Each attention calculation looks at all inputs.\n(+) Can operate over unordered sets or ordered sequences with positional encodings.\n(+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel.\n(-) Requires a lot of memory:  alignment and attention scalers need to be calculated and stored\nfor a single self-attention head. (but GPUs are getting bigger and better)\n\nImage Captioning using Transformers\n\nScreen Shot 2022-05-19 at 4.17.39 PM\n\n\n\n\nResidual connection\nMLP over each vector individually LayerNorm over each vector individually\nResidual connection\nLayer norm\nAttention attends over all the vectors\nAdd positional encoding\n\nMade up of  encoder blocks. In vaswani et al. \n\n\n\n\nMost of the network is the same the transformer encoder.\nMulti-head attention block attends over the transformer encoder outputs.\nFor image captions, this is how we inject image features into the decoder.\n\n\n\nScreen Shot 2022-05-19 at 4.33.53 PM\n\n\n\nDosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020 Colab link to an implementation of vision transformers\n\n\nScreen Shot 2022-05-19 at 4.42.20 PM\n\n\nAn Image is Worth 16x16 Words\n\nScreen Shot 2022-05-19 at 4.44.04 PM\n\n\n\nScreen Shot 2022-05-19 at 4.48.25 PM\n\n\nA ConvNet for the 2020s. Liu et al. CVPR 2022\n\nScreen Shot 2022-05-19 at 4.52.49 PM\n\n\nDeiT III: Revenge of the ViT\n\nSummary\n\nAdding attention to RNNs allows them to \"attend\" to different parts of the input at every time step\nThe general attention layer is a new type of layer that can be used to design new neural network architectures\nTransformers are a type of layer that uses self-attention and layer norm.\nIt is highly scalable and highly parallelizable\nFaster training, larger models, better performance across vision and language tasks\nThey are quickly replacing RNNs, LSTMs, and may(?) even replace convolutions.\n\n终于理解到通道数的内涵了！！！！，甚至能联想到RGB\n\n\n\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"循环神经网络","url":"/2022/05/06/Deep%20Learning/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","content":"\n\n草稿！！！\n\n\nRNN\n输出方式\n\none to one one to many\n\nBackpropagation through time Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient\none to many to one\n\nScreen Shot 2022-05-18 at 5.40.09 PM\n\n网络结构\n \n梯度\n$$\n\n$$\nAlmost always &lt; 1 Vanishing gradients What if we assumed no non-linearity? Largest singular value &gt; 1: Exploding gradients Largest singular value &lt; 1: Vanishing gradients \n\nLSTM\n网络结构\n \n梯度\n输入门、遗忘门和输出门，候选记忆元（candidate memory cell）LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.f ～1 就能保证梯度的传递，避免了梯度消失，后面大于0.  梯度爆炸，裁剪\n特征值提升到 t 次后，导致幅值不到一的特征值衰减到零，而幅值大于一的就会激 增。任何不与最大特征向量对齐的 h (0) 的部分将最终被丢弃。\nLSTM 中梯度的传播有很多条路径， 这条路径上只有逐元素相乘和相加的操作，梯度流最稳定；但是其他路径（例如  )上梯度流与普通 RNN 类似，照样会发生相同的权重矩阵反复连乘\n但是在其他路径上，LSTM 的梯度流和普通 RNN 没有太大区别，依然会爆炸或者消失。由于总的远距离梯度 = 各条路径的远距离梯度之和，即便其他远距离路径梯度消失了，只要保证有一条远距离路径（就是上面说的那条高速公路）梯度不消失，总的远距离梯度就不会消失（正常梯度 + 消失梯度 = 正常梯度）。因此 LSTM 通过改善一条路径上的梯度问题拯救了总体的远距离梯度。\n参考深度学习书籍\n6、同样，因为总的远距离梯度 = 各条路径的远距离梯度之和，高速公路上梯度流比较稳定，但其他路径上梯度有可能爆炸，此时总的远距离梯度 = 正常梯度 + 爆炸梯度 = 爆炸梯度，因此 LSTM 仍然有可能发生梯度爆炸。不过，由于 LSTM 的其他路径非常崎岖，和普通 RNN 相比多经过了很多次激活函数（导数都小于 1），因此 LSTM 发生梯度爆炸的频率要低得多。实践中梯度爆炸一般通过梯度裁剪来解决。\n7、对于现在常用的带遗忘门的 LSTM 来说，6 中的分析依然成立，而 5 分为两种情况：其一是遗忘门接近 1（例如模型初始化时会把 forget bias 设置成较大的正数，让遗忘门饱和），这时候远距离梯度不消失；其二是遗忘门接近 0，但这时模型是故意阻断梯度流的，这不是 bug 而是 feature（例如情感分析任务中有一条样本 “A，但是 B”，模型读到“但是”后选择把遗忘门设置成 0，遗忘掉内容 A，这是合理的）。当然，常常也存在 f 介于 [0, 1] 之间的情况，在这种情况下只能说 LSTM 改善（而非解决）了梯度消失的状况。\n8、最后，别总是抓着梯度不放。梯度只是从反向的、优化的角度来看的，多从正面的、建模的角度想想 LSTM 有效性的原因。选择性、信息不变性都是很好的视角，比如看看这篇：https\n\n\nGRU\n网络结构\n  #### 梯度\n重置门（reset gate），更新门（update gate），候选隐状态，重置门允许我们控制“可能还想记住”的过去状态的数量； 更新门将允许我们控制新状态中有多少个是旧状态的副本。LSTM内部结构比较复杂，因此衍生了简化版GRU，把LSTM的input gate和forget gate整合成一个update gate，也是通过gate机制来控制梯度： \n深度RNN\n\ndeep-rnn\n\n双向RNN\n类似的还有双向LSTM。\n\nbirnn\n\n如上篇文章BRNN所述同理，有些时候预测可能需要由前面若干输入和后面若干输入共同决定，这样会更加准确。因此提出了双向循环神经网络，网络结构如下图。可以看到Forward层和Backward层共同连接着输出层，其中包含了6个共享权值w1-w6。\n双向循环神经网络是由 [Schuster &amp; Paliwal,1997]提出的, 关于各种架构的详细讨论请参阅 [Graves &amp; Schmidhuber, 2005]。让我们看看这样一个网络的细节。 对于任意时间步 , 给定一个小批量的输入数据  (样本数： , 每个示例中的输入数：  ), 并且令隐藏层 激活函数为  在双向架构中, 我们设该时间步的前向和反向隐状态分别为  和 , 其中  是隐 藏单元的数目。前向和反向隐状态的更新如下:  其中, 权重  和偏置  都是模型参 数。 接下来, 将前向隐状态  和反向隐状态  连接起来, 获得需要送入输出层的隐状态  。在具有多个隐藏 层的深度双向循环神经网络中, 该信息作为输入传递到下一个双向层。最后, 输出层计算得到的输出为   是输出单元的数目  :  这里, 权重矩阵  和偏置  是输出层的模型参数。事实上, 这两个方向可以拥有不同数量的隐 藏单元。\nSequence to Sequence with RNNs\n\nScreen Shot 2022-05-18 at 5.44.40 PM\n\nSequence to Sequence with RNNs\nProblem: Input sequence bottlenecked through fixed-sized vector. What ifT=1000?\nIdea: use new context vector at each step of decoder!\n使用循环神经网络编码器和循环神经网络解码器的序列到序列学习¶\n\nseq2seq\n\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"注意力机制","url":"/2022/05/06/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/","content":"\n\n草稿！！！\n\n\n1. Seq2Seq with RNNs\n\nScreen Shot 2022-05-18 at 5.44.40 PM\n\nS_0 由H_t通过全连接前馈网络学到，或者直接设置为0（Pytorch）\n使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。\n这里课件貌似写错了，最初的实现有两种形式，而课件里的图把两种都塞一起了。\n\nseq2seq\n\n机器翻译\nSequence to Sequence with RNNs\nProblem: Input sequence bottlenecked through fixed-sized vector. What ifT=1000?\nIdea: use new context vector at each step of decoder!\n2. Seq2Seq with RNNs and Attention\n\n\n\nThe decoder doesn’t use the fact that h i form an ordered sequence – it just treats them as an unordered set {h i }\nCan use similar architecture given any set of input hidden vectors {h i }!\n3. Image Captioning using spatial features\n\nProblem: Input is \"bottlenecked\" through c - Model needs to encode everything it wants to say within c This is a problem if we want to generate really long descriptions? 100s of words long\nAttention idea: New context vector at every time step. Each context vector will attend to different image regions\n4. Image Captioning with RNNs and Attention\n\nThis entire process is differentiable. model chooses its own attention weights. No attention supervision is required\n\n\nHard attention：(requires reinforcement learning)\n因为为了防止维数过高时QKTQKTQKT的值过大导致softmax函数反向传播时发生梯度消失。那为什么是dkdk而不是dkd_kdk呢？这就是个经验值，从理论上来说，就是还需要让QKTQKTQKT的值适度增加，但不能过度增加，如果是dkd_kdk的话，可能就不增加了。\n5. General attention and self-attention\nGeneral attention\n\nScreen Shot 2022-05-19 at 2.20.24 PM\n\nAttention operation is permutation invariant.\n\nDoesn't care about ordering of the features\nStretch H xW = N into N vectors（in the bottom picture，x should be 9xD instead of 3xD, but for convenience, it's shown like that)\n\nChange  to a scaled simple dot product\n\nLarger dimensions means more terms in the dot product sum.\nSo, the variance of the logits is higher. Large magnitude vectors will produce much higher logits.\nSo, the post-softmax distribution has lower-entropy, assuming logits are IID.\nUltimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others\nDivide by  to reduce effect of large magnitude vectors\n\n\nScreen Shot 2022-05-19 at 2.41.52 PM\n\nMultiple query vectors\n\neach query creates a new output context vector Multiple query vectors\n\n\nNotice that the input vectors are used for both the alignment as well as the attention calculations. - We can add more expressivity to the layer by adding a different FC layer before each of the two steps.\n\nScreen Shot 2022-05-19 at 2.04.12 PM\n\nOutputs: context vectors: y (shape:Dv)\nNow, The input dimensions  and output dimensions  can now change depending on the key and value FC layers.\nSelf attention layer\nWe can calculate the query vectors from the input vectors, therefore, defining a \"self-attention\" layer.\n\nScreen Shot 2022-05-19 at 3.14.35 PM\n\n\n\n\nScreen Shot 2022-05-19 at 3.19.53 PM\n\nProblem:\n\nPermutation equivariant\nPermutation equivariant Self-attention layer doesn’t care about the orders of the inputs!\nHow can we encode ordered sequences like language or spatially ordered image features?\n\n\nScreen Shot 2022-05-19 at 3.23.04 PM\n\n\nFrom Attention Is All You Need :\nThis work use sine and cosine functions of different frequencies:  where pos is the position and  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  to . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  can be represented as a linear function of .\nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n\n\nScreen Shot 2022-05-19 at 3.47.36 PM\n\nreferences: 详解自注意力机制中的位置编码\n\n\n\n\nScreen Shot 2022-05-19 at 4.04.01 PM\n\n\n\nScreen Shot 2022-05-19 at 4.04.19 PM\n\n\nScreen Shot 2022-05-19 at 4.09.59 PM\n\nComparing RNNs to Transformer\nRNNs:\n\n(+) LSTMs work reasonably well for long sequences.\n(-) Expects an ordered sequences of inputs\n(-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done.\n\nTransformer:\n\n(+) Good at long sequences. Each attention calculation looks at all inputs.\n(+) Can operate over unordered sets or ordered sequences with positional encodings.\n(+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel.\n(-) Requires a lot of memory:  alignment and attention scalers need to be calculated and stored\nfor a single self-attention head. (but GPUs are getting bigger and better)\n\nImage Captioning using Transformers\n\nScreen Shot 2022-05-19 at 4.17.39 PM\n\n\n\n\nResidual connection\nMLP over each vector individually LayerNorm over each vector individually\nResidual connection\nLayer norm\nAttention attends over all the vectors\nAdd positional encoding\n\nMade up of  encoder blocks. In vaswani et al. \n\n\n\n\nMost of the network is the same the transformer encoder.\nMulti-head attention block attends over the transformer encoder outputs.\nFor image captions, this is how we inject image features into the decoder.\n\n\n\nScreen Shot 2022-05-19 at 4.33.53 PM\n\n\n\nDosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020 Colab link to an implementation of vision transformers\n\n\nScreen Shot 2022-05-19 at 4.42.20 PM\n\n\nAn Image is Worth 16x16 Words\n\nScreen Shot 2022-05-19 at 4.44.04 PM\n\n\n\nScreen Shot 2022-05-19 at 4.48.25 PM\n\n\nA ConvNet for the 2020s. Liu et al. CVPR 2022\n\nScreen Shot 2022-05-19 at 4.52.49 PM\n\n\nDeiT III: Revenge of the ViT\n\nSummary\n\nAdding attention to RNNs allows them to \"attend\" to different parts of the input at every time step\nThe general attention layer is a new type of layer that can be used to design new neural network architectures\nTransformers are a type of layer that uses self-attention and layer norm.\nIt is highly scalable and highly parallelizable\nFaster training, larger models, better performance across vision and language tasks\nThey are quickly replacing RNNs, LSTMs, and may(?) even replace convolutions.\n\n终于理解到通道数的内涵了！！！！，甚至能联想到RGB\n\n\n\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"注意力机制","url":"/2022/05/06/Deep%20Learning/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","content":"\n\n草稿！！！\n\n\n1. Seq2Seq with RNNs\n\nScreen Shot 2022-05-18 at 5.44.40 PM\n\nS_0 由H_t通过全连接前馈网络学到，或者直接设置为0（Pytorch）\n使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。\n这里课件貌似写错了，最初的实现有两种形式，而课件里的图把两种都塞一起了。\n\nseq2seq\n\n机器翻译\nSequence to Sequence with RNNs\nProblem: Input sequence bottlenecked through fixed-sized vector. What ifT=1000?\nIdea: use new context vector at each step of decoder!\n2. Seq2Seq with RNNs and Attention\n\n\n\nThe decoder doesn’t use the fact that h i form an ordered sequence – it just treats them as an unordered set {h i }\nCan use similar architecture given any set of input hidden vectors {h i }!\n3. Image Captioning using spatial features\n\nProblem: Input is \"bottlenecked\" through c - Model needs to encode everything it wants to say within c This is a problem if we want to generate really long descriptions? 100s of words long\nAttention idea: New context vector at every time step. Each context vector will attend to different image regions\n4. Image Captioning with RNNs and Attention\n\nThis entire process is differentiable. model chooses its own attention weights. No attention supervision is required\n\n\nHard attention：(requires reinforcement learning)\n因为为了防止维数过高时QKTQKTQKT的值过大导致softmax函数反向传播时发生梯度消失。那为什么是dkdk而不是dkd_kdk呢？这就是个经验值，从理论上来说，就是还需要让QKTQKTQKT的值适度增加，但不能过度增加，如果是dkd_kdk的话，可能就不增加了。\n5. General attention and self-attention\nGeneral attention\n\nScreen Shot 2022-05-19 at 2.20.24 PM\n\nAttention operation is permutation invariant.\n\nDoesn't care about ordering of the features\nStretch H xW = N into N vectors（in the bottom picture，x should be 9xD instead of 3xD, but for convenience, it's shown like that)\n\nChange  to a scaled simple dot product\n\nLarger dimensions means more terms in the dot product sum.\nSo, the variance of the logits is higher. Large magnitude vectors will produce much higher logits.\nSo, the post-softmax distribution has lower-entropy, assuming logits are IID.\nUltimately, these large magnitude vectors will cause softmax to peak and assign very little weight to all others\nDivide by  to reduce effect of large magnitude vectors\n\n\nScreen Shot 2022-05-19 at 2.41.52 PM\n\nMultiple query vectors\n\neach query creates a new output context vector Multiple query vectors\n\n\nNotice that the input vectors are used for both the alignment as well as the attention calculations. - We can add more expressivity to the layer by adding a different FC layer before each of the two steps.\n\nScreen Shot 2022-05-19 at 2.04.12 PM\n\nOutputs: context vectors: y (shape:Dv)\nNow, The input dimensions  and output dimensions  can now change depending on the key and value FC layers.\nSelf attention layer\nWe can calculate the query vectors from the input vectors, therefore, defining a \"self-attention\" layer.\n\nScreen Shot 2022-05-19 at 3.14.35 PM\n\n\n\n\nScreen Shot 2022-05-19 at 3.19.53 PM\n\nProblem:\n\nPermutation equivariant\nPermutation equivariant Self-attention layer doesn’t care about the orders of the inputs!\nHow can we encode ordered sequences like language or spatially ordered image features?\n\n\nScreen Shot 2022-05-19 at 3.23.04 PM\n\n\nFrom Attention Is All You Need :\nThis work use sine and cosine functions of different frequencies:  where pos is the position and  is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from  to . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset  can be represented as a linear function of .\nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n\n\nScreen Shot 2022-05-19 at 3.47.36 PM\n\nreferences: 详解自注意力机制中的位置编码\n\n\n\n\nScreen Shot 2022-05-19 at 4.04.01 PM\n\n\n\nScreen Shot 2022-05-19 at 4.04.19 PM\n\n\nScreen Shot 2022-05-19 at 4.09.59 PM\n\nComparing RNNs to Transformer\nRNNs:\n\n(+) LSTMs work reasonably well for long sequences.\n(-) Expects an ordered sequences of inputs\n(-) Sequential computation: subsequent hidden states can only be computed after the previous ones are done.\n\nTransformer:\n\n(+) Good at long sequences. Each attention calculation looks at all inputs.\n(+) Can operate over unordered sets or ordered sequences with positional encodings.\n(+) Parallel computation: All alignment and attention scores for all inputs can be done in parallel.\n(-) Requires a lot of memory:  alignment and attention scalers need to be calculated and stored\nfor a single self-attention head. (but GPUs are getting bigger and better)\n\nImage Captioning using Transformers\n\nScreen Shot 2022-05-19 at 4.17.39 PM\n\n\n\n\nResidual connection\nMLP over each vector individually LayerNorm over each vector individually\nResidual connection\nLayer norm\nAttention attends over all the vectors\nAdd positional encoding\n\nMade up of  encoder blocks. In vaswani et al. \n\n\n\n\nMost of the network is the same the transformer encoder.\nMulti-head attention block attends over the transformer encoder outputs.\nFor image captions, this is how we inject image features into the decoder.\n\n\n\nScreen Shot 2022-05-19 at 4.33.53 PM\n\n\n\nDosovitskiy et al, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”, ArXiv 2020 Colab link to an implementation of vision transformers\n\n\nScreen Shot 2022-05-19 at 4.42.20 PM\n\n\nAn Image is Worth 16x16 Words\n\nScreen Shot 2022-05-19 at 4.44.04 PM\n\n\n\nScreen Shot 2022-05-19 at 4.48.25 PM\n\n\nA ConvNet for the 2020s. Liu et al. CVPR 2022\n\nScreen Shot 2022-05-19 at 4.52.49 PM\n\n\nDeiT III: Revenge of the ViT\n\nSummary\n\nAdding attention to RNNs allows them to \"attend\" to different parts of the input at every time step\nThe general attention layer is a new type of layer that can be used to design new neural network architectures\nTransformers are a type of layer that uses self-attention and layer norm.\nIt is highly scalable and highly parallelizable\nFaster training, larger models, better performance across vision and language tasks\nThey are quickly replacing RNNs, LSTMs, and may(?) even replace convolutions.\n\n终于理解到通道数的内涵了！！！！，甚至能联想到RGB\n\n\n\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"注意力机制","url":"/2022/05/06/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B/","content":"\n基本都提到过了\n直接放链接了\n\n\n1. 非结构化建模的挑战\n2. 使用图描述模型结构\n2.1 有向模型\n2.2 无向模型\n2.3 配分函数\n2.4 基于能量的模型\n2.5 分离和 d-分离\n2.6 在有向模型和无向模型中转换\n2.7 因子图\n3. 从图模型中采样\n4. 结构化建模的优势\n5. 学习依赖关系\n6. 推断和近似推断\n7. 结构化概率模型的深度学习方法\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"优化算法","url":"/2022/05/06/Deep%20Learning/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/","content":"\n硬核程度：★★☆☆☆☆\n建议阅读花书第九章\n在很多任务中，我们关心某些概率分布并非因为对这些概率分布本身感兴趣，精确推断方法通常需要很大的计算开销, 因此在现实应用中近似推断方法更为常用. 近似推断方法大致可分为两大类: 第一类是采样(sampling)，通过使用随机化方法完成近似; 第二类是使用确定性近似完成近似推断，典型代表为变分推断(variational inference).\n\n\n两个凸集的交集是凸的\n\n詹森不等式 给定一个凸函数 , 最有用的数学工具之一就是詹森不等式 (Jensen' s inequality)。它是凸性定义的一种推 广:  其中  是满足  的非负实数,  是随机变量。换句话说, 凸函数的期望不小于期望的凸函数, 其中后 者通常是一个更简单的表达式。为了证明第一个不等式，我们多次将凸性的定义应用于一次求和中的一项。 詹森不等式的一个常见应用: 用一个较简单的表达式约束一个较复杂的表达式。例如, 它可以应用于部分观 察到的随机变量的对数似然。具体地说, 由于 , 所以  这里,  是典型的末观察到的随机变量,  是它可能如何分布的最佳猜测,  是将  积分后的分布。例 如, 在聚类中  可能是簇标签, 而在应用簇标签时,  是生成模型。\n拉格朗⽇函数\n学习和纯优化有什么不同\n经验风险最小化\n代理损失函数和提前终止\n批量算法和小批量算法\n神经网络优化中的挑战\n病态\n局部极小值\n高原、鞍点和其他平坦区域\n悬崖和梯度爆炸\n长期依赖\n非精确梯度\n局部和全局结构间的弱对应\n优化的理论限制\n基本算法\n随机梯度下降\n动量\nNesterov 动量\n自适应学习率算法\n目前，最流行并且使用很高的优化算法包括 SGD、具动量的 SGD、RMSProp、 具动量的 RMSProp、AdaDelta 和 Adam。\nwarmup\nAdaGrad\n作为⼀个预处理器，Adagrad算法按坐标顺序的适应性是⾮ 常可取的。\n这个属性让 AdaGrad (以及其它类似的基于梯度平方的方法，如 RMSProp 和 Adam)更好地避开鞍点。 Adagrad 将采取直线路径，而梯度下降(或相关的动量)采取的方法是“让我先滑下陡峭的斜坡，然后才可能担心较慢的方向”。 有时候，原版梯度下降可能非常满足的仅仅停留在鞍点，那里两个方向的梯度都是0。\nRMSProp\nRMSProp: “Leaky AdaGrad”\n然而，AdaGrad 的问题在于它非常慢。 这是因为梯度的平方和只会增加而不会减小。 Rmsprop (Root Mean Square Propagation)通过添加衰减因子来修复这个问题。\nAdam\n推荐Adam作为默认算法，一般比RMSProp要好一点\n实际用得最多的一种算法\n深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam 这样的发展历程。Google一下就可以看到很多的教程文章，详细告诉你这些算法是如何一步一步演变而来的。在这里，我们换一个思路，用一个框架来梳理所有的优化算法，做一个更加高屋建瓴的对比。\nSGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。\nSGD, SGD+Momentum, Adagrad, RMSProp, Adam all have\nlearning rate as a hyperparameter.\nQ: Which one of these learning rates is best to use?\nA: In reality, all of these are good learning rates.\nLearning Rate Decay: Linear Warmup\n选择正确的优化算法\n奇异值分解\n矩阵求导\nKKT条件\n参数初始化策略\n二阶近似方法\n牛顿法\n共轭梯度\nBFGS\n优化策略和元算法\n批标准化\n3种！\n多种标准化！！！\n\nnormalization\n\n坐标下降\nPolyak 平均\n监督预训练\n设计有助于优化的模型\n延拓法和课程学习\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"自编码器","url":"/2022/05/06/Deep%20Learning/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/","content":"\n硬核程度：★★☆☆☆☆\n/\n\n\n这章没什么好说的，直接看【】就行了\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"Monte  Carlo  Methods","url":"/2022/05/06/Deep%20Learning/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/","content":"\n\n在很多任务中，我们关心某些概率分布并非因为对这些概率分布本身感兴趣，精确推断方法通常需要很大的计算开销, 因此在现实应用中近似推断方法更为常用. 近似推断方法大致可分为两大类: 第一类是采样(sampling)，通过使用随机化方法完成近似; 第二类是使用确定性近似完成近似推断，典型代表为变分推断(variational inference).\n西瓜书都有逻辑和概念的错误，况且西瓜书这章还是抄的PRML的（\n典型的每句话单独拎出来看可能没什么问题，但塞一块问题就大了\n\n\n1. 蒙特卡罗法\n蒙特卡罗法：当概率分布的定义已知时，通过抽样获得概率分布的随机样本，通过得到的随机样本对概率分布的特征进行分析。\n\n从样本得到经验分布，从而估计总体分布；\n或者从样本计算出样本均值，从而估计总体期望。所以蒙特卡罗法的核心是随机抽样 (random sampling)。\n\n一般的蒙特卡罗法\n\n直接抽样法\n接受-拒绝抽样法\n重要性抽样法\n\n接受-拒绝抽样法、重要性抽样法适合于概率密度函数复杂 (如密度函数含有多个变量，各变量相互不独立，密度函数形式复杂)，不能直接抽样的情况。\n1.1 随机抽样\n算法: 接受-拒绝抽样法 (accept-reject sampling method)： 输入: 已知但不可直接抽样的概率密度函数 ; 输出: 服从  的随机样本  。 参数: 样本数 \n\n选择建议分布  , 且其对任一  满足 ，其中  。\n按照建议分布  随机抽样得到样本 ，再按照均匀分布在  范围内抽样得到  。\n如果 ，则将  作为抽样结果。\n回到步骤 (2)直至得到  个随机样本, 结束。\n\n接受-拒绝法的优点是容易实现，缺点是如果  的涵盖体积占  的涵盖体积的比例很低，就会导致拒绝的比例很高，抽样效率很低。且在高维空间进行抽样时，即使  与  很接近，两者涵盖体积的差异也可能很大(与我们在三维空间的直观不同）。\n\n1.2 数学期望估计\n对，蒙特卡罗法按照概率分布  独立地抽取  个样本 ，可得函数  的样本均值   根据大数定律可知，当样本容量增大时，样本均值以概率 1 收敛于数学期望:  得数学期望的近似计算方法: \n1.3 积分计算\n蒙特卡罗法也可以用于定积分的近似计算，称为蒙特卡罗积分 (Monte Carlo integration）。取 ，把积分问题转为求期望问题，计算积分：  也就是说，任何一个函数的积分都可以表示为某一个函数的数学期望的形式。而函数的数学期望又可以通过函数的样本均值估计，即可利用样本均值来近似计算积分： \n2. 马尔可夫链蒙特卡罗法（MCMC）\n相比传统的蒙特卡罗法（接受-拒绝法、重要性抽样法等），MCMC更适合于随机变量是多元的、密度函数是非标准形式的、随机变量各分量不独立等情况。MCMC 通过构造平稳分布为  的马尔可夫链来产生样本，给定任意合理的初始状态 ，只要马尔可夫链运行时间足够长，生成样本的分布即可收敛到平稳状态 ，即有 ，(注意：连续情况下则代表： )\n2.1 马尔可夫链的一些性质\n马尔可夫过程由其转移概率  唯一定义，当满足以下两个条件时，它具有唯一的平稳分布  ：\n\n存在平稳分布：存在平稳分布 的一个充分不必要条件是满足细致平稳条件，它要求每个转换  是可逆的，即： \n平稳分布唯一：平稳分布  必须是唯一的，其由马尔可夫过程的遍历定理保证，即要求每个状态是：\n\n非周期性的：系统不会以固定的时间间隔返回相同的状态；\n正常返的： 对任意一个状态，从其他任意一个状态出发，当时间趋于无穷时，首次转移到这个状态的概率不为 0 。\n\n\n\n遍历定理：非周期，不可约且正常返的马尔可夫链，有唯一平稳分布存在。\n事实上：\n\n实践中转移概率  基本都满足上述两个条件；\n不可约：从任意状态出发，经过充分长时间可以到达任意状态；\n有限马尔可夫链没有零常返状态，即不可约的有限马尔可夫链的状态都是正常返状态；若马尔可夫链有一零常返状态，则必有无限多个零常返状态。\n\n有限状态空间情形*：\n\n若状态空间有限，则非常返状态构成的集合一定不是闭集；\n有限不可约马尔可夫链的状态都是常返状态。\n\n2.2 Metropolis-Hastings 算法\nMCMC 利用马尔可夫过程渐近地达到一个唯一的平稳分布  ，即： $ (x)p(x) , t $ 。马尔可夫链转移概率的构造至关重要，不同的构造方法将产生不同的 MCMC 算法。现需设计一个满足上述两个条件的马尔可夫过程（通过构造转移概率)，使其平稳分布  收敛为 ，即满足细致平稳条件：  通常​ 不方便直接设计，故将其写成如下乘积：  其中：提议分布  由易计算的转移概率提供，接受概率 是接受提议的概率，且： \n\n注意这有个小Trick⚠️：\n实践中经常有难计算而  容易计算的情况，例如在贝叶斯学习中常见下述等式，其中  是一个难以计算的在高纬度积分的归一化因子。此外这也说明了算法对尺度是不敏感的，假设有，即将概率密度转化为概率，将换成也不影响最终的计算结果。  现将换成，这样就在 难以计算时避开了对  的计算。由遍历定理知，对任意的初始分布  ，最后都会收敛于分布 ，即产生的样本逐渐服从于分布（收敛过程通常称为燃烧期）。\n\n\n这样就构造了细致平稳条件，下一步是选择合理的接受率，为避免选择的接受率太低导致收敛过慢，一种常见的选择是 Metropolis 选择（Metropolis-Hastings 算法）： $$\n\n$$ 此时有  或者  。\n\nMetropolis-Hastings 算法:\n输入: 分布 ，建议分布 ； 输出: 服从分布  的随机样本  参数: 收敛步数 ，总迭代步数  。 (1) 任意选择一个初始值 ，初始状态  迭代 (2) 对  循环执行 (a) 设状态 ，按照建议分布  随机抽取一个候选状态  。 (b) 计算接受概率  （c）从区间  中按均匀分布随机抽取一个数  。\n​ 如果，接受转移并设置 \n​ 如果 ，拒绝转移并设置 \n\n得到服从平稳分布 样本集合 \n\n\n\n2.3 单分量 Metropolis-Hastings 算法\n2.4 吉布斯采样\n\n吉布斯采样(Gibbs sampling)有时被视为  算法的特例, 它也使用马尔 可夫链获取样本, 而该马尔可夫链的平稳分布也是采样的目标分布 . 具体 来说, 假定 , 目标分布为 , 在初始化  的取值后, 通过 循环执行以下步骤来完成采样: (1) 随机或以某个次序选取某变量 ; (2) 根据  中除  外的变量的现有取值, 计算条件概率 , 其中  (3) 根据  对变量  采样, 用采样值代替原值.\n参考文献链接1\n\n\n\nHere is the footnote.↩︎\n\n\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"自编码器","url":"/2022/05/06/Deep%20Learning/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/","content":"\n硬核程度：★★☆☆☆☆\n/\n\n\n1. 贪心逐层无监督预训练\n1.2 何时以及为何无监督预训练有效？\n2. 迁移学习和领域自适应\n3. 半监督解释因果关系\n4. 分布式表示\n• 聚类算法，包含 k-means 算法：每个输入点恰好分配到一个类别。\n• k-最近邻算法：给定一个输入，一个或几个模板或原型样本与之关联。在 k &gt; 1 的情况下，每个输入都使用多个值来描述，但是它们不能彼此分开控制，因此 这不能算真正的分布式表示。\n• 决策树：给定输入时，只有一个叶节点（和从根到该叶节点路径上的点）是被 激活的。\n• 高斯混合体和专家混合体：模板（聚类中心）或专家关联一个激活的程度。和 k-最近邻算法一样，每个输入用多个值表示，但是这些值不能轻易地彼此分开 控制。\n• 具有高斯核 （或其他类似的局部核）的核机器：尽管每个 “支持向量’’ 或模板 样本的激活程度是连续值，但仍然会出现和高斯混合体相同的问题\n5. 得益于深度的指数增益\n6. 提供发现潜在原因的线索\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"变分推断","url":"/2022/05/06/Deep%20Learning/%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA/","content":"\n硬核程度：★★★☆☆☆\n本章节主要参考花书最后一章，此外，给花书介绍不清楚的地方添加了一些补充！！\n最为Hinton的杰作\n\n\n\n\n\n\nIsing model\n\n\n\n\n\n\n\n\n\n\nooooooo\n\nppppppp &gt;\n\n\n\n\n\n\n\n\n 也就是  这不就是 softmax 吗? 居然自然地在统计力学分布里面出现了（难怪之前 LeCun 让大家学物理）。 为了再次简化, 我们定义 , 于是就有  (因为这时候公式里面只有一个s, 就没有必要写下标了) 下面问题来了,  是什么?  又应该是什么? Hinton 看了看神经网络的一层, 其分为可见层 (输入层) 和隐含层 (中间层)。按照经典网络的定义, 神 经元有激活和末激活两个状态。那么干脆让  等于可见层  并上隐含层  神经元的状态吧（默认都用向量 的方式表示）: 于是，\n1. 玻尔兹曼机\n玻尔兹曼机最初作为一种广义的 “联结主义\" 引人，用来学习二值向量上的任意概率分布。玻尔兹曼机的变体（包含其他类型的变量 ）早已超过了原始玻尔兹曼机的流行程度。在本节中，我们简要介绍二值玻尔兹曼机并讨论训练模型和进 行推断时出现的问题。\n我们在  维二值随机向量  上定义玻尔兹曼机。玻尔兹曼机是一种基于能量的模型 (第  节 ), 意味着我们可以使用能量函数定义联合概率分布:  其中  是能量函数,  是确保  的配分函数。这是一个玻尔兹曼分布䍒, 其中归一化因子  为系统所有可能状态的能量之和, 即  玻尔兹曼机的能量函数如下给出 :  ---\nx看成速度，在格外加上一个偏置项，就成了玻尔兹曼里的能量函数，它常用语统计物理里描述物质的能量状态，能量正比于v的平方。对于\"理想气体\"（由基态的非相互作用原子所组成）的情况, 所有能量都是动能的形式。宏观粒子的动能与动量的关系为： \n\n其中  是模型参数的 “权重”\" 矩阵,  是偏置向量。\n\n在一般设定下，给定一组训练样本，每个样本都是  维的。式 (1) 描述了观察到的变量的联合概率分布。虽然这种情况显然可行，但它限制了观察到的变量和权重矩阵描述的变量之间相互作用的类型。具体来说，这意味着一个单元的概率由其他单元值的线性模型（逻辑回归）给出。\n当不是所有变量都能被观察到时, 玻尔兹曼机变得更强大。在这种情况下，潜变量类似于多层感知机中的隐藏单元，并模拟可见单元之间的高阶交互。正如添加隐藏单元将逻辑回归转换为 MLP, 导致 MLP 成为函数的万能近似器, 具有隐藏单元的玻尔兹曼机不再局限于建模变量之间的线性关系。相反，玻尔兹曼机变成了离散变量上概率质量函数的万能近似器 (Le Roux and Bengio, 2008)。\n正式地，我们将单元  分解为两个子集：可见单元  和潜在（或隐藏）单元 。 则能量函数（2）变为  玻尔兹曼机的学习玻尔兹曼机的学习算法通常基于最大似然。所有玻尔兹曼机都具有难以处理的配分函数因此，最大似然梯度必须使用第十八章中的技术来近似。\n\n\n玻尔兹曼机有一个有趣的性质，当基于最大似然的学习规则训练时，连接两个单元的特定权重的更新仅取决于这两个单元在不同分布下收集的统计信息:  和  。网络的其余部分参与塑造这些统计信息，但权重可以在完全不知道网络其余部分或这些统计信息如何产生的情况下更新。这意味着学习规则是 “局部” 的，这使得玻尔兹曼机的学习似乎在某种程度上是生物学合理的。我们可以设想每个神经元都是玻尔兹曼机中随机变量的情况，那么连接两个随机变量的轴突和树突只能通过观察与它们物理上实际接触细胞的激发模式来学习。特别地，正相期间，经常同时激活的两个单元之间的连接会被加强。这是 Hebbian 学习规则  的一个例子，经常总结为好记的短语_— \"fire together, wire together\"。 Hebbian 学习规则是生物系统学习中最古老的假设性解释之一，直至今天仍然有重大意义 (Giudice et al., 2009)。 不仅仅使用局部统计信息的其他学习算法似乎需要假设更多的学习机制。例如，对于大脑在多层感知机中实现的反向传播, 似乎需要维持一个辅助通信的网络，并借此向后传输梯度信息。已经有学者 (Hinton, 2007a; Bengio, 2015) 提出生物学上可行（和近似）的反向传播实现方案，但仍然有待验证，Bengio (2015) 还将梯度的反向传播关联到类似于玻尔兹曼机（但具有连续潜变量）的能量模型中的推断。\n\n就像有一堆磁铁，每个用钉子固定在一个盘子上，给予一个扰动，就会稳定到一个状态，这个状态与每个磁铁的位置个互相磁力的大小有关。如果固定住一部分磁铁，最后稳定的状态就会有所不同。而训练就是调整这些磁铁的位置和磁力，使得当固定的磁铁符合某种模式时，最后稳定的磁铁也会符合另一种模式。\n统计物理的玻尔兹曼分布在统计力学与数学中，玻尔兹曼分布（或称吉布斯分布[1]）是系统中的粒子在各种可能微观量子态（英语：microstate (statistical mechanics)）的概率分布、概率测度，或频度分布（英语：frequency distribution）。具有以下形式  其中  是量子态能量（随着个别量子态有所不同）,  (对于一个玻尔兹曼分布来说是常数）是玻尔兹曼常数与热力学温度的乘积。而概率分布则可表达为：  其中  是量子态  的概率,  是量子态的能量,  是玻尔兹曼常数,  是系统温度且  为系统具有的量子态数目。对于两个状态之玻尔兹曼分布的比值, 得到玻尔兹曼因子。可见其仅与量子态间的 能量差有关。  玻尔兹曼分布取自路德维希·玻尔兹曼，他在1868年研究热平衡气体的统计力学时初次构想了此一分布。而后约西亚·威拉德·吉布斯在1902年提出了玻尔兹曼分布更为一般化的形式。[4]:Ch.IV要特别的注意玻尔兹曼分布与麦克斯韦-玻尔兹曼分布的差别。前者给出粒子在各量子态的分布概率，后者则是用来描述粒子在理想气体中的速率分布。[3]\n麦克斯韦-玻尔兹曼分布是一个描述一定温度下微观粒子运动速度的概率分布，在物理学和化学中有应用。最常见的应用是统计力学的领域。对于\"理想气体\"（由基态的非相互作用原子所组成）的情况, 所有能量都是动能的形式。宏观粒子的动能与动量的关系为：  可见玻尔兹曼机的能量函数，在U为对角阵，偏置项为零的时候，能表示成动能的形式即：\n速率的分布 通常, 我们更感兴趣于分子的速率, 而不是它们的速度分量。麦克斯韦-玻尔兹曼速率 分布为:  其中速率  定义为:  注意: 在这个方程中,  的单位是概率每速率, 或仅仅是速率的倒数, 如右图那样。 由于速率是三个独立、呈正态分布的速度分量的平方之和的平方根, 因此这个分布是麦 克斯韦-玻尔兹曼分布。\n 由此，就把玻尔兹曼分布与神经网络联系了起来\n启发于\n\n2. 受限玻尔兹曼机\n2.1 RBM推导\n这种全连接的玻尔兹曼机模型，理论上可以用来描述样本特征中很复杂的慨率关系，但估计其参数非常困难。l986年，Smolensky提出了一种称作和谐网络（harmony network或harmoniums）.的模型，对用它描述高维数据进行了深入的理论研究②。和谐模型把玻尔兹曼机中的可见节点称作表示节点，隐节点称作知识节点，它们之间有完全的互相连接，但可见节点之间不连接，隐节点之间也不连接。这样，模型就比玻尔兹曼机大大简化，在给定可见节点即训练样本的情况下，隐节点之间条件独立。Hinton等对和谐模型进行了系统的发展，并称之为限制性玻尔兹曼机（restricted Boltzmann machines，RBM），意指这个模型是把玻尔兹曼机中的连接限制在可见层和隐层之间，取消了可见层和隐层各自内部的连接，如图12-21所示。这就是在机器学习中有重要影响的RBM模型。\n\n由于弓人了对同层节点间没有连接的限制, RBM 模型的能量函数形式比式 (3)得到了很大简化, 成为  ---\n就像普通的玻尔兹曼机，受限玻尔兹曼机也是基于能量的模型，其联合概率分布由能量函数指定:   的能量函数由下给出  其中  是被称为配分函数的归一化常数:  虽然  难解，但 RBM 的二分图结构具有非常特殊的性质，其条件分布  和  是因子的, 并且计算和采样是相对简单的。从联合分布中导出条件分布是直观的:  由于我们相对可见单元  计算条件概率, 相对于分布  我们可以将它们视为 常数。条件分布  因子相乘的本质, 我们可以将向量  上的联合概率写成单 独元素  上（末归一化 ）分布的乘积。现在原问题变成了对单个二值  上的分布 进行归一化的简单问题。  现在我们可以将关于隐藏层的完全条件分布表达为因子形式：  类似的推导将显示我们感兴趣的另一条件分布，  也是因子形式的分布: \n\n我们把限制性玻尔兹曼机即 RBM 网络的模型完整地描述如下: 一个 RBM 网络，是由两部分二值随机节点和它们之间的连接组成的无向二分图。输入节点即可见节点记作 , 隐层节点记作 , 它们的联合概率为  其中  为归一化因子, 也称作配分函数 (partition function);   为对应于输人节点  和隐节点  的能量, 定义为  其中的参数为  为隐节点与输人节点之间的连接权值矩阵,  分别为输人 节点和隐节点的偏置参数。\nRBM 模型可以用来描述复杂数据内部的概率关系。在确定了模型参数的情况下, 给定 输人数据, 可以用模型得到最大概率的隐节点状态; 反之, 给定隐节点状态, 可以产生最大 可能的可见节点值, 即产生最可能的样本。因此, RBM 模型是一种生成模型 (generative model), 它试图通过模型结构和参数来反映样本数据背后的生成规律。 把式 (12-22) 代人到式 (12-20) 中, 可得输人节点和隐节点的联合概率密度函数为  其中,  和  分别表示隐层第  个节点和输人层第  个节点。 由于  的概率可以被分解成多个子函数的乘积, 每个子函数对应一个隐节点, 这种模 型早期被称作“专家乘积” (product of experts)模型, 每个子函数看作是对  进行计算的一个专家, 和谐网络模型中称之为对样本的一种知识。根据式 (12-23), 可以得到给定  下  的条件概率  其中,  是归一化因子, . 表示权值矩阵中第  个行向量。 由于  的元素为二值节点, 我们可以方便地对每个隐节点的概率进行归一化:  即  同理可得  这里  为 Sigmoid 函数。 可以把式 (12-26) 和式 (12-27) 合并到一个统一的公式中, 即  类似地, 我们也可以得到给定  下  的条件概率为:  在已知模型参数的情况下, 利用式 (12-28)、式 (12-29) 这两个条件概率, 在给定样本下, 可以用采样的办法得到隐节点向量值的实例; 在给定隐节点向量情况下, 可以采样得到生 成样本实例。\n在第 3 章中我们介绍了楖率密度函数的基本方法,在已知概率密度函数形式的情况下, 可以采用最大似然法根据已知样本对概率密度函数进行参数估计。第 4 章中我们介绍了隐马尔可夫模型和贝叶斯网络这种比较复杂的概率模型的参数学习方法。  模型也是一 种概率模型, 我们同样可以用样本来估计 RBM 模型的参数, 这就是 RBM 网络的学习, 或者 称作对 RBM 网络的训练。 在 RBM 模型中, 式 (12-23)给出了式见节点与隐节点的联合概率密度函数,样本  出 现的概率是对所有可能隐节点冯态求  的边缘分布, 即  其中,  代表模型中的全部参数。 如果模型参数已知, 可以用式(12-30)计算任一个样本出现的概率。在用训练样本对模\n\n记得说清楚RBM里的一些巧合之处\n没想到把，这是不是和sigmoid函数特别像！！！！！！！\n这绝对不是巧合！！！\n\n2.2 训练RBM\n可腊的是, RBM 网络的似然函数形式复杂，无法直接求最优，需要迭代求解最优参数。为了与其他机器学习方法最小化损失函数的做法相一致，我们把式 (12-31)最大化似然函数的目标等价地写成最小化负对数似然函数的目标，即  其中, 为简化起见我们扡  写成  。仍然采用梯度下降法的基本思路, 我们求负对数似然函数对参数的梯度： \n要最小化负对数似然函数，就是要沿这个梯度的负方向对参数进行调整。 式（12-34）的梯度可以从直观上进行认识：公式右边的两项都是能量对参数的梯度在某个分布上的期望。能量梯度方向是能量增大即概率减小的方向，其负方向就是概率增大的方向。式（1234）右边第一项是能量梯度在训练样本对应状态上的期望，沿它的负方向调整参数，将使模型对训练样本的似然度增大；右边第二项是能量梯度对所有可能样本和所有可能状态的期望，也就是梯度在整个问题空间中的期望，沿这个梯度的反方向调整参数，将使模型对空间中任意样本的似然度都最大。右边第二项在总梯度中是负项，雕当我们沿总梯度的负方向调整参数时，第一项的存在是使模型向增加训练样本似然度的方向调整，第 二项的存在是使模型的调整朝向减小对空间中任意样本的似然度。 把（12-19）代入式（1234）中，可得：在训练时刻 , 训练样本为 , 目标函数对各个参数的梯度分别为 $$\n\n$$ 这里,我们把期望  简写为 , 把期望  简写为  。\n有了目标函数对这三组参数的梯度方向, 在训练中䋦一步按它们的负方向调整参数, 即 可完成 RBM 模型的参数学习。\nRBM 训练的 CD 算法 式 (12-35)给比的梯度并不容易计算，因为其中需要求两个期望值。其中,  可以在 当前数据  下根据式 (12-28) 的条件概率  求得，但如何估计模型对所有可能样本和可能欧态的期望  。\n\n这个问题被称作是不可解的配分函数 (intractable partition function) 问题，无法求到解析解。求解这类问题的基本方法是，不试图去求期望，而是通过随机抽样获得适当的点估计，用点估计来替代期望。方法可以参考我有关马尔可夫链蒙特卡洛 (MCMC) 和 吉布斯采样 (Gibbs sampling) 的博客：博客\n\n ()是在当前样本下对隐节点状态分布求期望，我们可以用 (12-28)的条佯概率生成 一个最大概率的 , 即用当前训练样本下最可能的隐节点羽态向量作为对  的期望。如果 用同样的思路井单个样本估计期望  (),我们需要得到该问题空间中“有代表性的”可见 节点和隐节点状态, 根据吉布斯采样的相关理论, 可以用下述蒙特卡洛迷代的方式来对状态 空间进行采样。\n如图 12-22 所示, 首先把当前时刻训练样本  作为 RMB网络的输人向量, 记作 , 获得当前模型下最可能的隐节点向量 , 用  根据式 (12-29) 获得当前隐节点下最可 能的可见节点向量 , 再用  获得 , 以此类推。当按照最大条件概率进行无穷多次这样 的交替采样后, 就可以得到能够代表系统平衡状态的  组合, 可以用它计算对期望的估计。 在实际问题中, 要通过交替采样达到平衡态, 需要非常庞大的计算, 而且也很难判断是 否达到了平衡态。因此人们常常设定一个交替采样的次数 , 用进行  步交替采样后得到的 可见节点和隐节点状态  上的值作为对期望  ()的估计。令人惊奇的是, 在 RBM 网络 的学习中, 只需  即可得到比较理想的效果, 即只进行一轮交替采样, 用  和  作为对训练集样本之外可能样本和状态的代表。代人到式(12-35)中, 权值的更新方向就成为  于是, 在 RBM 训练的  时刻, 网络全部权值和偏置参数的学习规则就是 \n 其中,  为学习的步长。 应该说明, 经过上述儿次简化估计后, 这里的参数学习规则已经不是严格按照负对数似 然函数下降的梯度方向进行寻优, 但实验证明这种学习规则可以得到很好的效果。 上述学习算法被称为 contrastive divergence 算法即 CD 算法, 有人译作对比分跂算法、 对比差异算法、对比发散算法等。关于这一算法的理论性质有很多研究 𝟙, 本书在此不作深 人讨论。 我们可以以权值更新规则式 (12-37) 为例对算法给出一个直观的解释: 在每一步学习 中, 除当前的真实训练样本外, 再用一步或  步采样获得一个偏离真实样本的假想样本。 用真实样本及与之最可能配对的隐节点状态作为正例, 用假想样本及与之最可能配对的隐 节点状态作为反例, 用正例内积和反例内积的差作为权值更新的方向。 进一步看, 由于可见节点和隐节点都是二值节点, 两个向量的内积只有在对应元素同时 为 1 时为 。 算法的学习过程, 就是不断加强真实样本中同时为 1 的可见节点与隐节点 之间的连接权值, 而不断削弱假想样本中同时为 1 的可见节点与隐节点之间的权值。这与 Hebb 学习律的基本思想是一致的。\n应该说明, 经过上述儿次简化估计后, 这里的参数学习规则已经不是严格按照负对数似 然函数下降的梯度方向进行寻优, 但实验证明这种学习规则可以得到很好的效果。 上述学习算法被称为 contrastive divergence算法即  算法, 有人译作对比分値算法、 对比差异算法、对比发散算法等。关于这一算.法的理论性质有很多研究 , 本书在此不作深 人讨论。 我们可以以权值更新规则式 (12-37) 为例对算法给出一个直观的解释：在每一步学习 中, 除当前的真实训练样本外, 再用一步或  步采样获得一个偏离真实样本的假想样本。 用真实样本及与之最可能配对的隐节点状态作为正例, 用假想样本及与之最可能配对的隐 节点状态作为反例, 用正例内积和反例内积的差作为权值更新的方向。 进一步看, 由于可见节点和隐节点都是二值节点,两个向量的内积只有在对应元素同时 为 1 时为 。 算法的学习过程, 就是不断加强真实样本中同时为 1 的可见节点与隐节点. 之间的连接权值, 而不断削弱仳想样本中同时为 1 的可见节点与隐节点之间的权值。这与 Hebb 学习律的基本思想是一致的。\n关于 RBM 模型的 CD算法, 是求解具有不可解配分函数的似然函数优化问题的一种通 用的近似算法, 文亁中有很多研究。如果在算法中采样阶段不是只进行一轮交替采样, 而是 进行  轮, 则算法被称作 CD-k 算法。人们也发展了一些某方面具有更好性能的改进的  算法和替代算法。\nRBM 模型为描述样本特征背后复杂的概率模型提供了一个有效的框架。RBM 隐节点 状态可以看作是对样本的特征提取。我们可以利用隐节点提取的特征对样本进行模式识别 和预测等进一步的分析应用。 上面介绍的基本的 RBM 模型针对的是二值输人向量。RBM 也可以扩展到取值为实 数向量或多值向量的样本, 这时需要改变 RBM 模型中的能量函数形式,但模型基本原理柏 学习算法与二值向量情况下类似, 我们在本书中不再展开讨论。 利用这些模型, RBM 在图像和自然语言等的模式识别问题上取得了很好的应用。\n除了把隐层节点作为对样本提取的内在特征用于后续分类或预测外，RBM网络也有很 多其他的应用方式。例如它较早就被用到预测观众对电影的评级等应用中，了解这一应用 的思路对直观理解RBM的原理非常有帮助。 假设我们有100部电影，并收集了1万名观众对这些电影的喜好。为简单起见，把喜好简化为1或0两个值，1表示喜欢而0表示不喜欢。每位观众对这些电影的评价构成一个100维二值向量，1万名观众就提供了1万个这种向量的样本。我们把这100维向量输入到 可见节点，构造一个包含一定数目隐节点的RBM网络，用这些样本对网络进行训练，就可 以得到观众对这100部电影喜好情况的概率模型。 如果训练样本中存在缺失值，即并非所有观众都看过全部电影，我们仍然可以用带有缺 失值的样本集合对RBM网络进行训练，只是在训练时如果遇到缺失值，则不改变与缺失值 对应的权值和参数。经过充分训练之后，倾向于被观众同时喜欢的电影，会倾向于在RBM 网络中与一个或几个共同的隐节点形成较强的连接，而隐节点往往捕提了被共同喜欢的电影中可能存在的某些内在共性，如相同的电影类型、相同导演、某些相同演员、相同的时代背景，等等。 当面对一位新的观众时，我们可以了解她看过这些电影中的哪些作品和对这些作品的 喜好，用她的不完整的喜好向量输人到RBM网络中，可以得到概率最大的隐节点向量状态 值，用隐节点状态值再返回来对输人向量中的缺失值给出最大可能的估计，从而可以向这位观众推荐她最可能会喜欢的电影。 这只是一个简化了的思路介绍。Hinton等设计了以对电影的排序评价作为可见节点 输入的RBM模型，每个可见节点是一个代表了对前几名电影排序的向量，每个隐节点仍然 为二值，取得了很好的效果①。\n2.3 RBM用于智能推荐\n3. 深度信念网络（DBN）\n逐层训练！！！\n逐层训练！！！\n深度信念网络（deep belief network, DBN ）是第一批成功应用深度架构训练的 非卷积模型之一 (Hinton et al., 2006a; Hinton, 2007b)。2006 年深度信念网络的引人 开始了当前深度学习的复兴。在引人深度信念网络之前, 深度模型被认为太难以优 化。具有凸目标函数的核机器引领了研究前沿。深度信念网络在 MNIST 数据集上表 现超过内核化支持向量机，以此证明深度架构是能够成功的 (Hinton et al., 2006a)。 尽管现在与其他无监督或生成学习算法相比, 深度信念网络大多已经失去了青睐并 很少使用, 但它们在深度学习历史中的重要作用仍应该得到承认。 深度信念网络是具有若干潜变量层的生成模型。潜变量通常是二值的, 而可见 单元可以是二值或实数。尽管构造连接比较稀疏的 DBN 是可能的, 但在一般的模型 中, 每层的每个单元连接到每个相邻层中的每个单元（没有层内连接 )。顶部两层之 间的连接是无向的。而所有其他层之间的连接是有向的, 箭头指向最接近数据的层。 见图  的例子。 具有  个隐藏层的  包含  个权重矩阵:  。同时也包含  个偏置向量: , 其中  是可见层的偏置。  表示的概率分布由下式 给出:  在实值可见单元的情况下, 替换  为便于处理,  为对角形式。至少在理论上, 推广到其他指数族的可见单元是直观 的。只有一个隐藏层的  只是一个 \n为了从 DBN 中生成样本, 我们先在顶部的两个隐藏层上运行几个 Gibbs 采 样步骤。这个阶段主要从 RBM（由顶部两个隐藏层定义 ）中采一个样本。然后, 我 们可以对模型的其余部分使用单次原始采样, 以从可见单元绘制样本。 深度信念网络引发许多与有向模型和无向模型同时相关的问题。 由于每个有向层内的相消解释效应, 并且由于无向连接的两个隐藏层之间的相 互作用, 深度信念网络中的推断是难解的。评估或最大化对数似然的标准证据下界也 是难以处理的, 因为证据下界基于大小等于网络宽度的团的期望。 评估或最大化对数似然, 不仅需要面对边缘化潜变量时难以处理的推断问题, 而 且还需要处理顶部两层无向模型内难处理的配分函数问题。 为训练深度信念网络, 我们可以先使用对比散度或随机最大似然方法训 练  以最大化  。  的参数定义了  第一层的参数。 然后, 第二个  训练为近似最大化  其中  是第一个  表示的概率分布,  是第二个  表示的概率分布。 换句话说, 第二个  被训练为模拟由第一个  的隐藏单元采样定义的分布, 而第一个  由数据驱动。这个过程能无限重复, 从而向  添加任意多层, 其 中每个新的  对前一个  的样本建模。每个  定义  的另一层。这 个过程可以被视为提高数据在 DBN 下似然概率的变分下界 (Hinton et al., 2006a)。 在大多数应用中, 对 DBN 进行贪心逐层训练后, 不需要再花功夫对其进行联合 训练。然而, 使用醒眠算法对其进行生成精调是可能的。 训练好的  可以直接用作生成模型, 但是  的大多数兴趣来自于它们改 进分类模型的能力。我们可以从 DBN 获取权重, 并使用它们定义 MLP:  利用 DBN 的生成训练后获得的权重和偏置初始化该 MLP 之后, 我们可以训练 该 MLP 来执行分类任务。这种 MLP 的额外训练是判别性精调的示例。 与第十九章中从基本原理导出的许多推断方程相比, 这种特定选择的 MLP 有 些随意。这个 MLP 是一个启发式选择, 似乎在实践中效果不错, 并在文献中一贯使 用。许多近似推断技术是由它们在一些约束下，并在对数似然上找到最大紧变分下\n\n器模型类似，这种网络的更大应用是把得到的生成模型的最顶端节点作为对样本新的特征表示，用这种表示进行后续的分类或预测任务。例如，我们可以把圳练得到的深度信念网络 看作是一个具有同样结构的多层感知器，把DB的训练看作是对多层感知器的非监督预训练，即把DBN的权值作为多层感知器权值的初始值，再用监督学习的分类或预测目标去 对网络进行再训练。与传统的多层感知器相比，通过这种方法可以构造层数很多的多层感知器，由于采用了有效的预训练，这种深度的多层感知器可以更好地进行训练，较好地解决了多层感知器层数增加后由于参数搜索空间急剧增大而带来的训练困难的问题。与卷积神经网络相比，深度信念网络没有对深度的多层感知器采用针对输人样本局部 特性的特殊设计，是一种更普适的深度神经网络模型。深度信念网络（DBN）是第一个非卷 积的深度神经网络模型，也是人们接受并广泛使用“深度学习”这个概念的开始。\n\n&gt;这是一个块引用！\n4. 深度玻尔兹曼机\n","categories":["Deep Learning"],"tags":["Deep Learning","CNN"]},{"title":"统计学习理论概要","url":"/2022/04/30/Pattern%20Recognition/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E6%A6%82%E8%A6%81/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E6%A6%82%E8%A6%81/","content":"\nThe project will be configured before the first of June\n\n\n统计学习理论概要\n机器学习问题可以形式化地表示为：已知变量  与输人  之间存在一定的末知依赖关系，即存在一个末知的联合概率密度函数 , 机器学习就是根据 ​ 个独立同分布观测样本  在侯选函数集  中求一个最优的函数 ，使它组出的预测的期望风险最小。 \n经验风险为在训练样本上损失函数的平均，历史上大部分机器学习方法实际上都是用最小化经验风险来替代最小化期望风险的目标。  实际上, 以期望风险最小为目标来分析经验风险最小化原则的做法，合理性并没有充分的理论保证。 和  都是  的泛函，概率论中的大数定律只说明了随机变量的 均值在样本倾向于无穷大时会收敛于其期望，但这个定律对泛函是否仍然成立? 这一点在当时并没有数学上的结论。其次，即使我们类比随机变量的情况认为  在样本倾向于无穷大时会充分接近 ，这并不是我们需要的结果。我们需要的是  在  个样本上取得极小值的解  收敛于使  取得最小值的解  。通常，两个函数充分接近并不能保证它们的极值点也充分接近，这里实际上提垻来一个更基本的问题: 所谓用  近似  或当样本趋向 无穷多时  收敛于 , 应该用什么来作为两个函数接近程度的度量？再次，即使我们有办法证明或通过一定条件保证在样本趋向于无穷多时，使经验风险最小的解也使期望风险最小，在实际问题中需要多少样本才能达到接近无穷多的效果? 如果样本远非无穷多而是非常有限，经验风险最小化是否还可行? 得到的解是否还有推广能力? 在机器学习领域多数研究者都将注意力集中到如何更好地求解最小经验风险问题上时，理论分析学派则对用经验风险最小化原则的上述问题进行了系统深人的研究，得到了一套完整的理论，这就是本章将要慨要介绍的统计学习理论。 统计学习理论包括以下四分内容： （1）经验风险最小化学习过程一致的概念及充分必要条件。它回答了在样本趋向于无穷多的情况下，什么样的函数集可以采样经验风险最小化原则进行学习。 （2）采样经验风险最小化的学习过程，随着样本数目增加的收敛速度有多快。 （3）如何控制学习过程的收敛速度，有限样本下机器学习的结构风险最小化原则。 （4）在结构风险最小化原则下设计机器学习算法，包括支持向量机推广能力的理论依据。\n1. 结构风险最小化\n2. 学习过程中的一致性\n所谓一致性，是指对于函数集  和联合概率密度函数 ， 以下两个序列在样㭘趋向于无穷多时依概率收敛到同一个极限;  为保证一致性不是由于函数集中的个别函数导致的，统计学习理论提出了非平凡一致性 (nontrivial consistency)的概念, 即要求上式对函数集的所有子集都成立。 只有非平凡一致性才是实际上有意义的。传统上要判断一个函数与另一个函数充分接近，常用准则：  统计学习理论指明，在上式的意义下经验风险收敛于期望风险，仍无法保证学习过程具有一致性。学习过程一致性需要满足：对于有界的㧹失函数，经验风险最小化学习一致的充分必要条件是，经验风险在如下意义上一致地收敛于真实风险；  可以看到，经验风险最小化学习是否具有一致性，不是取决于平均情况，而是取决于最坏情况。统计学习理论是最坏情况分析。可以獬读为，如果一个机器学习方法在最坏猜况下仍能表现良好，则我们对它的推广能力才有信心。显然，这种思路有些偏悲观，所以有人指出统计学习理论对学习机器推广性的判断偏保守。\n3. 函数容量与VC维\n3.1 函数容量\n为了研究函数集在经验风险最小化原则下的学习一致性问题和一致性收敛的速度，统计学习理论定义了一系列有关函数集学习侏能的指标，这些指标多是从两类分类函数 (即指示函数)提出的，后又推广到一般函数。现只针对指示函数集进行讨论： 统计学习理论用函数集在一组样本集上可能实现的分类方案数来度量函数集的容量，把这个容量的对数在符合同一分布的样本集上的期望称作函数集的熵，而把容量对数在所有可能样本集上的上界定义为函数集的生长函数 (growth function)，记作 。其反映了函数集在所有可能的  个样本上的最大能力或容量。显然  。关于学习过程的一致性，有结论: 【定理】函数集学习过程一致收敛的充分必要条件是, 对任意的样本分布, 都有  而且, 这时学习过程收敛速度一定是快的, 也就是满足  其中,  是常数。 直观上理解，这个定理说明一个采用经验风险最小化原则的学习过程要一致, 函数集的能力不能随着样本数无限增长。经验上，人们知道机器学习模型的复杂程度要与样本数目相适应，过于复杂的分类模型容易导致过学习，这个定理给出了其理论依据。\n3.2 VC维\n指示函数集的VC维：1968 年, Vapnik 和 Chervonenkis 发现了生长函数的一个重要规律，即一个函数集的生长函数，如果不是一直满足 ， 则一定在样本数增加到某个值  时有：  这个特殊的样本数  被定义为函数集的 VC 维。如果这个值是无穷大，即不论样本数多大，总有 ， 则称函数集的  维为无穷大。直观理解，函数集的 VC维度量了当样本数目增加到多少之后函数集的能力就不会继续跟随样本数等比例增长。因此VC 维有限是学习过程一致侏的充分必要条件，而且这时学习过程是比较快的。生长函数和  维为我们选择什么样的函数集来设计学习机器提供了原理上的指导，但这两个度量都不直观，Vapnik 和 Chervonenkis 又为 VC 维给出了下面的直观定义： 假如一个有  个样本的样本集能被一个函数集中的函数按照所有可能的  种形式分为两类，则称函数集能把样本数为 的样本集打散。指示函数集的VC维，就是用这个函数集中的函数所能够打散的最大样本集的样本数目。 在指示函数集的VC维的基础上，可以定义一般实值函数集的VC维，其基本思想是通过一系列阈值把实值函数集转化为指示函数集，所以VC维的理论不但适用于机器学习中的分类问题，也适合于实函数映射的机器学习。\n4. 推广能力的界与结构风险最小化原则\n【定理】对于两类分类问题，对指示函数集中的所有函数 (当然也包括使经验风险最小的函数)，经验风险和实际风险之间至少以概率  满足如下关系:  当函数集中包含无穷多个元素时，  可进一步简写为  其中,  是样本数  的单调减函数、VC 维  的单调增函数。而当函数集中包含有限  个元素时：  设计一个机器学习模型就意味着选择了一定的函数集，用样本训练的过程是寻求经验风险  最小化。一个学习机器的推广能力不是取决于经验风险最小能有多小，而是在于期望风险与经验风险有多大差距，这个差距越小则推广能力越好。所以上面反映的期望风险与经验风险差距的上界被称作推广性的界。\n进一步分析可以发现，当  较小时 (如小于 20 )，置信范围  较大，用经验风险最小化取得的最优解可能会有较大的期望风险，即可能推广性差; 如果群本数较多， 较大，则置信范围就会很小，经验风险最小化的最优解就接近实际的最优解。\n另一方面，对于一个特定的问题，样本数  是固定的，此时学习机器的 VC 维越高 (即复杂性越高)，则置信范围就越大, 导致真实风险与经验风险之间可能的差就越大, 推广能力就可能越差。人们在实验中认识到对于有限样本应该尽可能选肼相对简单的分类器, 其背后 的原因就在于此。因此, 在设计分类器时, 我们不但要考虑函数集中的函数是否能使经验风 险有效减小,还要使函数集的 VC 维尽量小, 从而缩小置信范围, 以期获得尽可能好的推广 能力。\n需要指出的是, 如学习理论关键定理一样, 推广性的界也是对最坏情况的结论, 所给出 的界在很多情况下是很松的, 尤其当 VC 维比较高时更是如此。而且, 这利界主往只在对同 一类学习函数进行比较时是有效的, 可以指导我们从函数集中选择最优的函数, 但在不同函数集之间比较却不一定成立, 因为界的松紧程度可能有较大差别。\n既然学习的目标是最小化式期望风险上界，有没有可能直接根据这个原则来 设计学习机器, 而不是进行上述的试错性设计?  维是函数集的性质而并非单个函数的性质，因此式右边的两项并无法直接通过优化算法来最小化。统计学习理论提出了一种一般性的策略来解决这个问题, 做法是：\n首先把函数集  分解为一个函数子集序列 (或叫子集结构)  使各个子集能够按照置信范围  的大小排列, 也就是按照  维的大小悱列, 即  在划分了这样的函数子集结构后, 学习的目标就变成在函数集中同时进行子集的选择和子集中最优函数的选择。选捀最小经验风险与置信范围之和最小的子集，就可以达到期望风险的最小，这个子集中使经验风险最小的函数就是要求的最优函数。这种思想称作结构风险最小化(structural risk minimization), 简称 SRM 原则。 一个合理的函数子集结构应满足两个基本条件:一是每个子集的 VC 维是有限的且满足式 (7-24) 的关系; 二是每个子集中的函数对应的损失函数或是有界的非负函数，或是无界但能量有限的函数。这样的函数子集结构被称作容许结构。\n统计学习理论的一个基本结论是，在有限样本下，设计和训练学习机器不应该采用经验风险最小化原则，而应该采用结构风险最小化原则。对于多层感知器神经网络，不同的网络结构对应着不同的VC维，因此，可以按隐节点数目把多层感知器实现的函数集划分为若干个子集。结构风险最小化机器学习就是要在这一系列结构中选择能使经验风险和置信范围之和最小的函数子集和其中的函数。另外，神经网络反向传播算法中对权值加约束以改善网络学习性能的做法，也等价于由权值正则化项引人的函数子集结构，通过正则化目标函数实现结构风险最小化。\n5. 支持向量机的理论分析\n根据结构风险最小化原则，学习机器需要同时最小化经验风险和取得经验风险最小的函数子集的置信范围。支持向量机从线性可分这样的最简单情况人手来实现这个日标。 对于样本集线性可分的情况, 存在很多线性判别函数能够得到零经验风险, 这䄿情况 下, 仆么样的判别函数具有最好的推广能力, 取决于判别函数来自什么样的函数子集。支持 向量机用分类间隔对函数集进行子集的划分, 其依据是下面的关于分类闾隔与 VC 维关系的理论。\n我们先来定义  间隔超平面的概念:  维空间中权值归一化的超平面  如果它把样本用以下的形式分开  则称为  间隔超平面  。具有间隔  的超平面构成函数子集，它的  维有下而的界：\n【定理】设样本集中在空间中属于一个半径为  的超球范围内,  间隔超平面的 VC 维  满珀  其中 [] 为取整。 前面我们看到,  维空间中不加约束的线生函数集的  维是 , 而对于间隔为  的线性函数子集来说, 如果这个间隔足㿟大, 则函数子集的  维将主要由间隔决定, 有垭 能小于甚至远小于空间维数。 把这个定理转述为支持向量机中采刖的规范化超平面的形式, 绳论是：若  维空间中 规范化分类超平面权值的模为 , 则函数子集的  维满足  其中  是空间中包含全部训练样本的最小超球的半径。\n所以, 支持向量机中最大化分米间隔，就是通过最小化  以实现最小化函数子集 VC 维的上界。在高维空间中，尤其是经过核函数变换后的高维空间中，空间维数很大甚至是无穷大，但通过控制分米间隔，可以有效控制函数子集的  维，从而保证在函数子集中求得经验风险最小的解具有好的推广能力。 【定理】如果包含  个样本的训练集被最大间隔超平面分开,那么超平面在末来独立 测试集上测试错误率的期望有如下的界  其中,  是支持向量个数,  是包含数据的超球半径,  是分类间隔,  是空间的维数。\n错误率上界则不再由原空间的维数决定，可以大大降低。同时也看到，支持向量机圳练后得到的支持向量数目在全部训练样本中所占的比例，也体现了学习后的机器的推广能力，比例越小则期望的测试错误率上界越小。 正是由于这些理论性质，保证了在高维小样本问题上支持向量机表现出色，而且使它在引入核函数进行等效的升维后仍然能保证良好的推广能力。\n统计学习理论也存在其局限性。一方面，对于大部分常见非线性机器学习模型，对应的函数集的V℃维难以估计，这使得很多定量的结论难以直接用于指导其他机器学习模型和算法的设计。另一方面，在第12章将要介绍的深度学习中，很多场景下面临的训练数据虽然有限，但已经超出了统计学习理论所主要针对的小样本情形，导致在VC维基础上得到的各种界都比较松弛。在第12章中我们将会看到，深度学习采用了多种复杂的深度神经网络模型来获得更高的表示能力，一定意义上是增加函数集的能力，但同时在模型设计和算法设计中又采用多种技巧和策略来降低自由参数的数目、缩小参数的取值空间，这与结构风险最小化中一方面追求经验风险最小、另一方面对函数子集进行约束的原理是一致的，只是结构风险最小化理论的定量结果尚无法直接用于解释和指导深度学习模型和算法。如何与深度学习相结合，拓展统计学习理论，发展新的关于学习机器推广性的理论，是机器学习和模式识别未来研究的重要方向。\n6. 不适定问题和正则化方法简介\n6.1 不适定问题\n有些研究者把机器学习问题抽象为一个求解反演问题（inverse problem）的任务来进行数学上的研究。例如，假设研究对象具有某种我们感兴趣但不易观测的特性乙，但可以观测到它经过了一定映射后的另外的特性，例如最简单情况下它和z有如下的关系：\n如果方程的解存在、唯一且稳定，这样的问题就称作适定问题。稳定是指方程的解对参数或输入数据的依赖是连续的，微小变化带来的影响也是微小的。但实际上，例如对问题，即使解存在且唯一，问题也不一定是适定的。因为我们得到的观测通常是带有噪声的，逆算子经常是不连续的，导致有噪声的观测有微小变化时， 可能会有很大变化。这时问题就是不适定问题： 在很多情况下，求解算子方程  的问题是不适定的。即使方程存在唯一解，方程右边的微小扰动  会带来解的很大变化。在无法得到准确的观测  的情况下，对带有噪声的观测  ，常最小化下面的目标泛函来得到对解  的好的估计, 即使扰动  趋向于零也如此。  的方法无法得到对解  的好的估计, 即使扰动  趋向于零也如此。\n6.2 正则化方法\n将支持向量机用正则化框架表示  在这里，目标函数的第一项对应着支持向量机原问题中用松弛因子表示的分类错误惩罚，第二项对应着支持向量机原问题中最大化间隔的项，两项之间折中的系数变成了这里的正则化系数  。\n范数就是对参数向量中非零参数个数的计数，把它放到目标函数中进行最小化，就是在要求经验风险最小化的同时希望函数中非零参数的个数尽可能少，实现在减小训练误差的同时实现特征选择的功能，也就是常说的学习对样本特征的稀疏表示，这也是所谓“压 缩感知”的基本思想，但范数的优化计算很难。\n范数即参数向量各元素的绝对值之和也可以用来作为对非零参数个数的一种惩罚，所以比较广泛地被果用。\n范数由于采用了平方和，在计算上有很大的方便性，L2范数能够有效地防止参数变得过大，可以较有效地避免过拟合，但平方惩罚 对于强制小的参数变成0的作用不大。采用L2范数的线性回归方法也称作铃回归（ridgeregression）。支持向量机中的最大化分类间隔就等价于采用范数作为正则化项，但与正侧化回归方法不同，支持向量机中对错误的度量不是采用平方误差函数，而是对分类错误采用了线性的惩罚。\n弹性网方法采用了范数与范数相结合的方式，它既发挥范数的作用防止参数值过大带来的过学习风险，也利用范数有效减少菲零参数个数，两个目标通过人为确定的常数来进行权衡。弹性网 (亦称混合正则化)： \n","categories":["模式识别"],"tags":["模式识别"]},{"title":"隐马尔可夫模型与贝叶斯网络","url":"/2022/04/29/Pattern%20Recognition/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/","content":"\nThe project will be configured before the first of June\n\n\n第四章：隐马尔可夫模型与贝叶斯网络\n1. 贝叶斯网络的基本概念\n贝叶斯网络，又称信念网络 (belief network)，它是一种用有向无环图（DAG）表示的概率模型。贝叶斯网络是概率论与图论的结合，它提供了一种表示联合概率分布的紧凑方法，在机器学习和概率推断 (probability inference) 中有重要应用。概率推断问题可以抽象为在一个联合概率分布模型中，已知部分随机变量的取值 ，希望推断末知变量取值  的概率分布。\n2. 隐马尔可夫模型\n隐马尔可夫模型由初始状态概率向量  、状态转移概率矩阵  和观测概率矩阵  决定。  和  决定不可观测的状态序列， 决定观测序列。因此, 隐马尔可夫模型  可以用三元符号表示，即   称为隐马尔可夫模型的三要素。从定义可知，隐马尔可夫模型作了两个基本假设:\n（1）齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻  的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关：  （2）观测独立性假设,，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态, 与其他观测及状态无关:  隐马尔可夫模型有 3 个基本问题:\n\n概率计算问题：已知模型  和观测序列 ，计算在模型  下观测序列  出现的概率  。\n解码问题：已知模型  和观测序列 ，求使条件概率 最大的状态序列 \n学习问题：已知观测序列 ，估计模型  参数，使得在该模型下观测序列概率  最大。即用极大似然估计的方法估计参数。\n\n2.2 HMM评估问题\n本节介绍计算观测序列概率  的前向（forward）与后向（backward）算法。 先介绍概念上可行但计算上不可行的直接计算法。\n2.2.1 直接计算法\n列举所有可能的长度为  的状态序列 ，求各个状态序列  与观测序列 的联合概率 ，然后对所有可能的状态序列求和，得到  。 状态序列为  的概率：  对固定的状态序列 ，观测序列  的概率：   和  同时出现的联合概率：  然后, 对所有可能的状态序列  求和, 得到观测序列  的概率 , 即  但上式计算量很大, 是  阶的，算法不可行。\n2.2.2 前向算法\n给定隐马尔可夫模型 ，定义到时刻  部分观测序列为  且状态为  的概率为前向概率, 记作  观测序列概率的前向算法： 输入：隐马尔可夫模型 ，观测序列 ； 输出：观测序列概率  。\n（1）初值：  （2）递推：对 ,  （3）终止：  前向算法可减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果，避免重复计算。这样利用前向概率计算  的计算量是  阶的，而不是直接计算的  阶。\n2.2.3 后向算法\n给定隐马尔可夫模型 ，定义在时刻  状态为  的条件 下, 从  到  的部分观测序列为  的概率为后向概率，记作  观测序列概率的后向算法： 输入: 隐马尔可夫模型 , 观测序列 ; 输出: 观测序列概率  。\n（1）  （2） 对   （3）  利用前向概率和后向概率的定义可以将观测序列概率  统一写成：  一些概率与期望值的计算：利用前向概率和后向概率，可以得到关于单个状态和两个状态概率的计算公式。\n\n给定模型  和观测 ，在时刻  处于状态  的概率。记\n\n * 给定模型  和观测 ，在时刻  处于状态  且在时刻  处于状态  的概率。记\n 将  和  对各个时刻  求和，可以得到一些有用的期望值：\n（1）在观测  下状态  出现的期望值  （2）在观测  下由状态  转移的期望值  （3）在观测  下由状态  转移到状态  的期望值 \n2.3 HMM解码问题\n2.3.1 近似算法\n给定隐马尔可夫模型  和观测序列 ，在时刻  处于状态  的概率  是  在每一时刻  最有可能的状态  是  从而得到状态序列  。近似算法计算简单，但不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列可能有实际不发生的部分。事实上，上述方法得到的状态序列中有可能存在转移概率为 0 的相邻状态，即对某些  。尽管 如此，近似算法仍然是有用的。\n2.3.2 维特比算法\n输入: 模型  和观测 ; 输出: 最优路径  。\n（1）初始化  （2）递推。对   （3）终止  （4）最优路径回溯。对   求得最优路径  。\n2.4 HMM学习问题\n隐马尔可夫模型的学习, 根据训练数据是包括观测序列和对应的状态序列还是只 有观测序列, 可以分别由监督学习与无监督学习实现。本节首先介绍监督学习算法, 而后介绍无监督学习算法—Baum-Welch 算法 (也就是 EM算法)。\n2.4.1 监督学习方法\n假设已给训练数据包含  个长度相同的观测序列和对应的状态序列 , ，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。 具体方法如下。\n\n转移概率  的估计\n\n 2. 观测概率  的估计  3. 初始状态概率  的估计  为  个样本中初始状态为  的频率。 由于监督学习需要使用标注的训练数据，而人工标注训练数据往往代价很高，有时就会利用无监督学习的方法。\n2.4.2 Baum-Welch 算法\n假设给定训练数据只包含  个长度为  的观测序列  ，目标是学习隐马尔可夫模型  的参数。这时隐马尔可夫模型事实上是一个含有隐变量的概率模型：  它的参数学习可以由 EM算法实现。\n（1）确定完全数据的对数似然函数 所有观测数据写成 所有隐数据写成, 完全数据是  完全数据的对数似然函数是  。\n（2） 算法的  步: 求  函数   其中：  （3） 算法的  步: 极大化  函数  求模型参数 。 由于要极大化的参数在式中单独地出现在 3 个项中, 所以只需对各项分别极大化。\n\n第 1 项\n\n 注意到  满足约束条件 ，利用拉格朗日乘子法，写出拉格朗日函数:  对其求偏导数并令结果为 0  得  对  求和得到   即得 \n\n第 2 项\n\n 类似第 1 项, 应用具有约束条件  的拉格朗日乘子法可以求出 \n\n第 3 项\n\n\n同样用拉格朗日乘子法，约束条件是  。只有在  时  对  的偏导数才不为 0 , 以  表示。得  Baum-Weich模型参数估计公式： Baum-Welch 算法是 EM算法在隐马尔可夫模型学习 中的具体实现。将各概率分别用  表示有：  Baum-Welch 算法 输入: 观测数据 ; 输出: 隐马尔可夫模型参数。\n（1）初始化。对 , 选取 , 得到模型  。\n（2）递推。对 ,  右端各值按观测  和模型  计算。式中  由式 (10.24) 和式 (10.26) 给出。\n（3）终止。得到模型参数  。\n3. 朴素贝叶斯分类器\n朴素贝叶斯假设各个特征的取值只依赖于类别标签且特征之间是互相独立的，该假设下，联合概率可以分解为：  类别的先验概率可通过统计训练样本中第  类样本占总训练样本的比率来进行估计：  对于各个特征的条件概率，可以通过第  类样本在该特征上的取值进行估计：  当训练样本量较少，或者某些特征取值概率较低时, 可能会出现分子的情况。这时如果将  直接设置为 0 可能并不太合理，通常会采用拉普拉斯平滑。  其中,  为类别数， 为第  维特征的可能取值个数。\n4. 在贝叶斯网络上的条件独立性\n在有向无环图(DAG) 中,任意三个节点之间的路径关系可以概括为下面三种形式。\n形式 1: 头对头\n\n\n\n这时联合概率满足:   取值末知时 和  是互相独立的，也称为头对头条件独立。  如果  的取值已知为 ，则  和  的取值对于  条件不独立。 \n形式 2: 尾对尾\n\n\n\n这时联合概率满足  如果  的取值已知为 ，则  与  关于  条件独立。  这时  的边缘概率分布与观测  无关，即  若  的取值末知，  和  不独立。 \n形式 3: 头对尾\n\n\n\n这时联合概率满足  在给定取值  的情况下，随机变量  利  关于  独立  而在  取值未知的情况下，显然  和  不独立。\nD-分离\n一种判断 DAG 概率图中随机变量间条件独立的图形化方法。一条无向路径  被取值已知的节点集合 D-分离，当且仅当至少满足下面一种情况时成立:\n\n 包念  或者 ，且节点  属于集合 ；\n 包含 , 且节点  属于集合 ；\n 包含 , 且  和  的后继节点不属于集合  。\n\n利用D-分离特性可知，一个节点在给定其父节点的情况下，它与它的非子节点之间是条件独立的。基于 D-分离的概念，在贝叶斯网络上可以定义一个节点或节点集合的马尔可夫覆盖 (Markov Blanket): 对于网络中的一个节点 ，如果存在节点集合使得在条件于该节点集合情况下，  与网络中的其他节点条件独立，则这些集合中的最小集合被定义为  节点的马尔可夫覆盖。对于 DAG中的单个变量节点 ，其马尔可夫覆盖就是由该节点的父节点、子节点以及子节点的父节点组成的集合，用 表示。对于不在  中的任意变量 ，均与  条件于  独立，即：  也就是说，要推断  的概率取值，只需要知道  集合中的节点取值就够了。因此，当研究对象背后存在一系列复杂的概率依赖关系时，只要能把概率依赖关系梳理成适当的贝叶斯网络形式，则可以通过马尔可夫覆盖分解计算在已知某些观测情况下所感兴趣的节点上的后验概率，从而可以应用贝叶斯决策的思想进行类别判别或定量预测。这一过程实际上是一种对事件发生可能性的推理过程，因此，贝叶斯网络也被称作信念网络（belief network），意思是在观测到一定证据的情祝下，对各种相关事件发生的可能性进行推断，类似于对各种事件的信念在网络上传播。\n5. 贝叶斯网络模型的学习\n5.1 贝叶斯网络的参数学习\n当知道模型的网络结构，但不知道具体的概率分布时，可以建模为在给定数据的情况下，求解最大后验概率（maximum a posteriori estimation，MAP）的问题。已知数据集  的情况 下, 模型参数  的后验概率密度表示为:  即求  如果模型中不同参数的先验概率相同，则变为参数最大似然值估计 (MLE) 问题。 利用贝叶斯网络的条件独立性，有:  其中， 表示与  和  的父节点有关的子数据集， 为模型在这一部分中的参数。通常假设参数的先验分布是互相独立的, 即  则后验概率整体上可以分解为:  利用这个性质，可以将各个部分的概率密度进行分解后分别进行计算，分别利用  来求解  。在有的学习任务中，会遇到有一部分数据存在特征缺失，或者网络中存在部分不可观测的节点（隐变量）的情况。对于这种情况，通常使用EM算法进行参数的学习。\n5.2 贝叶斯网络的结构学习\n贝叶斯网络能描述复杂数据内在关系。应用中通常会构造一个对网络模型的评分函数来控制模型复杂度。用  表示某个网络结构  如果对模型没有先验知识，可以假设对于各种可能的网终结构先验概率  都相等, 使问题进一步简化为最大似然问题  加入惩罚函数，结构学习的问题变成了最小化以下目标函数的问题。  基于打分函数，可以根据一定的算法对各种可能的网络结构进行逐一分析，在每种结构下利用最大似然方法来估计参数，并计算打分函数值，最终输出为使得该评分最小的模型及其参数。除了基于打分的方法外，另一类常用的网络学习方法是基于约束条件的结构学习。其思想是通过一系列条件独立的假设检验来逐步构建网络。在网络结构比较稀疏的情况下，这类方法表现出较高的学习效率。在所有可能的网络结构空间搜索最优贝叶斯网络结构被证明是一个NP难的问题。很多情况下需采用一些约束假设和启发式方法来降低运算的复杂程度。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第十章：模式识别系统的评价","url":"/2022/04/28/Pattern%20Recognition/%E5%8D%81%EF%BC%8C%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BB%B7/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BB%B7/","content":"\n\n\n\n第十章：模式识别系统的评价\n1. 错误率估计\n通常采用实验方法估计分类器的错误率：训练错误率，测试错误率。\n1.1 交叉验证\n总样本不变，随机选用一部分样本作为临时的训练集，用剩余样本作为临时的测试集，得到一个错误率估计；然后随机选用另外一部分样本作为临时训练集，其余样本作为临时测试集，再得到一个错误率估计，如此反复多次，最后将各个错误率求平均，得到交叉验证错误率。一般让临时训练集较大，临时测试集较小，这样得到的错误率估计就更接近用全部样本作为训练样本时的错误率。而测试集过小带来的错误率估计方差大的问题通过多轮实验的平均可以得到一定的缓解。\nk轮n倍交叉验证\n总样本随机划分为n个等份，在一轮实验中轮流抽出其中的1份样本作为测试样本，用其余（n-1）份作为训练样本，得到n个错误率后进行平均，作为一轮交叉验证的错误率；由于对样本的一次划分是随意的，往往进行多轮这样的划分（比如k轮），得到多个交叉验证错误率估计，最后将多个估计再求平均。\n留一法交叉验证\n样本数较少时最常用的方法。每轮实验拿出一个样本来作为测试样本，用其余的N一1个样本作为训练样本集，训练分类器。下一轮把之前测试的样本放回，拿出另外一个样本作为测试样本，用剩余的N一1个样本作训练，再对留出的样本作测试，依次类推。全部N轮实验完成后，统计总共出现的测试错误数占总样本数的比例就是留一法交叉验证错误率。\n1.2 自举法与.632估计：\n自举样本集包含原样本集中63.2%左右的样本。训练错误率是对真实错误率偏乐观的估计，而自举错误率是偏保守的估计。将这两种估计按照一定的方式结合起来有.632 估计。是在全部样本上的训练错误率， 是自举错误率。 是对错误率更好的估计。\n\n事实上，考虑样本随机性，如果仅基于交叉验证，不存在错误率估计量方差的无偏估计。\n2. 用扰动重采样估计SVM错误率的置信区间\n之所以单纯靠样本划分或重采样无法获得对分类器错误率变换范围的无偏估计，是因为在划分或重采样得到的数据集之间存在不可避免的相关性。这一问题可以通过适当引人扰动的方法来解决。\n3. 特征提取与选择对分类性能的影响\nCV1\n把所有样本都用来进行特征选择与提取，而后把所选择和提取的特征固定下来，再把样本分成训练集和测试集。\nCV2\n在未作任何特征选择与提取前把测试样本和训练样本分开，在每一轮里只用训练样本选择和提取特征。采用CV2策略进行交叉验证，可以得到对包括特征选择与提取部分在内的模式识别系统性能的真实估计，但是却没有得到一组唯一的用于分类的特征。这时，一种做法是利用所有样本重新进行一次特征选择与提取，得到唯一的特征组合，用所有样本设计分类器；另外一种做法是，将CV2交叉验证中得到的各个特征组合方案进行综合，从中选择在各轮交叉验证中被选中次数最多的若干特征组成最后的特征集，用这些特征在所有样本上设计分类器。\n4. 从分类显著性推断特征与类别的关系\n随机置换法：在保持已知样本集中两类样本比例不变的情况下，随机打乱样本的类别标号。再用同样的特征选择、提取和分类方法进行分类，得到的分类性能就反映了这样的模式识别方法在无分类信息的数据上的表现。多次重新随机置换样本类别标号，就可以统计出在没有分类信息情况下模式识别分类性能的空分布，然后把在真实数据上得到的性能估计与这个空分布进行比较，得到分类器性能的随机置换P值。如果该P值很小（比如，通常以小于0.05为参考），则说明在原样本集上得到的分类性能具有统计显著性，初步推断系统S很可能真实存在。\n5. 非监督模式识别系统性能的评价\n\n紧致性(compactness)或一致性(homogeneity)：\n最常见的指标是类内方差或者平方误差和。除此还有其他类型的类内一致性度量：类内两两样本之间的平均或最大距离、平均或最大的基于质心的相似度，基于图理论的紧致性度量等。比如,可用指标： \n\n\n连接性质(connectedness)\n衡量了聚类是否遵循了样本的局部密度分布及相邻的样本是否被划分到同一类。如连接度(connectivity)，即样本中相邻的数据点被划分到同一个聚类中的程度。如果第  个样本与其第  个近邻不在同一个聚类中,则 , 否则为 0 。连接度指标越小越好。 \n\n\n分离度(separation) 可用两类中心距离或两类最近样本之间的距离来计算两类间的距离。分离度指标越大则各类间分离越好。\n\n\n综合性质的评价准则: 紧致性与分离度是两个极端,如果不断增加聚类数目,紧致性将会随之增加,但分离度也会相应的减小。因此,可将这两 个指标组合起来, 定义能同时反映类内距离和类间距离的新指标，比如Silhcuette 值:   代表样本  到和它同类的所有样本的平均距离,  表示样本  到其他聚类中最近一个聚类的所有样本的平均距离。所 有样本的 Silhouette 值的平均称作 Silhouette 宽度, 其取值在  之间。越大则聚类效果越好。\n\n\nDunn 指数(Dunn index)   是聚类  中最大的类内距离,  是  和  两类中相邻最近的样本对间的距离。  的取值 范围是 , 此指数的目标是最大化。\n\n由于没有先验认识，非监督学习的目标是多样的，无论采用什么方法混合多个指标，都不可避免地导致某些方面信息的损失。另外一种不同的策略是，同时评价各个指标， 并且仅当一个方法在某一个指标上超出另一个方法、同时在所有指标上都等同于或超出另一方法时，才断定该方法在聚类性能上胜过另一方法。这是个多目标优化的问题。可以用多目标优化的方法，来寻找在多个指标上都优胜的聚类方法，或确定方法中的可变参数。\n\n稳定性：\n即其结果的可重复性。采用重复地随机重采样或者对样本加人随机扰动等方法获得多个不同的样本集，在不同的样本集上实施同样的聚类算法，定义某个统计量来衡量在这些重采样或扰动的样本集上得到的聚类结果的一致性，用它来评价从原始的数据上得到的聚类结果的显著性。\n\n\n预测效力：\n将样本随机地划分成两份，两份样本上都各自进行聚类；用其中一份中得到的聚类结果作为临时训练样本，对另外一份中的样本实行最近邻法分类，比较这样的分类与直接在这份样本上的聚类划分之间的重合程度，重合程度越大则聚类结果越稳定。实际应用中，这样的实验通常需要多次重复进行，最后以指标的平均值来作为稳定性的度量。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第九章：非监督模式识别","url":"/2022/04/28/Pattern%20Recognition/%E4%B9%9D%EF%BC%8C%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/","content":"\n\n\n\n第九章：非监督模式识别\n1. 混合模型的估计\n1.1 假设条件\n\n样本来自类别数为  的各类中;\n每类先验概率  已知；\n类条件概率密度  形式已知；\n末知的仅是  个参数向量  的值。 定义混合密度：   。类条件密度  称为分量密度, 先验概率  称为混合参数。 被观察样本的似然函数定义为：  对数似然函数  最大似然估计  应满足  #### 1.2 可识别问题\n\n​ 设 ，如果对于混合分布中每个  都有 ，则称密度  是可识别的。常见连续随机变量的分布密度函数都是可识别的，而离散随机变量的混合概率函数则往往是不可识别的，这是由于混合分布中末知参数  的分量数目多于独立方程的数目而造成的。\n1.3 计算问题\n对于可识别的最大似然函数，可以用一般方法求最大似然估计量  。假定似然函数  对  可微，用对数似然函数  对  分别求导  如果当  时  和  的元素在函数上是独立的,并且引进后验概率  则有  得出最大似然估计  满足  其为由  个微分方程组成的方程组，解这个方程组就可以得到参数  的最大似然估计  先验概率 末知时，对  的最大似然值的搜索应该在 且\n如果似然函数可微，利用求条件极值的 Lagrange 乘子法。 对数似然函数：  Lagrange 函数  对  求导并使导数为零，即可解出  的最大似然解  。  利用贝叶斯公式  将  个方程相加  因为  所以   的最大似然估计  进一步推导可得必要条件  其中  ​ 非监督情况下的最大似然解  和 ，原则上可以从以上微分方程组中解出，但实际上要求出闭式解是相当复杂的。所以处理非监督参数估计问题时经常采用迭代法求解。\n2. 高斯混合模型\n模型中的各个分布是多维正态分布的, ,\n2.1 第一种情况：均值向量  末知\n这时参数  就是 , 可以得到最大似然估计解的必要条件。  由于  所以  的最大似然估计  应满足  两边左乘 , 可得  它表明  的最大似然估计是样本加权平均。其中对第  个样本  的权就是属于第  类有多大可能性的 一个估计。如果  对某些样本为 1，而对其余样本为零，那么  是被估计为属于第  类样本的平均。但遗惐的是并没有明显地给出  的值,这是因为其中  是末知的。利用贝叶斯公式  并将  代人，将变成一组十分复杂的非线性联立方程组，求解是相当困难的，而且一般没有唯一解，必须对得到的解进行检验以获得一个实际上使似然函数为最大的解。为解决这一困难，可以应用迭代法。如果有某种方法能得到末知均值的一个较好的初始估计 ，可以得到一种迭代算法来改进估计。  这基本上是一种使对数似然函数极大化的梯度法，分量密度之间的重叠较时收敛较快。显然，这个算法也存在一般梯度法的缺点，即算法得到的不是全局最优解，而是局部最优解，其结果将受到初值  的 影响，甚至可能收玫到鞍点，对运算的结果应注意分析和检验。\n2.2 第二种情况:  均末知\n只有类别数目  已知。写出对数似然函数  (不过  是由  所组成)，然后在限制条件  下构造出 Lagrange 函数  对  分别相对于  及  求导,并令导数为零。可得  其中 :  在特殊情况下，即当  来自  类时, , 否则就等于零, 此时有:  ​ 其中  为来自  的样本数,  为来自  的样本。这样上式说明  是 来自  的样本的百分比,  为  类的样本均值,  为  类的样本协方差阵。一般  是介于 0 与 1 之间的数,而且所有的样 本都对估计值起某种作用,这些估计基本上仍然是加权的频数比、样本均值和样本协方差阵。 ​ 解这些方程一般是很困难的，有效的方法还是采用迭代法，即用一个初始估计值计算式 , 然后反复迭代。如果初始估计较好，比如它是用一些标有类别的样本求出的，那么收敛就比较快。但所得结果与初值选择有关, 因而所得的解仍是局部最优解。另一个问题是迭代算法和样本协方差阵求逆都需要很多运算时间。要是有理由假定样本协方差矩阵是对角阵, 那么运算可大大简化, 而且还减少了末知参数的个数。在样本数不多的情况下, 减少末知参数的 个数是十分重要的。如果对角阵假设太勉强, 但可以假定 c 个协方差矩阵都相同的话, 也能减少计算时间。 ​ 不论是参数化的混合模型估计方法, 还是非参数化的单峰子集分离方法，由于涉及样本概率密度估计的问题，都需要较多的样本数或对样本分布的先验知识。这在很多非监督学习问题中是不易满足的。除了这类基于模型的方法，人们还发展了很多直接基于数据进行聚类的方法。\n3. 动态聚类算法 \n(1)选定某种距离度量作为样本间的相似性度量。 (2)确定某个评价聚类结果质量的准则函数。 (3)给定某个初始分类, 然后用迭代算法找出使准则函数取极值的最好聚类结果。\n3.1  均值算法 \n\n初始划分  个聚类, , 计算  和 ;\n任取一个样本 , 设 ;\n若 , 则转 (2); 否则继续;\n计算  \n考查  中的最小者 , 若 , 则把  从  移到  中;\n重新计算  和 \n若连续  次迭代  不改变,则停止; 否则转(2)。 这是一个局部搜索算法, 并不能保证收敛到全局最优解。算法结果受到初始划分和样本调整顺序的影响。样本初始划分一般先选择一些代表样本点作为初始聚类的核心,然后根据距离把其余的样本划分到各初始类中。  均值聚类的  是事先给定的。类别数目末知的情况下，有时可以逐一用  来进行聚类，每次聚类都计算出最后的误差平方和 , 考查  随  的变化推断合理的类别数。如果作一条  随  变化的曲线，则曲线的拐点处对应的类别数就是接近最优的聚类数。\n\n3.2 ISODATA 方法\n与  均值算法相比: 第一，它把全部样本调整完后才重新计算各类的均值。第二，聚类过程中引入了对类别的评判准则，可以根据这些准则自动地将某些类别合并或分裂。 (1) 初始化，设初始聚类数 ，用与  均值法相同的办法确定  个初始中心  (2) 把所有样本分到距离中心最近的类  中,  (3) 若某个类  中样本数过少 ，则去掉这一类，并根据各样本到其他类中心的距离分别合入其他类，置  。 (4) 重新计算均值  (5) 计算第  类样本与其中心的平均距离和总平均距离  (6) 若是最后一次迭代(由参数  确定)，则程序停止，否则: 若 ，则转(7) (分裂); 若 , 或是偶数次迭代，则转(8)(合并)。否则（即既不是偶数次迭代, 又不满足  ) 分裂。 (7) (分裂) (1) 对每个类，求各维标准偏差    为第  个样本的第  个分量,  是第  个聚类中心的第  个分量,  是第  类第  个分量的标准差 (2) 对每个类,求出标准偏差最大的分量 ; (3) 对各类的 , 若存在某个类的  (标准偏差参数), 且  且 , 或 , 则将  分裂为两类, 中心分别为 和 , 置   分裂项可以为  为常数), 也可以是  。 (8) (合并) (1) 计算各类中心两两之间的距离  (2) 比较  与  (合并参数), 对小于  者排序  (3) 从  开始，把每个  对应的  和  合并,组成新类, 新的中心为  并置  。每次迭代中避免同一类被合并两次。 (9) 若是最后一次迭代，则终止; 否则,，迭代次数加 1 , 转 (2)。 (必要时可调整算法参数)\n3.3 基于样本与核相似度量的动态聚类算法\n定义一个样本  到核的距离  。类似于  均值算法定义准则函数为  步骤 1 选择初始划分, 即将样本集划分为 c 类, 并确定每类的初始核  。 步骤 2 若  则  。 步骤 3 重新修正核  。若核  保持不变, 则算法终止; 否则转步骤 2 。 (1) 正态核函数  参数集为 , 样本到核的相似性度量为  (2) 主轴核函数样本的主轴可通过  变换得到。  这里  是和  矩阵的  个最大本征值相对应的本征向量系统。 任何一个样本  与  之间的相似性程度可以用  与  类主轴之间的欧氏距离的平方来度量。  ### 4. 模糊聚类方法 \n​ 隶属度函数表示一个对象  隶属于集合  的程度。自变量范围是所有可能属于集合  的对象，且  。 对于有限个对象  ，模糊集合  可以表示为:  或  模糊  均值 : 用隶属度函数定义的聚类损失函数:   是一个可以控制聚类结果模糊程度的常数。通常选择  取值在 2 左右。在不同的隶属度定义下最小化损失函数就得到不同的模糊聚类方法。FCM 要求一个样本对于各个聚类的隶属度之和为 1, 即  在上条件下求  的极小值，令  对  和  的偏导数为 0 ，可得必要条件：  和  用迭代方法求解式上式: (1) 设定聚类数目  和参数  (2) 初始化各个聚类中心  (3) 重复下面的运算,直到各个样本的隶属度值稳定： ① 用当前的聚类中心根据计算隶属度函数; ② 用当前的隶属度函数更新计算各类聚类中心。 当算法收敛时,就得到了各类的聚类中心和各个样本对于各类的隶属度值,从而完成了模糊聚类划分。 还可以将模糊聚类结果进行去模糊化,即用一定的规则把模糊聚类划分转化为确定性分类。\n改进的模糊 C 均值算法: 在模糊  均值算法中，由于引人了归一化条件，在样本集不理想的情况下可能导致结果不好。野值的存在将影响迭代的最终结果。为此，提出了放松的归一化条件，使所有样本对各类的隶属度总和为 , 即  在这个新的条件下,计算  仍不变,而  变成  ### 5. 分级聚类方法\n\n初始化， 每个样本形成一个类;\n合并: 计算任意两个类之间的距离(或相似性)，把距离最小(或相似性最大)的两个类合并为一类，记录下这两个类之间的距离(或相似性),其余类不变;\n重复 (2), 直到所有样本被合并到两个类中。 算法核心问题是如何度量样本之间以及类之间的距离或相似性度量。样本之间采用何种距离或相似性度量， 取决于所面对的问题中特征的物理意义及相互之间的关系。如果特征是欧式空间中的向量, 通常可以用欧氏距离作为距离度量, 或用相关系数作为相似性度量。在两个样本之间距离或相似性度量确定后，有三种方法定义两个类  之间的距离或相似性度量, 也称作类间的连接:\n最近距离 \n最远距离 \n均值距离 \n\n6. 自组织映射神经网络\n​ SOM网络的神经元节点都在同一层上，在一个平面上呈规则排列。常见的排列形式包括方形网格排列或蜂窝状排列。样本特征向量的每一维都通过一定的权值输入到SOM网络的每一个节点上。神经元节点之间并没有直接的连接，但在神经元平面上相邻的节点间在学习（训练）过程中有一定的相互影响，构成邻域相互作用。神经元节点的计算功能是对输入的样本给出响应。输入向量连接到某个节点的权值组成的向量称作该节点的权值向量。一个节点对输入样本的响应强度，就是该节点的权值向量与输入向量的匹配程度，可以用欧氏距离或者内积来计算，如果距离小或内积大则响应强度大。对一个输人样本，在神经元平面上所有的节点中响应最大的节点称作获胜节点。\n学习算法:  是  维样本向量集合，记所有神经元集合为 ，第  个每个神经元的权值为  (1) 权值初始化: 用小随机数初始化权值向量。注意各个节点的初始权值不能相等。 (2) 在时刻 ，按照给定的顺序或随机顺序加入一个样本，记为  。 (3) 计算神经元响应，找到当前获胜节点  如用欧氏距离作为匹配准则，则获胜节点为  (4) 权值竞争学习。对所有神经元节点,用下述准则更新各自的权值  其中,  是学习的步长,  是两个向量间的欧氏距离，  是节点  与  间的近邻函数值，如果采用方形网格结构，则相当于在节点  的周围定义一个矩形邻域范围  。  (5) 更新步长  和邻域 ，达到终止条件，则算法停止; 否则置 , 继续（2） 这是一个自学习的过程，在学习过程中没有已知的类别标号做引导，也无法定义类似训练误差之类的收敛目标。这个算法终止条件一般是事先确定的迭代次数。为了网络能够更有效地达到自组织状态，步长  和邻域  通常在算法开始时可以设置得大一些，而随着时间  的增加单调减小（Warm up），到算法终止时邻域缩小到只包含最佳节点本身。除了矩形邻域外，还可以使用其他形式的邻域函数，如高斯函数等。在经过了适当的自学习后, SOM 网络会表现出自组织现象: 对某个输入样本 , 对应获胜节点  会逐渐趋于固定。把固定下来的获胜节点  称作样本  的像, 把样本  称作柛经元节点  的原像。一个样本只能有一个像，而一个神经元可能有多个原像, 也可能没有原像。当学习过程终止后，可以统计在每个神经元节点上有多少个原像，即有多少个样本映射到该节点，把这个量叫做像密度。如果把各个节点的像密度按照神经元本来的排列图示出来，就得到一张像密度图。SOM 网络的自组织现象，就是在对样本经过了适当的学习后，每个样本固定映射到一个像节点，在原样本空间中距离相近的样本趋向于映射到同一个像节点或者在神经元平面上排列。 对于有  个样本的样本集，如果邻域函数固定(不随学习时间改变)，则 SOM 学习算法实际是通过梯度下降算法最小化：  ​ 事实上，如果在 SOM 学习算法中取消邻域作用，即只对获胜节点自身做权值修正，SOM就退化为  均值算法的一种随机迭代实现，其中的聚类数  就是神经元结点的数目。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第八章：特征提取","url":"/2022/04/28/Pattern%20Recognition/%E5%85%AB%EF%BC%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/","content":"\n\n\n\n第八章：特征提取\n1. 基于类别可分判据的特征提取 :\n如果采用类别可分性判据作为衡量新特征的准则，则特征提取的问题就是求最优的 , 使：   可是基于类内类间距离或基于概率距离或熵的可分性判据。 采用基于类内类间距离的可分性判据 ，经过  的特征变换后，类内离散度矩阵和类间离散度矩阵分别变为  和 ,则特征提取的问題就是求 , 使下列准则最优 :  这些准则得到的最优变换矩阵是相同的: 设矩阵  的本征值为 ，按大小顺序排列$ {1} {2} _{D}$，选前  个本征值对应的本征向量构成的变换阵就是在这些准则下的最优变换阵 。 也可以采用基于概率距离的判据或基于熵的判据作为准则来进行特征提取。但一般情况下只能靠数值求解，在数据服从正态分布并满足某些特殊条件时可以得到形式化的解。\n2. 主成分分析法 :\n​ 从一组特征中计算出一组按重要性从大到小排列的新特征，它们是原有特征的线性组合，并且相互之间不相关。 记  为  个原始特征设新特征  是这些原始特征的线性组合：  统一尺度：  矩阵形式是 :  求解最优的正交变换  使新特征  的方差达到极大值。正交变换保证了新特征间不相关，而新特征的方差越大，特征也就越重要。  方差  拉格朗日函数  求导得最优解  满足 :  即  是矩阵  的本征向量,  是对应的本征值。  最优的  是  的最大本征值对应的本征向量。  称作第一主成分，它在原始特征的所有线性组合里是方差最大的。对第二个新特征，它除了满足和第一个特征同样的要求（方差最大、模为 1），还必须与第一主成分不相关,即：  可得：  再考虑到 , 不相关的要求等价于要求  和  正交 在  和  的约末条件下最大化  的方差，可以得到  是  的第二大本征值对应的本征向量，  称作第 二主成分。协方差矩阵  共有  个本征值 , 从大到小排序为  。按照与上面相同的方法， 可以得出由对应这些本征值本征向量构造的  个主成分   ​ 它等于各个原始特征的方差之和。变换矩阵  的各个列向量是由  的正交归一的本征向量组成的, 因此, , 即  是正交矩阵。从  到  的逆变换是  取前  个主成分，可知这  个主成分所代表的数据全部方差的比例是  ​ 数据中的大部分信息常集中在较少的几个主成分上。可根据本征值谱图来决定选择几个主成分来代表全部数据。在很多情况下，可以事先确定希望新特征所能代表的数据总方差的比例,比如  或  ，根据上式来试算出适当的  。 计算中常把主成分进行零均值化，这种平移并不影响主成分的方向。即：  ### 3.  变换 ：\n​  变换基本的形式原理上与主成分分析是相同的，但  变换能够考虑到不同的分类信息实现监督的特征提取。  变换是从  展开引出的。对  维随机向量 , 可以用一个完备的正交归一向量系  来展开  其中  有：  用有限的  项  来逼近   与原向量的均方误差是  记 , 即  的二阶矩阵, 则  要在正交归一的向量系中最小化这一均方误差,就是求解下列优化问题  采用拉格朗日法，得到无约束的目标函数  对各个向量求偏导并令其为零, , 得 :  即  是矩阵  的本征向量满足   是矩阵  的本征值, 均方误差为  ​ 把矩阵  的本征值按从大到小排列，选前  个本征值对应的本征向量，即可使表示样本均方误差最小。   组成了新的特征空间，样本  在这个新空间上的展开系数  就组成了样本的新的特征向量。矩阵  称作  变换的产生矩阵。这里得到的  个新特征与主成分分析中的  个主成分很相似，当原特征为零均值或者对原特征进行去均值处理时，二者等价。\n4. 多维尺度法：\n​ 根据样本之间的距离关系或不相似度关系在低维空间里生成对样本的一种表示。如果样本之间的关系是定义在一定的特征空间上的，那么这种表示也就实现了从原特征空间到低维表示空间的一种变换。MDS分为度量型和非度量型两种类型。\n4.1 古典尺度法：\n​ 给定一个两两点之间距离的矩阵，确定这些点在空间里的坐标。假定给定的距离矩阵是欧氏距离。设有  个  维样本 , 所有样本组成的  维矩阵是 , 样本间两两内积组成的矩阵为  。样本  与  之间的欧氏距离为  所有两两点之间的欧氏距离组成的矩阵为：   是矩阵  的对角线元素组成的向量, 即   现已知矩阵 求  。对坐标的平移不会影响样本间的距离,假设所有样本的质心为坐标原点, 即  定义中心化矩阵   是单位对角阵。显然  在式  的假设下,有  对  两边乘以中心化矩阵, 得  可得内积矩阵  这种做法也称作双中心化(double centering)。 如果  是由欧氏距离组成的矩阵,则  是对称矩阵, 可以用奇异值分解的方法来求解   其中,  是由矩阵  的本征向量组成的矩阵,  是以  的本征值为对角元素的对角阵  如果样本不是中心化的，则只要知道样本的均值向量  就可以求得各个样本原来的坐标  如果要用  维空间来表示这些样本, 则可以按照本征值从大到小排序  ​ 用  组成 , 只用这些本征值对应的本征向量组成  。如果已知样本集,，从中计算出 ，再用古典尺度法得到  的低维表示，结果与主成分分析相同。\n4.2 度量型MDS：\n古典尺度法是度量型 MDS 的一种特殊形式。  作为目标函数,则当  是欧氏距离时,得到的低维空间表 示就是样本在主成分上的投影。很多 MDS 压力函数可以统一为：   是对样本对的加权, 比如   是预先定义的函数。比如，如果希望  与  之间是线性关系，则可以选  另外一种常用的压力函数形式是  scale 是一个尺度因子, 可取为 scale , 此时压力函数称作 Kruskal 压力。\n4.3 非度量型 MDS :\n非度量型 MDS 就是追求样本的坐标能反映出定性的顺序信息。也需要最小化上述形式的目标函数但其中的函数  或  只需要是某种单调函数或弱单调函数即可。这种单调函数可以通过所谓 “单调回归\"来实现。目标是，用低维空间坐标表示的样本点之间的距离关系，尽可能接近地反映原相异度矩阵所表示的顺序关系。\n4.4 MDS 在模式识别中的应用 :\n​ 通常用 MDS 在二维或三维上可视化地显示一组复杂样本之间的关系。如果样本间的距离/相异度矩阵是定义在某一特征空间中的，那么 MDS 也可以看作是样本的一种特征变换。\n5. 非线性变换方法\n5.1 核主成分分析：\n\n通过核函数计算矩阵 , 其元素为\n解矩阵  的特征方程 \n解矩阵  的特征方程  得到归一化本征向量  按照对应的本征值从大到小排列。本征向量的维数是 , 向量的元素记作   。由于引人了非线性变换,这里得到的非零本征值数目可能超过样本原来的维数。根据需要选择前若干个 本征值对应的本征向量作为非线性主成分。第  个非线生主成分是 \n计算样本在非线性主成分上的投影。对样本 , 它在第  个非线性主成分上的投影是  样本  在前  个非线性主成分上的坐标就构成样本在新空间的表示  。\n\n5.2 IsoMap\n​ 当样本在高维空间中按照某种复杂结构分布时，直接计算两个样本点之间的欧氏距离，就损失了样本分布的结构信息。如果样本分布较密集，可以假定样本集的复杂结构在每个小的局部都可以用欧式空间来近似。计算每个样本与相邻样本之间的欧氏距离；对两个不相邻的样本，寻找一系列两两相邻的样本构成连接这两个样本的路径，用两个样本间最短路径上的局部距离之和作为两个样本间的距离。这种距离称作测地距离。有了样本间的距离矩阵, 就可以用度量型 MDS 等方法映射到低维空间。\n5.3 LLE\n\n在原空间中, 对样本  选择一组领域样本；\n用这一组邻域样本的线性加权组合重构  ，得到一组使重构误差  最小的权值  ；\n在低维空间里求向量  及其邻域的映射,使得对所有样本用同样的权值进行重构得到的误差  最小。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第七章：特征选择","url":"/2022/04/28/Pattern%20Recognition/%E4%B8%83%EF%BC%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","content":"\nThe project will be configured before the first of June\n\n\n第七章：特征选择\n1. 类别可分性准则：\n定义与错误率有关又便于计算的类别可分性准则 ，用来衡量在一组特征下第  类和第  类之间的可分程度，\n\n其应该满足： (1) 判据应该与错误率(或错误率的上界)有单调关系。 (2) 当特征独立时,判据对特征具有可加性,即  (3) 应具有度量特性 当时当时 (4) 对特征具有单调性, 即加入新的特征不会使判据减小 \n\n1.1 基于类内类间距离的可分性判据：\n &gt;基于类内类间距离的判据： &gt;\n1.2 基于概率分布的可分性判据：\n基于类内类间距离的可分性判据没有直接考虑样本的分布情况，很难与错误率建立直接的联系。分布密度的交叠程度可用  及  这两个分布密度函数之间的距离  来度量。\n\n概率距离度量函数应满足： (1) ; (2) 两类完全不交叠时  取最大值, 即对所有  有  时 , 则  (3) 当两类分布密度相同时  应为零，即若  ，则  。\n\nBhattacharyya 距离 :  理论上的错误率 \nChernoff 界限 :  散度 :  当两类样本都服从正态分布  当两类协方差矩阵相等时,  。这等于两类均值之间的 Mahalanobis 距离。\n\n概率相关性判据 : 可以用  与  之间的函数距离作为特征对分类贡献的判据  只需把  换成  换成  即可。\n1.3 基于熵的可分性判据：\n\nShannon 熵  平方熵 \n\n对特征的所有取值积分，得到基于熵的可分性判据，  越小可分性越好。  #### 1.4 统计检验作为可分性判据：\n计算在空假设下有多大的概率会得到所观察统计量的取值, 这个概率小于  值，则拒绝空假设，接受备择假设。\n\n 检验 : 假设两类样本服从方差相同的正态分布，在同一特征上的观测。总体样本方差：  统计量：  服从自由度为  的  分布。双边 -检验空假设 : , 备择假设是  。单边  检验空假设 : , 备 择假设是  。  检验属于参数化检验方法,对数据分布有一定假设,必要时需要检验样本分布是否符合该假设。\n\n\n秩和检验: 把两类样本混合在一起,对所有样本按照所考査的特征从小到大排序。在两类样本中分别计算所得排序序号之和  和 , 称作秩和, 考查某一类的秩和是否显著小于或大于另一类的秩和。两类的样本数分别为  和  。 不同样本数目情况下  的空分布是不同的。对于小的样本数,人们预先计算出了  的分布。当  和  较大时(比如都大于10), 可用正态分布  来近似秩和  的空分布, 其中 \n\n过滤方法： 指依据一定的统计量来过滤出与所研究的分类问题密切相关的特征,再采用一定的分类方法进行分类。这种方法实现起来比较简单,但是，所采用的过滤准则与后期分类器所采用的准则并不一定有很好的联系。\n2. 特征选择的最优算法\n分枝定界法：从所有候选特征中逐步去掉不被选中的特征。这种方法具有回溯的过程，能够考虑到所有可能的组合：\n\n同一层按照去掉单个特征后的准则函数值来对各个结点排序，如果去掉某个特征后准则函数的损失量最大，则认为这个特征是最不可能被去掉的，把它放在该层的最左侧节点，依次类推。搜索：从最右侧开始向下搜索，当到达叶节点时计算当前达到的准则函数值，记作界限  。算法向上回溯，每回溯一步回收相应节点上舍弃的特征。遇到最近的分枝节点时停止回溯，从这个分枝节点向下搜索左侧最近的一个分枝。当搜索到某一个节点时，准则函数值已经小于界限  ，则说明最优解已不可能在本节点之下的叶节点上，因此停止沿本树枝的搜索，从此节点重新向上回溯。如果搜索到一个新的叶节点， 则更新界限  值，向上回溯。如果回溯过程一直到了根节点，而且根据界限  不能再向下搜索其他树枝，则算法停止，最后一次更新  时取得的特征组合就是特征选择的结果。\n\n3. 特征选择的次优算法\n1，单独最优特征的组合 假设单独作用时性能最优的特征，它们组合起来也是性能最优的。但即使是特征间统计独立时，单独最优特征的组合也不一定是最优的，这还与所采用的特征选择的准则函数有关, 只有当所采用的判据是每个特征上的判据之和或之积时, 这种做法选择出的才是最优的特征。 2，顺序前进法 3，顺序后退法 4，增  减  法 与广义顺序前进法和广义顺序后退法类似。可以每次选择或剔除多个特征，这种做法称作  法。这样与  法相比能够既考虑到特征间的相关性又保持适当的计算量。\n4. 特征选择的遗传算法 :\n把候选对象编码为一条染色体  。把所有特征表述为一条由  个  字符组成的字符串，求一条仅有  个 1 的染 色体,这样的染色体共有  种。优化的目标为适应度函数, 每一条染色体对应一个适应度值  。可用类别可分性判 据作为适应度。针对不同的适应度有不同的选择概率  。\n\n遗传算法： (1) 初始化, , 随机地产生一个包含  条不同染色体的种群 ; (2) 计算当前种群  中每一条染色体的适应度 ; (3) 按选择概率  对种群中的染色体采样,由采样出的染色体繁殖出下一代染色体,组成种群 ; (4) 回到 (2), 直到达到终止条件（常是某染色体的适应度达到设定國值）,输出适应度最大的染色体作为找到的最优解。 在第(3)步产生后代的过程中, 有两个最基本的操作 : 一个是重组也称交叉, 两条染色体配对, 并在某个随机的位 置上以一定的重组概率  进行交叉, 互换部分染色体。另一个是突变, 每条染色体的每一个位置都有一定的概率  发生突变 (从 0 变成 1 或从 1 变成 0 ) 。算法有很多可调节参数 : 种群大小  、选择概率、重组概率、突变概率等。\n\n5. 以分类性能为准则的特征选择方法：\n包裹法：把分类器与特征选择集成在一起利用分类器进行特征选择的方法。\n过滤法：利用单独的可分性准则来选择特征再进行分类的方法。\n\n包裹法对分类器的基本要求：一是分类器能处理高维的特征向量；二是分类器能在特征维数很高但样本数有限时仍能得到较好的效果。 支持向量机能较好地满足这两个要求。递归支持向量机和支持向量机递归特征剔除的核心是线性的支持向量机，特征选择与分类采用的是同样的算法步骤。两种算法的不同在于它们评估特征在分类器中贡献的方法不同。\n\n两种算法的基本步骤都是：\n\n用当前所有候选特征训练线性支持向量机；\n评估当前所有特征在支持向量机中的相对贡献，按照相对贡献大小排序；\n根据事先确定的递归选择特征的数目选择出的排序在前面的特征（SVM-RFE中描述为剔除排序在后面的特征），用这组特征构成新的候选特征，转（1），直到达到所规定的特征选择数目。\n\n支持向量机的输出函数：  R-SVM 定义两类在当前特征上的分离程度为  写成各个特征之和的形式：   是当前候选特征的维数, 、分别是两类样本在第  维特征上的均值。每个特征的贡献是：  SVM-RFE 采用灵敏度的方法来推导各个特征在 SVM 分类器中的贡献。它把 SVM 输出与正确类别标号  之间平均平方误差作为 SVM 分类的损失函数：  考查各个权值对这个损失函数的影响，各个特征的贡献应该用下式衡量。  ​ 在很多实际应用中,R-SVM 与 SVM-RFE 从分类上看性能基本相同,但 R-SVM 在选择特征的稳定性和在对末来样本的推 广能力方面有一定优势,尤其是当训练样本中存在较大的噪声和野值时优势更明显。 包裏法递归进行特征选择与分类的做法可推广到 SVM 采用非线性核的情况。 SVM 对偶问题的目标函数：  用  表示去掉第  维特征后的样本, 去掉第  个特征对这一目标函数的影响是  可以用这个量作为特征在非线性 SVM 分类器中的贡献，并利用递归方法来进行包裹法特征选择。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第六章：其他分类方法","url":"/2022/04/28/Pattern%20Recognition/%E5%85%AD%EF%BC%8C%E5%85%B6%E4%BB%96%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/%E5%85%B6%E4%BB%96%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/","content":"\n\n\n\n第六章：其他分类方法\n1. 最近邻决策\n 最近邻法的渐近错误率最坏不会超出两倍的贝叶斯错误率，而最好则可能接近或达到贝叶斯错误率。\n\n设  个样本下最近邻法的平均错误率为 , 样本  的最近邻为 , 平均错误率可以写成   为贝叶斯错误率, 存在关系 \n\n2. k 近邻法\n决策规则 则 当  趋于无穷大时，  近邻法错误率渐近贝叶斯错误率。近邻法只是确定一种决策原则，并不需要利用已知数据事先训练出一个判别函数，但需要始终存储所有的已知样本，并将每一个新样本与所有已知样本进行比较和排序，其计算和存储成本都很大。\n3. 近邻法的快速算法\n : 样本集；  : 节点  对应的样本子集；  中的样本数；  : 样本子集  中的样本均值；  : 从  到  的最大距离；\n\n一, 样本集划分：将  分为  个子集。每个子集再分成  个子集得到树结构。对每个节点计算  和  求得 。\n\n二, 搜索。 （1）置  是当前水平,  是当前节点。 （2）将当前节点所有直接后继节点放入当前水平一个目录表中, 并对节点计算  。 （3）如果有 ，则从目录表中去掉  。 （4）如果目录表没有节点，则  则停止）步骤 3。否则转步骤 5 。 （5）在目录表中选择使  最小化的最近节点 ，并从目录表中去掉  。如果当前的水平  是最终水平, 则转步骤 6 。否则置  转步骤 2 。 （6）对当前执行节点  中的每个 ，如果  则计算  。若  置  和  。当前执行节点中所有  被检验之后，转步骤 3\n\n算法结束：输出  的最近邻  和  与  的距离  。\n4. 剪辑近邻法\n落在最优分类面错误一侧的训练样本会误导决策。重叠区域内两类已知样本都存在，可能会使分类面的形状变得 非常复杂，去掉重叠区域可使近邻法的决策面更接近最优分类面。\n\n\n将已知样本集划分为考试集  和训练集  ，用训练集  中的样本对考试集  中的样本进行近邻法分类, 从  中除去被错误分类的样本，剩余样本构成剪辑样本集 ,用  对末来样本进行近邻法分类。\n\n如果在剪辑阶段和分类阶段都用最近邻法，则剪辑近邻法得到的渐近错误率与近邻法错误率的关系是：  其中,  是近邻法的错误率，剪辑后错误率减小。如果近邻法的错误率不大，则有  近邻法的渐近错误率上界是两倍的贝叶斯错误率，，可知剪辑近邻法的渐近错误率近似等于贝叶斯错误率。如果在剪辑阶段用  近邻法，分类阶段用最近邻法,则当 、 但  时，剪辑近邻法的渐近错误率收敛于贝叶斯错误率。同样的方法应用到多类问题上，剪辑近邻法对性能的改善比两类情况下更显著。\n\nMULTIEDIT： 当样本数较多时、为了消除考试集、训练集划分中的偶然性造成的影响。 (1) 划分：把样本集随机划分为  个子集,  。 (2) 分类：用  对  中的样本分类,  。比如, 如果  ，则用  对  分类,用  对  分类，用  对  分类。 (3) 剪辑：从各个子集中去掉在 (2) 中被分错的样本。 (4) 混合：把剩下的样本合在一起, 形成新的样本集  (5) 迭代：用新的样本集  替代原样本集，转(1)。如果在最近的  次迭代中都没有样本被剪掉，则终止迭代，用最后的  作为剪辑后的样本集。 经过多重剪辑之后的近邻法分类面在数据分布的主要区域内已经非常接近贝叶斯分类面。\n5. 压缩近邻法\n将样本集  分为  和  两个活动子集。算法开始时，  中只有一个样本，其余样本均在  中。依次考查  中的每一个样本 ，若用  中的样本能够对它正确分类，则该样本保留在 ，否则移到  中, 依次类推，直到没有样本需要搬移为止。最后用  中的样本作为代表样本，对末来样本进行近邻法分类。\n\n可多重剪辑后再使用压缩近邻法\n\n6. 决策树与随机森林\n6.1 ID3 方法\n信息熵 :  这个度量称为熵不纯度，如果特征把  个样本划分成  组，则不纯度减少量为  其中,  。\n\nGini 不纯度度量，也称方差不纯度  误差不纯度   是当前节点上的  个样本中属于第  类的样本数占总样本数的比例。多数情况下,采用不同的不纯度度量对 分类结果的影响不大。\n\n二类分类中基尼指数、熵之半和分类误差率的关系：\n\n\n6.2 C4.5 算法\n采用信息增益率代替信息增益 \n\nC4.5 增加了处理连续数值的功能\n\n6.3 CART算法\n每一个节点上都采用二分法，最后构成二叉树。\n\n分类和回归树算法 (CART) 既可以用于分类问题，也可以用于构造回归树对连续变量进行回归。\n\n\n6.4 过学习与决策树的剪枝\n对以把有限的样本全部正确划分为准则建立的决策规则，控制决策树生成算法的终止条件和对决策树进行剪枝是防止出现overfitting的主要手段。\n\n先剪枝：在决策树生长过程中决定某节点是否需要继续分枝还是直接作为叶节点。\n（1）数据划分法。将数据分成训练样本和测试样本，首先基于训练样本对决策树进行生长，直到在测试样本上的分类错误率达到最小时停止生长。通常采用多次的交叉验证方法以充分利用数据信息。\n（2）阈值法。预先设定一个信息增益阈值，当从某节点往下生长时得到的信息增益小于设定阈值时停止树的生长。但是此阈值往往不容易设定。\n（3）信息增益的统计显著性分析。统计已有节点获得的信息增益其分布，如果继续生长得到的信息增益与该分布相比不显著，则停止树的生长，通常可以用卡方检验来考查这个显著性。\n\n后剪枝：消除有相同父节点的叶节点后不会导致不纯度的明显增加则以其父节点作为新的叶节点。\n（1）减少分类错误修剪法。通过独立的剪枝集估计剪枝前后分类错误率的改变。\n（2）最小代价与复杂性的折中。对合并分枝后错误率增加与复杂性减少进行折中考虑。\n（3）最小描述长度准则。最简单的树就是最好的树。对决策树进行编码再剪枝得到编码最短的决策树\n7. Bootstrap\n随机森林， Bagging、adaboost、随机划分选择法等。\n7.1 随机森林\n建立很多决策树，组成一个决策树的“森林”，通过多棵树投票来进行决策。： 1，对样本数据进行自举重采样，得到多个样本集。 2，用每个重采样样本集作为训练样本构造一个决策树。在构造过程中每次从所有候选特征中随机地抽取m个特征，作为当前节点下决策的备选特征，从这些特征中选择最好地划分训练样本的特征。 3，得到所需数目的决策树后，以这些树的输出为投票，以得票最多的类作为随机森林的决策。\n\n这对训练样本和特征进行了采样，保证了所构建的每棵树之间的独立性，使投票结果更无偏。\n\n\nBoosting 方法：通过一个迭代过程对分类器的输入和输出进行加权处理，而非简单地对多个分类器的输出进行投票决策。\n7.2 AdaBoost 算法\n 表示  个弱分类器在样本  上的输出\n1，初始化训练样本  的权重  。\n2，对 , 重复以下过程 :\n\n\n利用  加权后的训练样本构造分类器  。\n计算样本用  加权后的分类错误率 , 并令  。\n令 , 并归一化使  。\n\n\n3，对于待分类样本 ，分类器的输出为  。\n\n用加权后的训练样本构造分类器，是指对分类器算法目标函数中各个样本所对应的项进行加权。对于最小平方误差判别，加权后的最小平方误差(MSE)准则函数为：  而对于决策树或一些其他方法，则可以根据每个样本的权值调整重采样的概率，用重采样得到的样本集构造新的弱分类器。在很多情况下，迭代次数（所采用的弱分类器数）较大时，Boosting 方法不会导致严重的过学习问题。\n\n\n待更新！！！！\n8. 罗森斯特回归\n多元线性回归问题：   为残差。特征  可以是连续变量，也可以是离散变量。求解线性回归常用最小二乘法，即求使各样本残差的平方和达到最小的系数  。这实际是在变量  服从正态分布假设下的最大似然估计。 Logistic 函数：  记为  。几率(odds):  对数几率 (log odds)   的 logit 函数  样本属于  类的概率是  决策函数：\n若 , 则  \n最大似然法：设共有  个独立的训练样本,把两类的输出分别编码为  和 .   为样本的总体概率密度函数。  个独立样本出现的似然函数为  最大化上式等价于最大化  取对数  在似然函数满足连续、可微的条件下，最大似然估计量就是以下微分方程的解。 可得：  如果  是  维特征, , 则  代入  得到一组关于  的非线性方程。其一般无法解析求解，可以采用迭代的策略求解。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第五章：非线性分类器","url":"/2022/04/28/Pattern%20Recognition/%E4%BA%94%EF%BC%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","content":"\n\n\n\n第五章：非线性分类器\n1. 分段线性判别函数\n用多个线性分类器片段来实现非线性分类。求解两类之间的分段线性判别函数，可把各类划分成适当的子类，在两类的多个子类之间构建线性判别函数，再分段合并成分段线性判别函数。\n分段线性距离分类器：\n\n最小距离分类器：当两类的类条件概率密度为正态分布，两类先验概率相等，而且各维特征独立且方差相等时，最小错误率贝叶斯决策就是直观的最小距离分类器。在很多情况下，只要每一类数据的分布是单峰的、在各维上的分布基本对称且各类先验概率基本相同，则最小距离分类器都不失为一种简单有效的分类方法。\n分段线性距离分类器：\n\n ​ 决策规则： 则 * 一般的分段线性判别函数(增广形式)：\n ​ 判别函数：  ​ 决策规则是： ，则决策 ​ 两个相邻的类的决策面方程：  &gt;子类的划分： &gt; &gt;第一种情况：视情况人工划分；或者用非监督学习方法对同一类的样本聚类，得到子类的划分。 &gt; &gt;--- &gt; &gt;第二种情况： 已知各类的子类数目，每个类都有一定数量训练样本，但是不知道子类的划分： &gt; &gt;1. 任意给定各类各子类的权值 。 &gt;2. 在时刻 ，考虑某个训练样本 ，找出  类的各子类中判别函数最大的子类，记为 ： &gt; &gt; &gt; &gt;​ 考査当前权值对样本  的分类情㑆 : &gt;​ (1) 若 ， &gt;​ 则   ； &gt;​ (2) 若对某个 ，存在子类  使得 ； &gt;​ 则选取  中最大的子类，记作  ； &gt; &gt;​ 其余权值不变。 &gt; &gt;3. , 考查下一个样本，回到第(2)步。迭代至收敛。 &gt; &gt;--- &gt; &gt;第三种情况：子类数目无法事先确定。可以采用分类树的思想来分级划分子类和设计分段线性判别函数。 &gt; &gt;\n\n\n\n2. 二次判别函数\n一般的正态分布情况下，贝叶斯决策面是二次函数。二次判别函数的一般形式是：  &gt; 其中,  是  维实对称矩阵,  为  维向量；判别函数中包含  个参数。\n通常采用参数化方法求判别函数。这时有二次判别函数：  &gt;  是一个阈值项，它受协方差矩阵和先验概率的影响。\n样本的均值和方差可以用估计：  两类间的决策面方程即是  决策规则是 若则 其中，可以通过调整两类的阈值  和  来调整两类错误率情况。\n\n另一种情况：\n 近似正态分布， 比较均匀地分布在其附近。可用二次判别函数：  决策规则 : 若则 其中：可用  来调整决策的偏向。\n3. 多层感知器神经网络\n阀值逻辑单元  用  和 表示要区分的两类，  代表正确分类，则有权值迭代：  对任意复杂形状的分类区域，可用多个神经元组成一定的层次结构来实现非线性分类面。  ---\n采用反向传播算法的多层感知器，用 Sigmoid 作为传递函数：  把常数项  作为一个固定输人 1 的权值合并到加权求和项中有：  &gt;单位超立方体  内的任意连续函数 ，都可以通过选择适当的  和  表示成 &gt; &gt;即：多层感知器神经网络能够实现任意复杂的函数映射。对任意一个从  到  的映射，都存在一个适当结构的 3 层前馈神经网络能够以任意的精度来逼近它。\nBP 算法：在给定多层感知器结构的情况下训练其权值的反向传播算法 输入向量 ，输入层记 ，第一个隐层记 ，以此类推。第  层第  个神经元的输出记作 ，输出向量 , 第  个隐层的神经元个数为  第  层的权值都用  表示,  表示第  层的节点  连接到第  层的节点  的权值。用  表示在第  步迭代时权值  的取值。\n\n\n确定神经网络的结构，用小随机数进行权值初始化，置  。\n从训练集中得到一个训练样本 , 记它期望的输出是  。\n计算在  输入下当前神经网络的实际输出  平方误差： \n 层权值修正  ​ 对输出层 , 计算梯度项  \n\n\n​ 对中间层，计算梯度项  $$\n\n$$\n\n\n更新全部权值后对所有训练样本重新计算输出，计算更新后的网络输出与期望输出的误差。检查算法终止条件，如果条件已达到则停止，否则 , 返回 (2)。\n\n\n此外，这里给出的 BP 算法是针对神经元节点为 Sigmoid 函数的。这时  的梯度函数是：  如果采用其他形式的 Sigmoid 函数或其他函数, 需要根据其梯度函数修正算法第 (4) 步。\n\n收敛结果有时受初始权值的影响很大，算法不能保证收敛到全局最优点。对于多层感知器神经网络，各个初始权值不能为 0，也不能都相同，而是应该采用较小的随机数。如果算法很难收敛，可以尝试改变初值重新试算。如果步长太大，收敛速度可能一开始会较快,但可能会容易导致算法出现振荡而不能收剑或收敛很慢；如果步长太小，则权值调整可能会非常慢，导致算法收敛太慢，而且一旦陷于局部极小点就容易停在那里。试算过程中观察不同步长下得到的误差收敛曲线有助于找到针对特定问题的较合理的步长。为了兼顾训练过程和训练的精度，有时采用变步长的办法。 为使算法有更好的收敛性能，可在权值更新过程中引人 “记忆项\"或 “惯性项”,把权值更新项改为： \n\n4. 支持向量机\n定义核函数：  对特征  进行非线性变换 ，新特征空间里构造的支持向量机决策函数是：  系数  是下列优化问题的解 $$\n\n通过支持向量求得： y_{i}({i=1}^{n} a{i} K(x_{i} x)+b)-1=0 条件一个对称函数，它是某个特征空间中的内积运算的充要条件是：对于任意的且有 &gt;K(x, x^{}) (x) (x^{})  x  x^{}&gt;0 &gt;选择一个满足条件的核函数，就可以构建非线性的支持向量机，且不用设计变换。是定义在空间上的对称函数，且对任意训练数据和任意实系数都有 &gt;&gt;{i, j} a{i} a_{j} K(x_{i}, x_{j})  &gt;&gt;$$ &gt;&gt;则 是一个正定核。 &gt; &gt;对于正定核，肯定存在一个从  空间到内积空间  的变换 ，使得  。\n\n一, 多项式核函数  采用这种核函数的支持向量机实现的是  阶的多项式判别函数。 二, 径向基(RBF)核函数  采用它的支持向量机实现与径向基网络形式相同的决策函数。 三, Sigmoid 函数  采用这种核函数的支持向量机在  和  满足一定条件的情况下等价于包含一个隐层的多层感知器神经网络。\n\n核函数及其参数的选择 一般先尝试简单的选择，比如线性核，当结果不满意时才考虑非线性核；如果选择 RBF 核函数，则先选用宽度比较大的核，宽度越大越接近线性,，然后再尝试减小宽度，增加非线性程度。\n\n核函数与相似性度量： 通过非线性变换将输入空间变换到一个高维空间, 然后在这个新空间中求最优分类面，这种非线性变换是通过定义适当的内积核函数实现的。支持向量机求得的分类函数，形式上类似于一个神经网络，其输出是若干中间层节点的线性组合，而每一个中间层节点对应于输人样本与一个支持向量的内积, 因此早期也被叫做支持向量网络。 决策过程可看作一种相似性比较的过程。输入样本与一系列模板样本（支持向量）进行相似性比较, 采用的相似性度量是核函数。样本与各支持向量比较后的得分进行加权后求和，权值就是训练时得到的各支持向量的系数  与类别标号的乘积。最后根据和值大小进行决策。采用不同的核函数, 可以看作是选择不同的相似性度量, 线性支持向量机就 是采用欧式空间中的内积作为相似性度量。根据这一思想, 除了可以选择常用的核函数形式外, 还可以根据相关领域的专门知识定义一些特殊的核函数。 维数与推广能力： 支持向量机通过采用核函数作为内积, 间接地实现了对特征的非线性变换, 避开了在高维空间进行计算。支持向 量机通过最大化分类间隔来控制函数集的 VC 维，使得在高维空间里的函数集的 VC 维可以大大低于空间的维数, 从而保证好的推广能力。支持向量机需要求解的是关于  的二次优化函数。这是一个有线性约束的二次优化问题, 有唯一的最优解, 这与多 层感知器神经网络相比是一个优势。而且, 问题的计算复杂度是由样本数目决定的, 计算复杂度不取决于样本的特征维 数和所采用的核函数形式。\n\n多类支持向量机： 支持向量机用正则化(regularization) 的框架重新表述如下: 设有训练样本集  是样本的特征,  是样本的类别标号。待求函数  是由核函数  定义的可再生希尔伯特空间。决策规则是  。 支持向量机求解的是这样的 , 它最小化以下的目标函数  如果样本的类别标号  和待求的函数  都从标量变为向量,则上述表述就可以用于多类分类问题。 对于  类问题,  是一个  维向量,如果样本  属于第  类,则  的第  个分量为 1 , 其余分里为 , 这样,  的各分量值总和为 0 。如 , 则 若若若 待求函数为 , 它的各分量之和须为 0 , 即 , 且每一个分量都定义在核函 数可再生希尔伯特空间中  把多个类别编码成这样的向量标签后，多类支持向量机就是求 , 使下列目标函数达到 最小  其中,  是损失矩阵  与样本类别  相对的行向量。损失矩阵  是一个  维的矩阵,例如：  得到函数  后,类别决策规则是 , 即决策为  各分量中取值最大的分量对应的类 别。\n\n用于回归的支持向量机：  目标函数:  对偶问题：  回归函数权值  回归函数  核函数变换：  系数  是以下优化问题的解 \n5. 核 Fisher 判别：\n对样本  进行非线性变换   这里的  是  空间里的权值向量,  和  分别是  空间里的类间离散度矩阵和类内离散度矩阵   是  空间里各类样本的均值  根据表示定理，上述问题的任何解  都处在  空间中所有训练样本张成的子空间中,即  因此  变换空间里的目标函数  其中：  最大化目标函数的解是  的最大本征值对应的本征向量，且最优解的方向是  原空间任意一个样本到 Fisher 判别的方向上的投影, 即只需要计算  通常，上述问题可能是病态的,因为矩阵  可能非正定,这是由于变换后样本维数升高导致的。一种补偿办法是,引入 一个新的矩阵  来代替原来的矩阵 (  是一个常数),使矩阵正定。这样做同时还实现了对  的正则化控 制,类似于支持向量机中控制间隔的作用。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第四章：线性分类器","url":"/2022/04/28/Pattern%20Recognition/%E5%9B%9B%EF%BC%8C%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","content":"\n\n\n\n第四章：线性分类器\n省去了对概率密度函数的估计，直接基于样本直接进行分类器设计，其需要确定三个基本要素：\n\n分类器的类型，也就是从什么样的判别函数类 (函数集) 中去求解；\n分类器设计的准则，根据样本从函数集中选择在该准则下最优的函数（通常是确定某些待定参数）；\n设计算法利用样本数据捜索到最优的函数参数。\n\n1. 线性判别函数\n\n线性判别函数一般表达式：   是个常数，称为阈值权。对于两类问题的线性分类器可以采用下述决策规则： 则则快策则快策可将任意分到某一类或拒绝  定义了一个决策面，判别函数  可以看成是特征空间中某点  到超平面的距离的一种代数度量  得到从原点到超平面  的距离  &gt; 注：超平面的正方向由权向量  指向确定，它的位置由阈值权  确定。判别函数  正比于  点到超平面的距离。判别式  大于零则  归类到  所指向的类别。\n2. Fisher线性判别分析（LDA）\n把所有样本都投影到一个方向上，然后在一维空间中确定一个分类的阀值，并使投影后两类相隔尽可能远，而同时每一类内部的样本又尽可能聚集。\n\n寻找一个投影方向  ，投影后的样本变成  投影后的一维空间，类均值向量： 𝕪𝕩 类间离散度：  类内离散度： 𝕪 用原样本空间类间离散度矩阵和类内离散度矩阵表示：  &gt; 是类间协方差矩阵 &gt; &gt; 是类内协方差矩阵 &gt;𝕩𝕩\n这一表达式被称作广义 Rayleigh 商。对  幅值的调节并不会影响  的方向，因此可把优化问题转化为：  拉格朗日函数  极值处  可得极值解  满足  假定  是非奇异的(样本数大于维数时通常是非奇异的)，可得  是矩阵  的本征向量。  代入  有 \n可知  的方向由  决定，因此有最优投影方向  &gt;FDA投影方向也直接求微分求得： &gt; &gt;即 &gt; &gt;由于  是标量，非奇异的条件下解满足 &gt;\n当样本是正态分布且两类协方差矩阵相同时最优贝叶斯分类器是线性函数   如果用  估计 , 用  估计 ，则 FDA 所得的方向实际就是最优贝叶斯决策的方向，\n\n其中 \n\n可以用  来作为分类阀值。  决策规则为： ，则 &gt;注意⚠️：这里阀值的尺度确定了， 的尺度也就确定了。\n\n如果先验概率相同，则可以采用阈值:  或者  其中， 是所有样本投影后的均值。\n\n注意到  是⼀组随机变量的和，因此根据中⼼极限定理，我们可以做出⾼斯分布的假设。即：在样本不是正态分布时，这种投影方向和阈值并不能保证是最优的，但通常仍可以取得较好的分类结果。在先验概率不同时，分界点向先验概率小的一侧偏移。FDA本身并不对样本的分布作任何假设，但在很多情况下，当样本维数比较高且样本数也比较多时，投影到一维空间后样本接近正态分布。这时可以在一维空间中用样本拟合正态分布，用得到的参数来确定分类阈值。\n\n3. 感知器(perceptron)\nFDA通过先确定方向再确定阀值的方法设计分类器。感知器可直接得到完整的线性判别函数。 定义增广的样本向量：  增广的权向量  则线性判别函数变为 \n决策规则 则 定义规范化增广样本向量  若若 决策规则变为  考虑噪声，引入余量  ：  \n定义惩罚项。  解向量   梯度下降方法迭代求解  即  即在每一步迭代把错分样本按照某个系数加到权向量上。通常每次只修正一个样本的固定增量法效率更高。即随机梯度下降： (1) 任意选择初始的权向量 ，置 ； (2) 考查样本  ，若 , 则 ，否则继续； (3) 考査另一个样本，重复 (2)，直至对所有样本都有 , 即  。\n如果考虑余量 ，只需将错分判断条件变成  即可。对于线性可分的样本集，采用这种梯度下降的迭代算法，经过有限次修正后一定会收敛到一个解向量  。图示这种单步的固定增量法采用的修正步长  。为了减少迭代步数可以使用可变的步长，比如绝对修正法采用步长:  样本不可分但多数可分时，让步长按照一定的启发式规则逐渐缩小，可以强制算法收敛。\n4. 最小平方误差判别\n求解一个  使不满足不等式的样本尽可能少，为简化计算引入待定常数   矩阵形式  其中   是增广的样本向量的维数,  。通常情况下, ，方程属于矛盾方程组，无法求得精确解，方程组的误差为 。可求方程组的最小平方误差解，即 :  最小平方误差( MSE )准则函数：  * 伪逆法求解：  在极值处对  的梯度为零\n 其中  是长方矩阵  的伪逆。可得  * 梯度下降法求解： (1) 任意选择初始的权向量 , 置 ; (2) 按照梯度下降的方向迭代更新权向量\n 直到满足  或者  时为止，其中  是事先确定的误差灵敏度。参照感知器算法中的单步修正法，对最小平方误差准则，也可以采用单样本修正法来调整权向量  其中,  是使得  的样本。这种算法称作 Widrow-Hoff 算法，也称LMS 算法\n\n选择不同的  会带来不同的结果。\n\n当同一类样本的  选择为相同的值，解等价于 Fisher 线性判别的解,把样本和权向量都还原成增广以前的形式后有:\n\n\n其中, 、 是两类各自的均值向每,  是总类内离散度矩阵。\n\n当  的选择为第一类样本对应的  都是 , 第二类样本对应的  都是  时, 阀值  为样本均值在所得一维判别函数方向的投影, 即\n\n\n其中 、 分别是第一类和第二类的样本数,  是样本总数,  是全部样本的均值, 即 \n\n当对所有样本都取 , 那么当  时, MSE 算法的解是贝叶斯判别函数\n\n\n的最小平方误差逼近。即,下面定义的均方逼近误差  在  时取得最小值,其中  表示由  个 1 组成的列向量。\n\n5. 最优分类超平面与线性支持向量机\n最优超平面能将训练样本没有错误地分开，并且分类间隔最大。最优超平面定义的分类决策函数：  \n确定  的尺度，即  有待求解问题  对每个样本引入一个拉格朗日系数 。优化问题转化为  最优解在  的鞍点上取得。由拉格朗日对偶性，上式等价于：  对求偏导得最优解处  且  代入拉格朗日泛函 (1) 可得最优超平面的对偶问题  这是一个对  的凸二次优化问题，通过对偶问题的解  可以求出原问题的解\n\n维数较大时常采用SMO算法\n\n 即最优超平面的权值向量等于训练样本以一定的系数加权后进行线性组合。根据 KKT 条件，拉格朗日泛函的鞍点处满足：  实际只有  的样本参与加权求和，这些样本被称作支持向量，对于这些支持向量来说有  理论上  可以根据任何一个支持向量求得。在实际的数值计算中，通常采用所有  非零的样本用式求解  后再取平均。对比感知器算法，也可以把最优超平面等价地看作是在限制权值尺度的条件下求余量的最大化。\n\nKKT 条件：  \n拉格朗日得到的是一个必要条件！！！\n\n\n大间隔与推广能力：\n期望风险和经验风险之间满足   称作置信范围，它与样本数  成反比，与参数  成正比。参数  称作 VC 维，对于规范化的分类超平面， 时, VC 维有上界：  其中，  是样本特征空间中能包含所有训练样本的最小超球体的半径，  是样本特征的维数。最大化分类间隔也就等价于最小化  ，因此，支持向量机中最大分类间隔的准则，是为了通过控制算法的 VC 维实现最好的推广能力。在这个意义下，所得的分类超平面是最优的。\n\n\n线性不可分时 引入松弛变量   增加对错误的惩罚项，定义广义最优分类面的目标函数：  转化为以下拉格朗日泛函的鞍点问题，其中,  是拉格朗日乘子：  ---\n\n\n\n\n可得对偶问题：  其中：  得解向量 :  广义最优分类面的判别函数：  根据KKT条件, 鞍点满足以下两套条件：  多数  为 0 , 只有 的样本才会使  。这些样本分为两种,一种是分类正确但处在分类边界面上的样本, 它们  ; 另外一种则是分类错误的样本, 它们  。通过  的样本可求得  。这两部分  的样本都是支持向量，有时也把   的支持向量叫做边界问量。由于广义最优分类面可以兼容线性可分情况下的最优分类面,所以人们通常采用的支持向量机都是考虑广义最优分类面的形式。目标函数式中只有  的二次项和一次项，这是一个对  在等式和不等式约束下的二次优化问题，具有唯一的极值点。\n6. 多线性分类器\n多个两类分类器的组合 : 1，“一对多\" ，  个两类分类器就可以实现  个类的分类。会有训练样本不均衡的问题。一些区域内的分类也可能会出现歧义。 2，“逐对\"分类， 个类别, 共需要  个两类分类器。不会出现两类样本数过于不均衡的问题, 决策歧义的区域通常要比“一对多\"分类器小。 3， 如果对所研究的类别有较好的认识, 能够根据类别间的内在关系把它们分级合并成多个两类分类问题, 则可以用二叉树来构建多个两类分类器。\n\n很多分类器在最后的分类决策前得到的是一个连续的量,分类是对这个量用某个阀值划分的结果, 比如所有线性分类器都是最后转化为一个线性判别函数  与某一阈值(通常是 0) 比较的问 题。SVM 也是这样一种分类器。在很多线性分类器中,一个正确分类的样本,如果它离分类面越远,则往往对它的类别判断就更确定,因此可以把分类器的输出值看作是对样本属于某一类别的一种打分。利用这种分类器, 可以用  个一对多的两类分类器来构造多类分类系统， 即每个类别对应一个分类器，其输出是对样本是否属于  类给出一个判断。在多类决策时，如果只有一个两类分类器给出了大于阈值的输出, 而其余分类器输出均小于阈值,则把这个样本分到该类。更进一步,如果各个分类器的输出是可比的,那么可以在决策时直接比 较各个分类器的输出, 把样本赋予输出值最大的分类器所对应的类别。(但是需要注意, 对很多分类器来说,如果它们是分别训练的，其输出值之间并不一定能保证可比性)\n\n7. 多类线性判别函数：\n对  类设计  个判别函数  决策规则： 若则 增广形式：  其中： 为增广权向量。 多类线性机器不会出现有决策歧义的区域。考虑多类线性可分情况，可用与感知器算法类似的单样本修正法来求解线性机器。 (1) 任意选捀初始的权向量 , 置  。 (2) 考查某个样本 ，若 , 则所有权向量不变；若存在某个类, 使  , 则选择  最大的类别 , 对各类的权值进行如下的修正   是步长,必要时可以随着  而改变。 (3) 如果所有样本都分类正确, 则停止; 否则考査另一个样本, 重复(2)。 如果样本集线性可分,则该算法可以在有限步内收敛于一组解向量。与感知器算法一样, 当样本不是线性可分时, 这种逐步修正法不能收敛，人们可以对算法作适当的调整而使算法能够停止在一个可以接受的解上，比如通过逐渐减小步长而强制使算法收敛。 同样, 也可以像在感知器算法中那样引人余量, 即把  变为 \n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第三章：概率密度函数的估计","url":"/2022/04/28/Pattern%20Recognition/%E4%B8%89%EF%BC%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1/","content":"\n\n\n\n概率密度函数的估计\n1. 概念\n\n基于样本的两步贝叶斯决策：先通过训练样本估计概率密度函数，再通过统计决策进行类别判定。\n参数估计：最大似然估计；贝叶斯估计。\n非参数估计：直方图法，近邻法，Parzen窗法。\n统计量：针对不同要求构造出样本的某种函数，这种函数在统计学中称为统计量。\n参数空间：总体分布未知参数的全部可容许值组成的集合。\n点估计，估计量，估计值： 点估计构造一个统计量  作为参数  的估计 ，  称为  的估计量。如果  是属于类别  的几个样本观察值，代人统计量  就得到对于第  类的  的具体数值，这个数值称为  的估计值。\n区间估计：要求用区间  作为  可能取值范围的一种估计，这个区间称为置信区间。\n无偏性、有效性：对于多次估计来说，估计量能以较小的方差平均地表示其真实值，并不能保证具体的一次估计的性能。\n一致性：保证当样本数无穷多时，每一次的估计量都将在概率意义上任意地接近其真实值。\n\n2. 最大似然估计\n假设：样本独立同分布采样得到且概率分布函数形式已知。有似然函数：\n 最大似然估计量  对数似然函数  当  有梯度算子  对梯度求导并令其等于零  即  方程的解就是对数似然函数的极值点。有时上述方程组会有多个解，其中使得似然函数最大的那个解才是最大似然估计量。此外，并不是所有的概率密度形式都可以用上面的方法求得最大似然估计。\n2.1. 正态分布下的最大似然估计\n单变量正态分布:\n 要估计的参数 ，用于估计的样本  。则有：  由（1）解得：  &gt;对于多元正态分布，均值和方差的最大似然估计是： &gt; &gt;最大似然估计量是平方误差一致估计量，不一定是无偏估计量。上例中  是无偏的，而  的无偏估计为： &gt; &gt;\n3. 贝叶斯估计与贝叶斯学习\n3.1 贝叶斯估计：\n把待估计的参数本身也看作随机变量。当用  来作为估计时总期望风险为  对所有的样本求条件风险最小，即  定义损失函数常用平方误差：  &gt;采用平方误差损失函数时，在样本集  下  的贝叶斯估计量  是给定  下  的条件期望： &gt;\n\n最小平方误差损失函数下，贝叶斯估计步骤 (1) 确定先验分布密度  。 (2) 假设样本是独立同分布的，则有联合分布：  (3) 利用贝叶斯公式求  的后验概率分布  (4) 得  的贝叶斯估计量  也可由后验概率分布直接得到样本的概率密度函数：  如果完全没有先验知识，即认为  为均匀分布，则  完全取决于  。如果先验知识非常强 ，除非  的似然函数为0，否则最后的估计就是，样本不再起作用。\n\n共轭：  为正态分布时，  也为正态分布。\n\n3.2 贝叶斯学习：\n贝叶斯学习则把贝叶斯估计的原理用于直接从数据对概率密度函数进行迭代估计。已有贝叶斯估计量：\n 其中：  当  时, 有  可得递推公式：  先验概率记作 。随着样本数的增加，可以得到一系列对概率密度函数参数的估计  称作递推的贝叶斯估计。如果随着样本数的增加，后验概率序列逐渐尖锐，逐步趋向于以  的真实值为中心的一个尖峰，当样本无穷多时收敛于在参数真实值上的脉冲函数，则这一过程称作贝叶斯学习。此时，估计的样本概率密度函数也逼近真实的密度函数，即： \n3.3 正态分布的贝叶斯估计：\n一维正态分布模型，假设均值  是待估计的参数，方差  已知，分布密度为：  假定均值  的先验分布是正态分布，其均值为 , 方差为  ，即  ---\n对均值  进行估计：  分子部分：  可见  也是一个正态分布，即：  &gt;其中的参数满足 &gt; &gt;整理后得 &gt;\n可得贝叶斯估计值：  也可由后验分布直接求出样本的密度函数：  贝叶斯估计不但使用样本中提供的信息进行估计，还能把待估计参数的先验知识融合进来，并且能够根据数据量大小和先验知识的确定程度来调和两部分信息的相对贡献。\n4. 概率密度估计的非参数方法\n对样本的分布并没有充分的了解，难以给出密度函数形式的情况下，需要非参数估计。即不对概率密度函数的形式作任何假设，而是直接用样本估计出整个函数。这种估计只能是用数值方法取得，无法得到完美的封闭函数形式。从另外的角度来看，概率密度函数的参数估计实际是在指定的一类函数中选择一个函数作为对末知函数的估计，而非参数估计则可以看作是从所有可能的函数中进行的一种选择。\n4.1 直方图法\n\n把  维向量样本  的每个分量在其取值范围内分成  个等间隔的小窗。则会得到  个体积为  的小舱。\n统计落人每个小舱内的样本数目  。\n把每个小舱内的概率密度看作是常数，并用  作为其估计值， 为样本总数。\n\n\n假定  附近位置上落入小舱的样本个数是 ，当样本趋于无穷多时  收敛于  的条件是： (1)  (2)  (3) \n\n直⽅图⽅法，⼀个明显的问题是估计的概率密度具有不连续性，这种不连续性是因为小窗的边缘造成的。直⽅图⽅法的另⼀个主要的局限性是维度上的问题。\n\n4.2 Kn 近邻法 :\n根据总样本确定一个参数  。在求  处的密度估计  时，调整小舱体积，直到小舱恰好落入  个样本  #### 4.3 Parzen 窗法 :\n假设  是  维特征向量，每个小舱是一个超立方体，它在每一维的棱长都为  ，则小舱的体积是：  定义  维单位方窗函数： 若其他 落入以  为中心的超立方体内的样本数为：  对于任意一点  的密度估计的表达式：  ---\n定义核函数（窗函数)：  它反映了一个观测样本  对在  处的概率密度估计的贡献，与样本  与  的距离有关, 可记作  。概率密度估计就是在每一点上把所有观测样本的贡献进行平均，即  估计函数需满足密度函数的要求，显然这只需要核函数本身满足要求即可，即： 且 以上定义的立方体核函数满足这一条件。Parzen 窗估计也可以看作是用核函数对样本在取值空间中进行插值。\n\n多种核函数：\n\n方窗\n\n若其他\n​  为超立方体的棱长。\n\n高斯窗（正态窗）\n\n\n​ 即以样本  为均值、协方差矩阵为  的正态分布函数。一维情况下则为 \n\n超球窗\n\n若其他\n​  是超球体的体积,  是超球体半径。\n\n高斯窗示例：\n\n\n这些窗函数都有一个表示窗口宽度的参数(平滑参数）, 反映了一个样本对多大范围内的密度估计产生影响。当被估计的密度函数连续时，在核函数及其参数满足下列条件下，Parzen 窗估计是渐近无偏和平方误差一致的。\n\n对称且满足密度函数条件、有界、核函数取值随着距离的减小而迅速减小\n对应小舱的体积随着样本数的增加而趋于零，但需慢于  趋于零的速度。\n\n\n作为非参数方法的共同问题是对样本数目需求较大，只要样本数目足够大，总可以保证收敛于任何复杂的未知密度，但是计算量和存储量都比较大。正如到⽬前为⽌讨论的那样，K 近邻⽅法和核密度估计⽅法都需要存储整个训练数据。如果数据集很⼤的话，这会造成很⼤的计算代价。当样本数很少时，如果能够对密度函数有先验认识，则参数估计方法能取得更好的估计效果。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第二章：统计决策方法","url":"/2022/04/28/Pattern%20Recognition/%E4%BA%8C%EF%BC%8C%E7%BB%9F%E8%AE%A1%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/%E7%BB%9F%E8%AE%A1%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/","content":"\n统计决策方法\n\n\n1. 贝叶斯决策\n在类条件概率密度和先验概率已知或可以估计的情况下，通过贝叶斯公式，将类别决策为后验概率大的一类，从而使总体错误率最小。两类问题时：  错误率  为所有服从同样分布的独立样本错误率的期望 如果决策如果决策\n正确率 \n2. 最小错误率贝叶斯决策\n2.1 两类最小错误率贝叶斯决策：\n则\n通过贝叶斯公式，引入似然比阀值  ，有 则 通常采用负对数形式，即 则\n\n错误率： \n\n其中：  为第一类样本决策为第二类的错误率   为第二类样本决策为第一类的错误率 \n\n即两种错误率用相应类别的先验概率加权就得到总的错误率：\n\n2.2 多类最小错误率贝叶斯决策：\n则\n正确率：  错误率： \n3. 最小风险贝叶斯决策\n最小风险贝叶斯决策是一种考虑各种错误造成损失不同时的一种最优决策。对于实际状态为  的向量 ，采取决策  所带来的损失由以下损失函数表示，通常由诀策表给出。  采取决策  的期望损失：  决策需最小化期望风险，即  有最小风险贝叶斯决策： 则\n\n当样本状态是两类且决策也是两类时，决策为 则 引入似然比有： ，则\n4. Neyman-Pearson决策\n4.1 混淆矩阵\n\nScreen Shot 2022-05-26 at 7.09.52 PM\n\n4.2 Neyman-Pearson决策\n固定一类错误率，使另一类错误率尽可能小，即  由Lagrange乘子法  设 、 分别是两类的决策区，有  为最小化 ， 应由下式确定  由此可得决策规则： 则 ---\n由此可见，决策边界上有  其中  的值由决策边界确定，且这个决策边界满足  此外，当  难求得封闭解时，可用数值方法求得 。即通过  到  的映射关系，由 ， 求得 ，并调节  大小得到合适的错误率。 \n\n注意⚠️：书上这部分其实写的有点小问题\n\n4.3 ROC 曲线\n上述三种决策规则的区别只是在于决策阈值的不同，采用不同的阈值，就能达到错误率的不同情况\n\n式 (1) 采用先验概率的比作阈值，达到总的错误率最小，即两类错误率之加权和最小。\n式 (2) 相比式 (1) 阈值中格外考虑了对两类错误率不同的惩罚，实现风险最小。\n式 (3) 通过调整阈值，使一类的错误率为指定数值，而另一类的错误率求最小。\n\n实际上，通过采用不同的阀值，可以使第一类错误率和第二类错误率连续变化。把真阳性率  作为纵坐标，假阳性率  作为横坐标，可得反映随着阈值的变化两类错误率的变化情况的ROC曲线：\n\n有限个测试样例ROC的绘图方法： 给定 个正例和 个反例，根据学习器预测结果对样例进行排序，先把分类阈值设为最大，此时真阳性率和假阳性率均为 0，在坐标  处标记一个点，然后调节分类阈值依次将排序的每个样例划分为正例。设前一个标记点坐标为 ， 划分  个样例后新增  真阳样例  假阳样例，则下个对应标记点的坐标为 ，用线段连接两点，以此递推即可画出ROC曲线，步长  可取值为1。曲线下的面积 AUC (area under ROC curves) 通常用来定量地衡量方法的性能，其可估算为： \n对角线的  是 ，没有任何分类能力。最理想的情况是  沿纵轴到  点后再沿水平直线到  点，此时  。用  可以定量地比较两种不同的方法。从整体上看， 越接近 , 方法的性能越好。除了可以用来比较不同的分类决策方法，ROC 曲线和 AUC 还可以用来评价和选择与分类有关的特征，即通过设定不同的阈值画出单独用一个特征作为指标划分两类时的 ROC 曲线，计算 AUC并通过比较不同特征间的 AUC 来得知哪个特征包含更多的分类信息。\n5. 正态分布下的统计决策\n5.1 多元正态分布\n$$\n\n期望和方差\n\n边缘分布\n\n协方差矩阵总是对称非负定阵（现仅考虑为正定阵的情况）。 =$$\n期望   方差  \n5,2 多元正态分布的性质\n\n参数  和  对分布的决定性\n\n\n\n\n\n等密度点的轨迹为一超椭球面 主轴方向由的本征向量所决定，主轴的长度与的本征值成正比。区域中心由确定，大小由确定。等密度点轨迹是由 到的 Mahalanobis 距离为常数的超椭球面。其大小是样本对于均值向量的离散度度量。\n\n\n\n不相关性等价于独立性 如果多元正态随机向量的协方差阵是对角阵，则  的分量是相互独立的正态分布随机变量。\n边缘分布和条件分布的正态性 多元正态分布的边缘分布和条件分布仍然是正态分布。\n线性变换的正态性\n\n ​ 由于为对称阵，则总可以找到非奇异阵使得各随机变量在新的坐标系中是独立的\n\n线性组合的正态性\n\n\n​ 若  为多元正态随机向量，则线性组合  是一维的正态随机变量\n5.3 正态分布概率模型下的最小错误率贝叶斯决策\n多元正态概率型下，判别函数为  决策面方程 \n\n\n一， 时：\n每类的协⽅差矩阵都相等，⽽且类内各特征间相互独⽴，具有相等的⽅差。 (1) 时，从几何上看，各类样本落人以  为中心的同样大小的一些超球体内，决策方程  判别函数为线性函数，决策面是由  所确定的一个超平面(  与  相毗邻) \n\n其中： \n\n\n 时，超平面通过  与  连线中点并与连线正交。先验概率不相等时，决策面与先验概率相等时的决策面平行，只是向先验概率小的方向偏移，即先验概率大的一类要占据更大的决策空间。\n\n\n\n决策面简化为：  此时决策规则为最小距离分类器 则\n\n\n二， 时：\n\n，几何上各类样本集中于以该类均值  点为中心的同样大小和形状的超椭球内。 判别式 \n\n\n其中： \n\n判别函数仍为线性函数，决策面是由  所确定的一个超平面(  与  相毗邻)，即 \n\n其中： \n\n\n，决策规则为计算出  到每类的均值点  的 Mahalanobis 距离平方 ，最后把  归于  最小的类别  此时决策面通过  与  连线的中点 。若先验概率不相等，  则在  与  连线上向先验概率小的均值点偏移。\n\n\n注：判别式的正向方向为指向的方向，判别式大于零则归类到所指向的的类别。\n\n\n三， 时：\n判别式 \n\n其中： 矩阵维列向量\n\n这时判别函数式将  表示为  的二次型。决策面是由  所确定的一个超二次曲面(  与  相毗邻)，即  决策面随着  的不同而呈现为某种超二次曲面，即超球面、超椭球面、超抛物面、超双曲面或超平面。\n6. 错误率的计算\n最小错误率贝叶斯决策的错误率是  类条件概率密度函数解析表达式较复杂时，计算错误率过于复杂。在处理实际问题时对错误率的计算或估计的方法可概括为： (1) 按理论公式计算； (2) 计算错误率上界； (3) 实验估计。\n6.1 正态分布下错误率的计算\n最小错误率贝叶斯决策规则的负对数似然比形式: \n决策面是  的二次型，当各协方差阵相等时，决策面就变成的线性函数，决策规则简化为：  易知服从一维正态分布，对于 ，可计算出决定一维正态分布的参数均值  及方差   同样对于  :  错误率计算 其中 \n6.2 高维独立随机变量错误率估计\n当  维随机向量  的分量间相互独立时，有：  负对数似然比   &gt; 其中 &gt; \n由中心极限定理，  较大时无论  密度函数如何，  的密度函数总是趋于正态分布，由此可得  的均值  及方差  ：  由于  和  都是一维随机变量  的函数，在大多数情况下，计算这些参数相对比较容易，即使非正态情况亦是如此，所以可以把  近似看成是服从  的一维正态分布的随机变量，再由 近似算出错误率。\n6.3 离散概率模型下的统计决策\n一阶马尔可夫链：第  时刻上的取值仅依赖于第  时刻的取值  转移概率  对一个长度为  的序列，我们观察到这个序列的概率是 \n 岛记作 “  \"，非  岛一类记作 “” 。判别通常采用对数似然比 :  &gt; 两类的转移概率密度可由下式估计 &gt; 和 &gt;\n判别阈值选取： 阚值的选取可以根据先验概率，也可以根据最小风险的原则确定，或者根据对两类错误率的特殊要求决定。如果两类的先验概率相同且两类错误的损失相同，则对数似然比决策的阈值就是0。在这里，由于概率模型是用数值方法估计的，很难从理论上计算错误率。\n\n在实际应用中，常把训练数据代到中，统计所有训练样本的似然比取值的分布。选用不同的阈值来做决策就会导致不同的错误情况，可从直方图上确定满意的阈值，或者通过变动不同阈值画出ROC曲线来决定阈值选择。\n6.4 隐马尔可夫模型\n\n对前  个位置，  表示当第  位置对应隐状态为  时能得到的最大概率。  写成递归形式:  最大似然路径 可通过回溯的方法求得，即：  这种做法叫Viterbi算法\n\n详见：\n\n"},{"title":"第一章：概论","url":"/2022/04/28/Pattern%20Recognition/%E4%B8%80%EF%BC%8C%E6%A6%82%E8%AE%BA/%E6%A6%82%E8%AE%BA/","content":"\n\n\n\n概论:\n1. 模式与模式识别\n通过以往对特定事物的认知来识别目标中的特定事物，例如从心电图中各波的形状判断病人的健康情况。\n2. 模式识别的主要方法\n\n基于知识的方法：根据样本特征与类别间关系的认知建立推理系统，对未知样本类别决策。\n基于数据的方法：依据训练样本建立不完全确定内部机理的表示与关系的系统，对未知样本类别决策。\n\n3. 监督模式识别与非监督模式识别\n\n监督模式识别：利用了有标签的训练样本。\n非监督模式识别：没有利用有标签的训练样本，对训练样本采用不同的划分方法可能导致不同的结果。\n\n4. 模式识别系统举例\n\n语音识别：对一系列连续的音素进行分类，需考虑音素之间的相互影响。例如利用多阶隐马尔可夫模型。\n说人话识别：与语言识别基本原理相同，只是分类目标由语音变成了说话人。\n字符与文字识别：OCR（detection-classification）等。单字识别是OCR的基础，将图片向多方向投影得到像素密度即数量特征；根据对汉字结构的认知提取有效特征点并编码成数字特征。特征提取后每个字就是一个特征向量代表的样本，接下来涉及到多分类问题。分类器设定通常需要结合对文字结构的认知（旋转和尺度不变性）。\n复杂图像中特定目标的识别：目标检测方法判断每个子图像是汽车还是背景，检测出汽车后可追踪其在连续图像中的运动轨迹来识别是否有违章行为等；可根据汽车图像识别出车牌位置再利用数字识别识别车牌号等；路人检测再进行人脸识别，行为识别等。\n根据地震勘探数据对地下储层性质的识别：在探井处利用地震信号提取特征并结合地下储层性质类别建立分类器；探井数不足以用来训练时可利用非监督学习方法对地震勘测信号聚类划分，由地质学家分析划分来实现对储层性质的识别。\n利用基因表达数据进行癌症的分类：利用基因表达作为病例特征研究病例之间的分类和聚类；利用病例表达作为基因特征研究基因之间的分类和聚类等。\n\n5. 模式识别系统的典型构成\n\n处理监督模式识别的一般步骤 分析问题，原始特征提取，特征提取与选择，分类器设计，分类决策；\n处理非监督模式识别的一般步骤 分析问题，原始特征提取，特征提取与选择，聚类分析，结果解释；\n\n本章概要：模式，样本，样本集，类或类别，特征，已知样本，未知样本。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding","url":"/2021/06/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/BERT/","content":"\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n\nBert\n","categories":["README"],"tags":["BERT"]},{"title":"README","url":"/2021/06/01/Deep%20Learning/README/","content":"\n打工是不可能打工的，这辈子都不可能打工的 打工是不可能打工的，这辈子都不可能打工的 打工是不可能打工的，这辈子都不可能打工的\n\n\nREADME\n","categories":["README"],"tags":["README"]}]