[{"title":"README","url":"/2022/04/26/README/","content":"\n打工是不可能打工的，这辈子都不可能打工的 打工是不可能打工的，这辈子都不可能打工的 打工是不可能打工的，这辈子都不可能打工的\n\n\nREADME\n","categories":["README"],"tags":["README"]},{"title":"第十章：模式识别系统的评价","url":"/2022/04/28/Pattern%20Recognition/%E5%8D%81%EF%BC%8C%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BB%B7/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BB%B7/","content":"\nThe project will be configured before the first of June\n\n\n第十章：模式识别系统的评价\n错误率估计：\n通常采用实验方法估计分类器的错误率：训练错误率，测试错误率。\n交叉验证：\n​ 总样本不变，随机选用一部分样本作为临时的训练集，用剩余样本作为临时的测试集，得到一个错误率估计；然后随机选用另外一部分样本作为临时训练集，其余样本作为临时测试集，再得到一个错误率估计，如此反复多次，最后将各个错误率求平均，得到交叉验证错误率。一般让临时训练集较大，临时测试集较小，这样得到的错误率估计就更接近用全部样本作为训练样本时的错误率。而测试集过小带来的错误率估计方差大的问题通过多轮实验的平均可以得到一定的缓解。\nk轮n倍交叉验证：\n​ 总样本随机划分为n个等份，在一轮实验中轮流抽出其中的1份样本作为测试样本，用其余（n-1）份作为训练样本，得到n个错误率后进行平均，作为一轮交叉验证的错误率；由于对样本的一次划分是随意的，往往进行多轮这样的划分（比如k轮），得到多个交叉验证错误率估计，最后将多个估计再求平均。\n留一法交叉验证：\n​ 样本数较少时最常用的方法。每轮实验拿出一个样本来作为测试样本，用其余的N一1个样本作为训练样本集，训练分类器。下一轮把之前测试的样本放回，拿出另外一个样本作为测试样本，用剩余的N一1个样本作训练，再对留出的样本作测试，依次类推。全部N轮实验完成后，统计总共出现的测试错误数占总样本数的比例就是留一法交叉验证错误率。\n自举法与.632估计：\n自举样本集包含原样本集中63.2%左右的样本。训练错误率是对真实错误率偏乐观的估计，而自举错误率是偏保守的估计。将这两种估计按照一定的方式结合起来有.632 估计。是在全部样本上的训练错误率， 是自举错误率。 是对错误率更好的估计。\n\n事实上，考虑样本随机性，如果仅基于交叉验证，不存在错误率估计量方差的无偏估计。\n用扰动重采样估计SVM错误率的置信区间：\n​ 之所以单纯靠样本划分或重采样无法获得对分类器错误率变换范围的无偏估计，是因为在划分或重采样得到的数据集之间存在不可避免的相关性。这一问题可以通过适当引人扰动的方法来解决。\n特征提取与选择对分类性能的影响：\nCV1：\n把所有样本都用来进行特征选择与提取，而后把所选择和提取的特征固定下来，再把样本分成训练集和测试集。\nCV2：\n​ 在未作任何特征选择与提取前把测试样本和训练样本分开，在每一轮里只用训练样本选择和提取特征。采用CV2策略进行交叉验证，可以得到对包括特征选择与提取部分在内的模式识别系统性能的真实估计，但是却没有得到一组唯一的用于分类的特征。这时，一种做法是利用所有样本重新进行一次特征选择与提取，得到唯一的特征组合，用所有样本设计分类器；另外一种做法是，将CV2交叉验证中得到的各个特征组合方案进行综合，从中选择在各轮交叉验证中被选中次数最多的若干特征组成最后的特征集，用这些特征在所有样本上设计分类器。\n从分类显著性推断特征与类别的关系：\n​ 随机置换法：在保持已知样本集中两类样本比例不变的情况下，随机打乱样本的类别标号。再用同样的特征选择、提取和分类方法进行分类，得到的分类性能就反映了这样的模式识别方法在无分类信息的数据上的表现。多次重新随机置换样本类别标号，就可以统计出在没有分类信息情况下模式识别分类性能的空分布，然后把在真实数据上得到的性能估计与这个空分布进行比较，得到分类器性能的随机置换P值。如果该P值很小（比如，通常以小于0.05为参考），则说明在原样本集上得到的分类性能具有统计显著性，初步推断系统S很可能真实存在。\n非监督模式识别系统性能的评价：\n紧致性(compactness)或一致性(homogeneity)：\n​ 最常见的指标是类内方差或者平方误差和。除此还有其他类型的类内一致性度量：类内两两样本之间的平均或最大距离、平均或最大的基于质心的相似度，基于图理论的紧致性度量等。比如,可用指标：  连接性质(connectedness)\n​ 衡量了聚类是否遵循了样本的局部密度分布及相邻的样本是否被划分到同一类。如连接度(connectivity)，即样本中相邻的数据点被划分到同一个聚类中的程度。如果第  个样本与其第  个近邻不在同一个聚类中,则 , 否则为 0 。连接度指标越小越好。  分离度(separation) 可用两类中心距离或两类最近样本之间的距离来计算两类间的距离。分离度指标越大则各类间分离越好。\n综合性质的评价准则: 紧致性与分离度是两个极端,如果不断增加聚类数目,紧致性将会随之增加,但分离度也会相应的减小。因此,可将这两 个指标组合起来, 定义能同时反映类内距离和类间距离的新指标，比如Silhcuette 值:   代表样本  到和它同类的所有样本的平均距离,  表示样本  到其他聚类中最近一个聚类的所有样本的平均距离。所 有样本的 Silhouette 值的平均称作 Silhouette 宽度, 其取值在  之间。越大则聚类效果越好。\nDunn 指数(Dunn index)   是聚类  中最大的类内距离,  是  和  两类中相邻最近的样本对间的距离。  的取值 范围是 , 此指数的目标是最大化。\n​ 由于没有先验认识，非监督学习的目标是多样的，无论采用什么方法混合多个指标，都不可避免地导致某些方面信息的损失。另外一种不同的策略是，同时评价各个指标， 并且仅当一个方法在某一个指标上超出另一个方法、同时在所有指标上都等同于或超出另一方法时，才断定该方法在聚类性能上胜过另一方法。这是个多目标优化的问题。可以用多目标优化的方法，来寻找在多个指标上都优胜的聚类方法，或确定方法中的可变参数。\n稳定性：\n​ 即其结果的可重复性。采用重复地随机重采样或者对样本加人随机扰动等方法获得多个不同的样本集，在不同的样本集上实施同样的聚类算法，定义某个统计量来衡量在这些重采样或扰动的样本集上得到的聚类结果的一致性，用它来评价从原始的数据上得到的聚类结果的显著性。\n预测效力：\n​ 将样本随机地划分成两份，两份样本上都各自进行聚类；用其中一份中得到的聚类结果作为临时训练样本，对另外一份中的样本实行最近邻法分类，比较这样的分类与直接在这份样本上的聚类划分之间的重合程度，重合程度越大则聚类结果越稳定。实际应用中，这样的实验通常需要多次重复进行，最后以指标的平均值来作为稳定性的度量。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第九章：非监督模式识别","url":"/2022/04/28/Pattern%20Recognition/%E4%B9%9D%EF%BC%8C%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/","content":"\nThe project will be configured before the first of June\n\n\n第九章：非监督模式识别\n混合模型的估计\n假设条件 :\n\n样本来自类别数为  的各类中;\n每类先验概率  已知；\n类条件概率密度  形式已知；\n末知的仅是  个参数向量  的值。 定义混合密度：   。类条件密度  称为分量密度, 先验概率  称为混合参数。 被观察样本的似然函数定义为：  对数似然函数  最大似然估计  应满足  #### 可识别问题 :\n\n​ 设 ，如果对于混合分布中每个  都有 ，则称密度  是可识别的。常见连续随机变量的分布密度函数都是可识别的，而离散随机变量的混合概率函数则往往是不可识别的，这是由于混合分布中末知参数  的分量数目多于独立方程的数目而造成的。\n计算问题:\n对于可识别的最大似然函数，可以用一般方法求最大似然估计量  。假定似然函数  对  可微，用对数似然函数  对  分别求导  如果当  时  和  的元素在函数上是独立的,并且引进后验概率  则有  得出最大似然估计  满足  其为由  个微分方程组成的方程组，解这个方程组就可以得到参数  的最大似然估计  先验概率 末知时，对  的最大似然值的搜索应该在 且\n如果似然函数可微，利用求条件极值的 Lagrange 乘子法。 对数似然函数：  Lagrange 函数  对  求导并使导数为零，即可解出  的最大似然解  。  利用贝叶斯公式  将  个方程相加  因为  所以   的最大似然估计  进一步推导可得必要条件  其中  ​ 非监督情况下的最大似然解  和 ，原则上可以从以上微分方程组中解出，但实际上要求出闭式解是相当复杂的。所以处理非监督参数估计问题时经常采用迭代法求解。\n高斯混合模型：\n模型中的各个分布是多维正态分布的, ,\n第一种情况：均值向量  末知\n这时参数  就是 , 可以得到最大似然估计解的必要条件。  由于  所以  的最大似然估计  应满足  两边左乘 , 可得  它表明  的最大似然估计是样本加权平均。其中对第  个样本  的权就是属于第  类有多大可能性的 一个估计。如果  对某些样本为 1，而对其余样本为零，那么  是被估计为属于第  类样本的平均。但遗惐的是并没有明显地给出  的值,这是因为其中  是末知的。利用贝叶斯公式  并将  代人，将变成一组十分复杂的非线性联立方程组，求解是相当困难的，而且一般没有唯一解，必须对得到的解进行检验以获得一个实际上使似然函数为最大的解。为解决这一困难，可以应用迭代法。如果有某种方法能得到末知均值的一个较好的初始估计 ，可以得到一种迭代算法来改进估计。  这基本上是一种使对数似然函数极大化的梯度法，分量密度之间的重叠较时收敛较快。显然，这个算法也存在一般梯度法的缺点，即算法得到的不是全局最优解，而是局部最优解，其结果将受到初值  的 影响，甚至可能收玫到鞍点，对运算的结果应注意分析和检验。\n第二种情况:  均末知\n只有类别数目  已知。写出对数似然函数  (不过  是由  所组成)，然后在限制条件  下构造出 Lagrange 函数  对  分别相对于  及  求导,并令导数为零。可得  其中 :  在特殊情况下，即当  来自  类时, , 否则就等于零, 此时有:  ​ 其中  为来自  的样本数,  为来自  的样本。这样上式说明  是 来自  的样本的百分比,  为  类的样本均值,  为  类的样本协方差阵。一般  是介于 0 与 1 之间的数,而且所有的样 本都对估计值起某种作用,这些估计基本上仍然是加权的频数比、样本均值和样本协方差阵。 ​ 解这些方程一般是很困难的，有效的方法还是采用迭代法，即用一个初始估计值计算式 , 然后反复迭代。如果初始估计较好，比如它是用一些标有类别的样本求出的，那么收敛就比较快。但所得结果与初值选择有关, 因而所得的解仍是局部最优解。另一个问题是迭代算法和样本协方差阵求逆都需要很多运算时间。要是有理由假定样本协方差矩阵是对角阵, 那么运算可大大简化, 而且还减少了末知参数的个数。在样本数不多的情况下, 减少末知参数的 个数是十分重要的。如果对角阵假设太勉强, 但可以假定 c 个协方差矩阵都相同的话, 也能减少计算时间。 ​ 不论是参数化的混合模型估计方法, 还是非参数化的单峰子集分离方法，由于涉及样本概率密度估计的问题，都需要较多的样本数或对样本分布的先验知识。这在很多非监督学习问题中是不易满足的。除了这类基于模型的方法，人们还发展了很多直接基于数据进行聚类的方法。\n动态聚类算法 :\n(1)选定某种距离度量作为样本间的相似性度量。 (2)确定某个评价聚类结果质量的准则函数。 (3)给定某个初始分类, 然后用迭代算法找出使准则函数取极值的最好聚类结果。\n 均值算法 :\n\n初始划分  个聚类, , 计算  和 ;\n任取一个样本 , 设 ;\n若 , 则转 (2); 否则继续;\n计算  \n考查  中的最小者 , 若 , 则把  从  移到  中;\n重新计算  和 \n若连续  次迭代  不改变,则停止; 否则转(2)。 这是一个局部搜索算法, 并不能保证收敛到全局最优解。算法结果受到初始划分和样本调整顺序的影响。样本初始划分一般先选择一些代表样本点作为初始聚类的核心,然后根据距离把其余的样本划分到各初始类中。  均值聚类的  是事先给定的。类别数目末知的情况下，有时可以逐一用  来进行聚类，每次聚类都计算出最后的误差平方和 , 考查  随  的变化推断合理的类别数。如果作一条  随  变化的曲线，则曲线的拐点处对应的类别数就是接近最优的聚类数。\n\nISODATA 方法\n​ 与  均值算法相比: 第一，它把全部样本调整完后才重新计算各类的均值。第二，聚类过程中引入了对类别的评判准则，可以根据这些准则自动地将某些类别合并或分裂。 (1) 初始化，设初始聚类数 ，用与  均值法相同的办法确定  个初始中心  (2) 把所有样本分到距离中心最近的类  中,  (3) 若某个类  中样本数过少 ，则去掉这一类，并根据各样本到其他类中心的距离分别合入其他类，置  。 (4) 重新计算均值  (5) 计算第  类样本与其中心的平均距离和总平均距离  (6) 若是最后一次迭代(由参数  确定)，则程序停止，否则: 若 ，则转(7) (分裂); 若 , 或是偶数次迭代，则转(8)(合并)。否则（即既不是偶数次迭代, 又不满足  ) 分裂。 (7) (分裂) (1) 对每个类，求各维标准偏差    为第  个样本的第  个分量,  是第  个聚类中心的第  个分量,  是第  类第  个分量的标准差 (2) 对每个类,求出标准偏差最大的分量 ; (3) 对各类的 , 若存在某个类的  (标准偏差参数), 且  且 , 或 , 则将  分裂为两类, 中心分别为 和 , 置   分裂项可以为  为常数), 也可以是  。 (8) (合并) (1) 计算各类中心两两之间的距离  (2) 比较  与  (合并参数), 对小于  者排序  (3) 从  开始，把每个  对应的  和  合并,组成新类, 新的中心为  并置  。每次迭代中避免同一类被合并两次。 (9) 若是最后一次迭代，则终止; 否则,，迭代次数加 1 , 转 (2)。 (必要时可调整算法参数)\n基于样本与核相似度量的动态聚类算法:\n定义一个样本  到核的距离  。类似于  均值算法定义准则函数为  步骤 1 选择初始划分, 即将样本集划分为 c 类, 并确定每类的初始核  。 步骤 2 若  则  。 步骤 3 重新修正核  。若核  保持不变, 则算法终止; 否则转步骤 2 。 (1) 正态核函数  参数集为 , 样本到核的相似性度量为  (2) 主轴核函数样本的主轴可通过  变换得到。  这里  是和  矩阵的  个最大本征值相对应的本征向量系统。 任何一个样本  与  之间的相似性程度可以用  与  类主轴之间的欧氏距离的平方来度量。  ### 模糊聚类方法 :\n​ 隶属度函数表示一个对象  隶属于集合  的程度。自变量范围是所有可能属于集合  的对象，且  。 对于有限个对象  ，模糊集合  可以表示为:  或  模糊  均值 : 用隶属度函数定义的聚类损失函数:   是一个可以控制聚类结果模糊程度的常数。通常选择  取值在 2 左右。在不同的隶属度定义下最小化损失函数就得到不同的模糊聚类方法。FCM 要求一个样本对于各个聚类的隶属度之和为 1, 即  在上条件下求  的极小值，令  对  和  的偏导数为 0 ，可得必要条件：  和  用迭代方法求解式上式: (1) 设定聚类数目  和参数  (2) 初始化各个聚类中心  (3) 重复下面的运算,直到各个样本的隶属度值稳定： ① 用当前的聚类中心根据计算隶属度函数; ② 用当前的隶属度函数更新计算各类聚类中心。 当算法收敛时,就得到了各类的聚类中心和各个样本对于各类的隶属度值,从而完成了模糊聚类划分。 还可以将模糊聚类结果进行去模糊化,即用一定的规则把模糊聚类划分转化为确定性分类。\n改进的模糊 C 均值算法: 在模糊  均值算法中，由于引人了归一化条件，在样本集不理想的情况下可能导致结果不好。野值的存在将影响迭代的最终结果。为此，提出了放松的归一化条件，使所有样本对各类的隶属度总和为 , 即  在这个新的条件下,计算  仍不变,而  变成  ### 分级聚类方法：\n\n初始化， 每个样本形成一个类;\n合并: 计算任意两个类之间的距离(或相似性)，把距离最小(或相似性最大)的两个类合并为一类，记录下这两个类之间的距离(或相似性),其余类不变;\n重复 (2), 直到所有样本被合并到两个类中。 算法核心问题是如何度量样本之间以及类之间的距离或相似性度量。样本之间采用何种距离或相似性度量， 取决于所面对的问题中特征的物理意义及相互之间的关系。如果特征是欧式空间中的向量, 通常可以用欧氏距离作为距离度量, 或用相关系数作为相似性度量。在两个样本之间距离或相似性度量确定后，有三种方法定义两个类  之间的距离或相似性度量, 也称作类间的连接:\n最近距离 \n最远距离 \n均值距离 \n\n自组织映射神经网络：\n​ SOM网络的神经元节点都在同一层上，在一个平面上呈规则排列。常见的排列形式包括方形网格排列或蜂窝状排列。样本特征向量的每一维都通过一定的权值输入到SOM网络的每一个节点上。神经元节点之间并没有直接的连接，但在神经元平面上相邻的节点间在学习（训练）过程中有一定的相互影响，构成邻域相互作用。神经元节点的计算功能是对输入的样本给出响应。输入向量连接到某个节点的权值组成的向量称作该节点的权值向量。一个节点对输入样本的响应强度，就是该节点的权值向量与输入向量的匹配程度，可以用欧氏距离或者内积来计算，如果距离小或内积大则响应强度大。对一个输人样本，在神经元平面上所有的节点中响应最大的节点称作获胜节点。\n学习算法:  是  维样本向量集合，记所有神经元集合为 ，第  个每个神经元的权值为  (1) 权值初始化: 用小随机数初始化权值向量。注意各个节点的初始权值不能相等。 (2) 在时刻 ，按照给定的顺序或随机顺序加入一个样本，记为  。 (3) 计算神经元响应，找到当前获胜节点  如用欧氏距离作为匹配准则，则获胜节点为  (4) 权值竞争学习。对所有神经元节点,用下述准则更新各自的权值  其中,  是学习的步长,  是两个向量间的欧氏距离，  是节点  与  间的近邻函数值，如果采用方形网格结构，则相当于在节点  的周围定义一个矩形邻域范围  。  (5) 更新步长  和邻域 ，达到终止条件，则算法停止; 否则置 , 继续（2） 这是一个自学习的过程，在学习过程中没有已知的类别标号做引导，也无法定义类似训练误差之类的收敛目标。这个算法终止条件一般是事先确定的迭代次数。为了网络能够更有效地达到自组织状态，步长  和邻域  通常在算法开始时可以设置得大一些，而随着时间  的增加单调减小（Warm up），到算法终止时邻域缩小到只包含最佳节点本身。除了矩形邻域外，还可以使用其他形式的邻域函数，如高斯函数等。在经过了适当的自学习后, SOM 网络会表现出自组织现象: 对某个输入样本 , 对应获胜节点  会逐渐趋于固定。把固定下来的获胜节点  称作样本  的像, 把样本  称作柛经元节点  的原像。一个样本只能有一个像，而一个神经元可能有多个原像, 也可能没有原像。当学习过程终止后，可以统计在每个神经元节点上有多少个原像，即有多少个样本映射到该节点，把这个量叫做像密度。如果把各个节点的像密度按照神经元本来的排列图示出来，就得到一张像密度图。SOM 网络的自组织现象，就是在对样本经过了适当的学习后，每个样本固定映射到一个像节点，在原样本空间中距离相近的样本趋向于映射到同一个像节点或者在神经元平面上排列。 对于有  个样本的样本集，如果邻域函数固定(不随学习时间改变)，则 SOM 学习算法实际是通过梯度下降算法最小化：  ​ 事实上，如果在 SOM 学习算法中取消邻域作用，即只对获胜节点自身做权值修正，SOM就退化为  均值算法的一种随机迭代实现，其中的聚类数  就是神经元结点的数目。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第八章：特征提取","url":"/2022/04/28/Pattern%20Recognition/%E5%85%AB%EF%BC%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/","content":"\nThe project will be configured before the first of June\n\n\n第八章：特征提取\n基于类别可分判据的特征提取 :\n如果采用类别可分性判据作为衡量新特征的准则，则特征提取的问题就是求最优的 , 使：   可是基于类内类间距离或基于概率距离或熵的可分性判据。 采用基于类内类间距离的可分性判据 ，经过  的特征变换后，类内离散度矩阵和类间离散度矩阵分别变为  和 ,则特征提取的问題就是求 , 使下列准则最优 :  这些准则得到的最优变换矩阵是相同的: 设矩阵  的本征值为 ，按大小顺序排列$ {1} {2} _{D}$，选前  个本征值对应的本征向量构成的变换阵就是在这些准则下的最优变换阵 。 也可以采用基于概率距离的判据或基于熵的判据作为准则来进行特征提取。但一般情况下只能靠数值求解，在数据服从正态分布并满足某些特殊条件时可以得到形式化的解。\n主成分分析法 :\n​ 从一组特征中计算出一组按重要性从大到小排列的新特征，它们是原有特征的线性组合，并且相互之间不相关。 记  为  个原始特征设新特征  是这些原始特征的线性组合：  统一尺度：  矩阵形式是 :  求解最优的正交变换  使新特征  的方差达到极大值。正交变换保证了新特征间不相关，而新特征的方差越大，特征也就越重要。  方差  拉格朗日函数  求导得最优解  满足 :  即  是矩阵  的本征向量,  是对应的本征值。  最优的  是  的最大本征值对应的本征向量。  称作第一主成分，它在原始特征的所有线性组合里是方差最大的。对第二个新特征，它除了满足和第一个特征同样的要求（方差最大、模为 1），还必须与第一主成分不相关,即：  可得：  再考虑到 , 不相关的要求等价于要求  和  正交 在  和  的约末条件下最大化  的方差，可以得到  是  的第二大本征值对应的本征向量，  称作第 二主成分。协方差矩阵  共有  个本征值 , 从大到小排序为  。按照与上面相同的方法， 可以得出由对应这些本征值本征向量构造的  个主成分   ​ 它等于各个原始特征的方差之和。变换矩阵  的各个列向量是由  的正交归一的本征向量组成的, 因此, , 即  是正交矩阵。从  到  的逆变换是  取前  个主成分，可知这  个主成分所代表的数据全部方差的比例是  ​ 数据中的大部分信息常集中在较少的几个主成分上。可根据本征值谱图来决定选择几个主成分来代表全部数据。在很多情况下，可以事先确定希望新特征所能代表的数据总方差的比例,比如  或  ，根据上式来试算出适当的  。 计算中常把主成分进行零均值化，这种平移并不影响主成分的方向。即：  ###  变换 ：\n​  变换基本的形式原理上与主成分分析是相同的，但  变换能够考虑到不同的分类信息实现监督的特征提取。  变换是从  展开引出的。对  维随机向量 , 可以用一个完备的正交归一向量系  来展开  其中  有：  用有限的  项  来逼近   与原向量的均方误差是  记 , 即  的二阶矩阵, 则  要在正交归一的向量系中最小化这一均方误差,就是求解下列优化问题  采用拉格朗日法，得到无约束的目标函数  对各个向量求偏导并令其为零, , 得 :  即  是矩阵  的本征向量满足   是矩阵  的本征值, 均方误差为  ​ 把矩阵  的本征值按从大到小排列，选前  个本征值对应的本征向量，即可使表示样本均方误差最小。   组成了新的特征空间，样本  在这个新空间上的展开系数  就组成了样本的新的特征向量。矩阵  称作  变换的产生矩阵。这里得到的  个新特征与主成分分析中的  个主成分很相似，当原特征为零均值或者对原特征进行去均值处理时，二者等价。\n多维尺度法：\n​ 根据样本之间的距离关系或不相似度关系在低维空间里生成对样本的一种表示。如果样本之间的关系是定义在一定的特征空间上的，那么这种表示也就实现了从原特征空间到低维表示空间的一种变换。MDS分为度量型和非度量型两种类型。\n古典尺度法：\n​ 给定一个两两点之间距离的矩阵，确定这些点在空间里的坐标。假定给定的距离矩阵是欧氏距离。设有  个  维样本 , 所有样本组成的  维矩阵是 , 样本间两两内积组成的矩阵为  。样本  与  之间的欧氏距离为  所有两两点之间的欧氏距离组成的矩阵为：   是矩阵  的对角线元素组成的向量, 即   现已知矩阵 求  。对坐标的平移不会影响样本间的距离,假设所有样本的质心为坐标原点, 即  定义中心化矩阵   是单位对角阵。显然  在式  的假设下,有  对  两边乘以中心化矩阵, 得  可得内积矩阵  这种做法也称作双中心化(double centering)。 如果  是由欧氏距离组成的矩阵,则  是对称矩阵, 可以用奇异值分解的方法来求解   其中,  是由矩阵  的本征向量组成的矩阵,  是以  的本征值为对角元素的对角阵  如果样本不是中心化的，则只要知道样本的均值向量  就可以求得各个样本原来的坐标  如果要用  维空间来表示这些样本, 则可以按照本征值从大到小排序  ​ 用  组成 , 只用这些本征值对应的本征向量组成  。如果已知样本集,，从中计算出 ，再用古典尺度法得到  的低维表示，结果与主成分分析相同。\n度量型MDS：\n古典尺度法是度量型 MDS 的一种特殊形式。  作为目标函数,则当  是欧氏距离时,得到的低维空间表 示就是样本在主成分上的投影。很多 MDS 压力函数可以统一为：   是对样本对的加权, 比如   是预先定义的函数。比如，如果希望  与  之间是线性关系，则可以选  另外一种常用的压力函数形式是  scale 是一个尺度因子, 可取为 scale , 此时压力函数称作 Kruskal 压力。\n非度量型 MDS :\n非度量型 MDS 就是追求样本的坐标能反映出定性的顺序信息。也需要最小化上述形式的目标函数但其中的函数  或  只需要是某种单调函数或弱单调函数即可。这种单调函数可以通过所谓 “单调回归\"来实现。目标是，用低维空间坐标表示的样本点之间的距离关系，尽可能接近地反映原相异度矩阵所表示的顺序关系。\nMDS 在模式识别中的应用 :\n​ 通常用 MDS 在二维或三维上可视化地显示一组复杂样本之间的关系。如果样本间的距离/相异度矩阵是定义在某一特征空间中的，那么 MDS 也可以看作是样本的一种特征变换。\n非线性变换方法\n核主成分分析：\n\n通过核函数计算矩阵 , 其元素为\n解矩阵  的特征方程 \n解矩阵  的特征方程  得到归一化本征向量  按照对应的本征值从大到小排列。本征向量的维数是 , 向量的元素记作   。由于引人了非线性变换,这里得到的非零本征值数目可能超过样本原来的维数。根据需要选择前若干个 本征值对应的本征向量作为非线性主成分。第  个非线生主成分是 \n计算样本在非线性主成分上的投影。对样本 , 它在第  个非线性主成分上的投影是  样本  在前  个非线性主成分上的坐标就构成样本在新空间的表示  。\n\nIsoMap\n​ 当样本在高维空间中按照某种复杂结构分布时，直接计算两个样本点之间的欧氏距离，就损失了样本分布的结构信息。如果样本分布较密集，可以假定样本集的复杂结构在每个小的局部都可以用欧式空间来近似。计算每个样本与相邻样本之间的欧氏距离；对两个不相邻的样本，寻找一系列两两相邻的样本构成连接这两个样本的路径，用两个样本间最短路径上的局部距离之和作为两个样本间的距离。这种距离称作测地距离。有了样本间的距离矩阵, 就可以用度量型 MDS 等方法映射到低维空间。\nLLE\n\n在原空间中, 对样本  选择一组领域样本；\n用这一组邻域样本的线性加权组合重构  ，得到一组使重构误差  最小的权值  ；\n在低维空间里求向量  及其邻域的映射,使得对所有样本用同样的权值进行重构得到的误差  最小。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第七章：特征选择","url":"/2022/04/28/Pattern%20Recognition/%E4%B8%83%EF%BC%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","content":"\nThe project will be configured before the first of June\n\n\n第七章：特征选择\n1. 类别可分性准则：\n定义与错误率有关又便于计算的类别可分性准则 , 用来衡量在一组特征下第  类和第  类之间的可分程度，其应该满足： (1) 判据应该与错误率(或错误率的上界)有单调关系。 (2) 当特征独立时,判据对特征具有可加性,即  (3) 应具有度量特性 当时当时 (4) 对特征具有单调性, 即加入新的特征不会使判据减小  #### 1.1 基于类内类间距离的可分性判据：\n 基于类内类间距离的判据：\n (各类之间的平均平方距离)    \n1.2 基于概率分布的可分性判据：\n​ 基于类内类间距离的可分性判据没有直接考虑样本的分布情况，很难与错误率建立直接的联系。分布密度的交叠程度可用  及  这两个分布密度函数之间的距离  来度量。 概率距离度量函数应满足: (1)  为非负, 即 ; (2) 当两类完全不交叠时  取最大值, 即对所有  有  时 , 则  (3) 当两类分布密度相同时，  应为零，即若  ，则  。 概率距离度量: Bhattacharyya 距离 :  理论上的错误率  Chernoff 界限 :  散度 :  当两类样本都服从正态分布  当两类协方差矩阵相等时,  。这等于两类均值之间的 Mahalanobis 距离。\n概率相关性判据 : 可以用  与  之间的函数距离作为特征对分类贡献的判据  只需把  换成  换成  即可。\n1.3 基于熵的可分性判据：\nShannon 熵  平方熵  对特征的所有取值积分，得到基于熵的可分性判据,  越小，可分性越好。  #### 1.4 统计检验作为可分性判据：\n计算在空假设下有多大的概率会得到所观察统计量的取值, 这个概率小于  值，则拒绝空假设，接受备择假设。  检验 : 假设两类样本服从方差相同的正态分布，在同一特征上的观测。总体样本方差：  统计量：  服从自由度为  的  分布。双边 -检验空假设 : , 备择假设是  。单边  检验空假设 : , 备 择假设是  。  检验属于参数化检验方法,对数据分布有一定假设,必要时需要检验样本分布是否符合该假设。\n秩和检验: 把两类样本混合在一起,对所有样本按照所考査的特征从小到大排序。在两类样本中分别计算所得排序序号之和  和 , 称作秩和, 考查某一类的秩和是否显著小于或大于另一类的秩和。两类的样本数分别为  和  。 不同样本数目情况下  的空分布是不同的。对于小的样本数,人们预先计算出了  的分布。当  和  较大时(比如都大于10), 可用正态分布  来近似秩和  的空分布, 其中  过滤方法： 指依据一定的统计量来过滤出与所研究的分类问题密切相关的特征,再采用一定的分类方法进行分类。这种方法实现起来比较简单,但是，所采用的过滤准则与后期分类器所采用的准则并不一定有很好的联系。 特征选择的最优算法： 分枝定界法：\n2. 特征选择的最优算法\n从所有候选特征中逐步去掉不被选中的特征。这种方法具有回溯的过程,能够考虑到所有可能的组合 同一层按照去掉单个特征后的准则函数值来对各个结点排序，如果去掉某个特征后准则函数的损失量最大，则认为这个特征是最不可能被去掉的，把它放在该层的最左侧节点，依次类推。搜索：从最右侧开始向下搜索，当到达叶节点时计算当前达到的准则函数值，记作界限  。算法向上回溯，每回溯一步回收相应节点上舍弃的特征。遇到最近的分枝节点时停止回溯，从这个分枝节点向下搜索左侧最近的一个分枝。当搜索到某一个节点时，准则函数值已经小于界限  ，则说明最优解已不可能在本节点之下的叶节点上，因此停止沿本树枝的搜索，从此节点重新向上回溯。如果搜索到一个新的叶节点， 则更新界限  值，向上回溯。如果回溯过程一直到了根节点，而且根据界限  不能再向下搜索其他树枝，则算法停止，最后一次更新  时取得的特征组合就是特征选择的结果。\n3. 特征选择的次优算法\n1，单独最优特征的组合 假设单独作用时性能最优的特征，它们组合起来也是性能最优的。但即使是特征间统计独立时，单独最优特征的组合也不一定是最优的，这还与所采用的特征选择的准则函数有关, 只有当所采用的判据是每个特征上的判据之和或之积时, 这种做法选择出的才是最优的特征。 2，顺序前进法 3，顺序后退法 4，增  减  法 与广义顺序前进法和广义顺序后退法类似。可以每次选择或剔除多个特征，这种做法称作  法。这样与  法相比能够既考虑到特征间的相关性又保持适当的计算量。\n特征选择的遗传算法 : 把候选对象编码为一条染色体  。把所有特征表述为一条由  个  字符组成的字符串，求一条仅有  个 1 的染 色体,这样的染色体共有  种。优化的目标为适应度函数, 每一条染色体对应一个适应度值  。可用类别可分性判 据作为适应度。针对不同的适应度有不同的选择概率  。\n4. 遗传算法：\n\n初始化, , 随机地产生一个包含  条不同染色体的种群 ;\n计算当前种群  中每一条染色体的适应度 ;\n按选择概率  对种群中的染色体采样,由采样出的染色体繁殖出下一代染色体,组成种群 ;\n回到 (2), 直到达到终止条件（常是某染色体的适应度达到设定國值）,输出适应度最大的染色体作为找到的最优解。 在第(3)步产生后代的过程中, 有两个最基本的操作 : 一个是重组也称交叉, 两条染色体配对, 并在某个随机的位 置上以一定的重组概率  进行交叉, 互换部分染色体。另一个是突变, 每条染色体的每一个位置都有一定的概率  发生突变 (从 0 变成 1 或从 1 变成 0 ) 。算法有很多可调节参数 : 种群大小  、选择概率、重组概率、突变概率等。\n\n5. 以分类性能为准则的特征选择方法：\n包裹法：把分类器与特征选择集成在一起利用分类器进行特征选择的方法。\n过滤法：利用单独的可分性准则来选择特征再进行分类的方法。\n包裹法对分类器的基本要求：一是分类器能处理高维的特征向量；二是分类器能在特征维数很高但样本数有限时仍能得到较好的效果。\n​ 支持向量机能较好地满足这两个要求。递归支持向量机和支持向量机递归特征剔除的核心是线性的支持向量机，特征选择与分类采用的是同样的算法步骤。两种算法的不同在于它们评估特征在分类器中贡献的方法不同。\n两种算法的基本步骤都是：\n（1）用当前所有候选特征训练线性支持向量机；\n（2）评估当前所有特征在支持向量机中的相对贡献，按照相对贡献大小排序；\n（3）根据事先确定的递归选择特征的数目选择出的排序在前面的特征（SVM-RFE中描述为剔除排序在后面的特征），用这组特征构成新的候选特征，转（1），直到达到所规定的特征选择数目。\n支持向量机的输出函数：  R-SVM 定义两类在当前特征上的分离程度为  写成各个特征之和的形式：   是当前候选特征的维数, 、分别是两类样本在第  维特征上的均值。每个特征的贡献是：  SVM-RFE 采用灵敏度的方法来推导各个特征在 SVM 分类器中的贡献。它把 SVM 输出与正确类别标号  之间平均平方误差作为 SVM 分类的损失函数：  考查各个权值对这个损失函数的影响，各个特征的贡献应该用下式衡量。  ​ 在很多实际应用中,R-SVM 与 SVM-RFE 从分类上看性能基本相同,但 R-SVM 在选择特征的稳定性和在对末来样本的推 广能力方面有一定优势,尤其是当训练样本中存在较大的噪声和野值时优势更明显。 包裏法递归进行特征选择与分类的做法可推广到 SVM 采用非线性核的情况。 SVM 对偶问题的目标函数：  用  表示去掉第  维特征后的样本, 去掉第  个特征对这一目标函数的影响是  可以用这个量作为特征在非线性 SVM 分类器中的贡献，并利用递归方法来进行包裹法特征选择。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第六章：其他分类方法","url":"/2022/04/28/Pattern%20Recognition/%E5%85%AD%EF%BC%8C%E5%85%B6%E4%BB%96%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/%E5%85%B6%E4%BB%96%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/","content":"\nThe project will be configured before the first of June\n\n\n第六章：其他分类方法\n最近邻决策：\n 设  个样本下最近邻法的平均错误率为 , 样本  的最近邻为 , 平均错误率可以写成   为贝叶斯错误率, 存在关系  最近邻法的渐近错误率最坏不会超出两倍的贝叶斯错误率, 而最好则可能接近或达到贝叶斯错误率。\nk-近邻法 ：\n设  个已知样本分属于  个类 , 新样本  的前  个近邻中有  个属于  类,则  类的判别函数:  决策规则 则 ​ 当  趋于无穷大时， -近邻法渐近错误达到贝叶斯错误率。近邻法只是确定一种决策原则，并不需要利用已知数据事先训练出一个判别函数，但需要始终存储所有的已知样本,，并将每一个新样本与所有已知样本进行比较和排序，其计算和存储成本都很大。\n近邻法的快速算法 :\n : 样本集 ;  : 节点  对应的样本子集 ;  中的样本数；  : 样本子集  中的样本均值;  : 从  到  的最大距离。 第一阶段: 将  分为  个子集。每个子集再分成  个子集得到树结构。每个节点  上对应一群样本。对每个节点计算  和 ，作为节点的属性。（ 在求  时已求得） 第二阶段 : 搜索。 步骤 1 置  是当前水平,  是当前节点。 步骤 2 将当前节点所有直接后继节点放入当前水平一个目录表中, 并对节点计算  。 步骤 3 如果有 , 则从目录表中去掉 。 步骤 4 如果目录表没有节点，则后退到前一个水平  则停止）转步骤 3。否则转步骤 5 。 步骤 5 在目录表中选择使  最小化的最近节点 ，并称该  为当前执行节点, 并从目录表中去掉  。如果当前的水平  是最终水平, 则转步骤 6 。否则置  转步骤 2 。 步骤 6 对当前执行节点  中的每个 ，如果  则  不是  的最近邻，否则计算  。若  置  和  。当前执行节点中所有  被检验之后,转步骤 3 当算法结束时, 输出  的最近邻  和  与  的距离  。\n\n\n\n剪辑近邻法：\n​ 落在最优分类面错误一侧的训练样本会误导决策。重叠区域内两类已知样本都存在, 可能会使分类面的形状变得 非常复杂。去掉重叠区域可使近邻法的决策面更接近最优分类面。将已知样本集划分为考试集  和训练集  两部分，用训练集  中的样本对考试集  中的样本进行近邻法 分类, 从  中除去被错误分类的样本,剩余样本构成剪辑样本集 ,用  对末来样本进行近邻法分类。 如果在剪辑阶段和分类阶段都用最近邻法，则剪辑近邻法得到的渐近错误率与近邻法错误率的关系是：  其中,  是近邻法的错误率，剪辑后错误率减小。如果近邻法的错误率不大，则有  近邻法的渐近错误率上界是两倍的贝叶斯错误率, , 可知剪辑近邻法的渐近错误率近似等于贝叶斯错误率。如果在剪辑阶段用  近邻法, 分类阶段用最近邻法,则当 、 但  时,剪辑近邻法的渐近错误率收敛于贝叶斯错误率。同样的方法应用到多类问题上，剪辑近邻法对性能的改善比两类情况下更显著。\nMULTIEDIT：当样本数较多时、为了消除考试集、训练集划分中的偶然性造成的影响。 (1) 划分 把样本集随机划分为  个子集,  。 (2) 分类 用  对  中的样本分类, , 比如, 如果  ，则用  对  分类,用  对  分类，用  对  分类。 (3) 剪辑 从各个子集中去掉在 (2) 中被分错的样本。 (4) 混合 把剩下的样本合在一起, 形成新的样本集  (5) 迭代 用新的样本集  替代原样本集,转(1)。如果在最近的  次迭代中都没有样本被剪掉,则终止迭代,用最后的  作为剪辑后的样本集。 经过多重剪辑之后的近邻法分类面在数据分布的主要区域内已经非常接近贝叶斯分类面。\n压缩近邻法 : 将样本集  分为  和  两个活动子集。算法开始时，  中只有一个样本，其余样本均在  中。考查  中的每 一个样本 , 若用  中的样本能够对它正确分类, 则该样本保留在 , 否则移到  中, 依次类推, 直到没有样本需要 搬移为止。最后用  中的样本作为代表样本, 对末来样本进行近邻法分类。\n决策树与随机森林\nID3 方法\n信息熵 :   为每种可能的结果对应的概率。对某个节点上的样本,把这个度量称为熵不纯度,如果特征把  个样本划分成  组, 则不纯度减少量为  其中,  。\nGini 不纯度度量，也称方差不纯度  误差不纯度   是当前节点上的  个样本中属于第  类的样本数占总样本数的比例。多数情况下,采用不同的不纯度度量对 分类结果的影响不大。\nC4.5 算法\n采用信息增益率代替信息增益 \nCART算法\n​ 分类和回归树算法。每一个节点上都采用二分法, 最后构成二叉树。CART既可以用于分类问题，也可以用于构造回归树对连续变量进行回归。\n过学习与决策树的剪枝\n对以把有限的样本全部正确划分为准则建立的决策规则，控制决策树生成算法的终止条件和对决策树进行剪枝是防止出现overfitting的主要手段。\n先剪枝：在决策树生长过程中决定某节点是否需要继续分枝还是直接作为叶节点。\n（1）数据划分法。将数据分成训练样本和测试样本，首先基于训练样本对决策树进行生长，直到在测试样本上的分类错误率达到最小时停止生长。通常采用多次的交叉验证方法以充分利用数据信息。\n（2）阈值法。预先设定一个信息增益阈值，当从某节点往下生长时得到的信息增益小于设定阈值时停止树的生长。但是此阈值往往不容易设定。\n（3）信息增益的统计显著性分析。统计已有节点获得的信息增益其分布，如果继续生长得到的信息增益与该分布相比不显著，则停止树的生长，通常可以用卡方检验来考查这个显著性。\n后剪枝：消除有相同父节点的叶节点后不会导致不纯度的明显增加则以其父节点作为新的叶节点。\n（1）减少分类错误修剪法。通过独立的剪枝集估计剪枝前后分类错误率的改变。\n（2）最小代价与复杂性的折中。对合并分枝后错误率增加与复杂性减少进行折中考虑。\n（3）最小描述长度准则。最简单的树就是最好的树。对决策树进行编码再剪枝得到编码最短的决策树\nBootstrap\n随机森林， Bagging、adaboost、随机划分选择法等。\n随机森林：\n建立很多决策树，组成一个决策树的“森林”，通过多棵树投票来进行决策。： 1，随机森林方法对样本数据进行自举重采样，得到多个样本集。 2，用每个重采样样本集作为训练样本构造一个决策树。在构造过程中每次从所有候选特征中随机地抽取m个特征，作为当前节点下决策的备选特征，从这些特征中选择最好地划分训练样本的特征。 3，得到所需数目的决策树后，以这些树的输出为投票，以得票最多的类作为随机森林的决策。 这对训练样本和特征进行了采样，保证了所构建的每棵树之间的独立性，使投票结果更无偏。\nBoosting 方法：Boosting通过一个迭代过程对分类器的输入和输出进行加权处理，而非简单地对多个分类器的输出进行投票决策。在不同应用中可以采用不同类型的弱分类器,在每一次迭代过程中,根据分类的情况对各个样本进行加权,而不仅仅是简单的重采样。\nAdaBoost 算法：\n给定  个训练样本 , 用  表示  个弱分类器在样本  上的输出\n1，初始化训练样本  的权重  。\n2，对 , 重复以下过程 : (1) 利用  加权后的训练样本构造分类器  。 (2) 计算样本用  加权后的分类错误率 , 并令  。 (3) 令 , 并归一化使  。\n3，对于待分类样本 , 分类器的输出为  。 用加权后的训练样本构造分类器,是指对分类器算法目标函数中各个样本所对应的项进行加权。对于最小平方误差判别, 加权后的最小平方误差(MSE)准则函数为：  ​ 而对于决策树或一些其他方法,则可以根据每个样本的权值调整重采样的概率,用重采样得到的样本集构造新的弱分类器。在很多情况下, 迭代次数（即所采用的弱分类器数）较大时, Boosting 方法不会导致严重的过学习问题。\n罗森斯特回归\n多元线性回归问题：   为残差。特征  可以是连续变量,也可以是离散变量,如性别、基因型等。求解线性回归常用最小二乘法, 即求使各样 本残差的平方和达到最小的系数  。这实际是在变量  服从正态分布假设下的最大似然估计。 Logistic 函数：  记为  。几率(odds):  对数几率 (log odds)   的 logit 函数  样本属于  类的概率是  决策函数：\n若 , 则  \n最大似然法：设共有  个独立的训练样本,把两类的输出分别编码为  和 .   为样本的总体概率密度函数。  个独立样本出现的似然函数为  最大化上式等价于最大化  取对数  在似然函数满足连续、可微的条件下,最大似然估计量就是以下微分方程的解。 可得：  如果  是  维特征, , 则  代入  得到一组关于  的非线性方程。其一般无法解析求解，可以采用迭代的策略求解。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第五非章：线性分类器","url":"/2022/04/28/Pattern%20Recognition/%E4%BA%94%EF%BC%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","content":"\nThe project will be configured before the first of June\n\n\n第五章：非线性分类器\n1. 分段线性判别函数\n用多个线性分类器片段来实现非线性分类。求解两类之间的分段线性判别函数，可把各类划分成适当的子类，在两类的多个子类之间构建线性判别函数，再分段合并成分段线性判别函数。\n1.1 分段线性距离分类器：\n\n最小距离分类器：当两类的类条件概率密度为正态分布，两类先验概率相等，而且各维特征独立且方差相等时，最小错误率贝叶斯决策就是直观的最小距离分类器。在很多情况下，只要每一类数据的分布是单峰的、在各维上的分布基本对称且各类先验概率基本相同，则最小距离分类器都不失为一种简单有效的分类方法。\n\n\n\n\n\n分段线性距离分类器：把属于  类的样本区域  划分为  个子区域 ，每个子类的均值是 ，判别函数定义为:\n\n 决策规则： 、则决策 * 一般的分段线性判别函数：\n 增广形式：  类  的分段线性判别函数定义为：  决策规则是： ，则决策 两个相邻的类的决策面方程：  子类的划分\n第一种情况，根据问题的领域知识和对数据分布的了解,人工确定子类的划分方案： 比如在字符识别中,一种字符作为一个类,而同一个字符又有不同的字体,可以把一种字体作为一个子类。有时也可以把同一种疾病的病人按照性别、年龄、地域或遗传学特征等分成子类。也可以用非监督学习方法对同一类的样本进行聚类分析,得到子类的划分。 第二种情况，已知各类的子类数目，每个类都有一定数量训练样本，但是不知道子类的划分： 1 ，任意给定各类各子类的权值 , 通常可选小的随机数。 2 ，在时刻 ， 考虑某个训练样本 , 找出  类的各子类中判别函数最大的子类，记为 ：  考査当前权值对样本  的分类情㑆 : (1) 若 ，则:   ； (2) 若对某个 , 存在子类  使得 ，则选取  中最大的子类(不妨记作  类的第  个子类)  其余权值不变。 3, , 考查下一个样本, 回到第(2)步。如此迭代，直至收政。 第三种情况是子类数目无法事先确定。 可以采用分类树的思想来分级划分子类和设计分段线性判别函数。 对于图示两类情况，可先用两类线性判别函数算法找一个权向量 , 它所对应的超平面  把整个样本集分成两部分,我们称之为样本子集。由于样本集不是线性可分的,因而每一部分仍然包含两类样本。接着再利用算法找出第二个权向量  、第三个权向量 , 超平面 、 分别把相应的样本子集分成两部分。若某 一部分仍然包含两类样本, 则继续上述过程, 直到某一权向量 (如图中  ) 把两类样本完全分开为止。这样得到的分类器显然也是分段线性的,基决策面如图中粗线所示。“  \"表示权向量  的方向, 它指向超平面  的 正侧。它的识别过程是一个树状结构,如图所示。图中用虚线显示了对末知样本  的决策过程。\n\n\n\n二次判别函数\n​ 一般的正态分布情况下,贝叶斯决策面是二次函数。二次判别函数也是一种比较常用的固定函数类型的分类方法，它的一般形式是：  其中,  是  维实对称矩阵,  为  维向量。 这个判别函数中包含  个参数，如果像线性判别函数那样， 直接根据一定的规则从数据去学习这些参数,计算起来会非常复杂,而且在样本数不多时估计如此多的参数,，结果的可靠性和推广能力很难保证。常假定每一类数据都是正态分布,这时每一类可以定义二次判别函数：   是一个阈值项，它受协方差矩阵和先验概率的影响。判别函数就是样本到均值的 Mahalanobis 距离的平方与固定阈值的比较, 样本的均值和方差可以用估计：  当两类都近似服从正态分布时,可以对每一类估计判别函数,两类间的决策面方程就是 可得:  决策规则是 若则 其中，可以通过调整两类的阈值  和  来调整两类错误率情况。 另一种情况:两类中  一类分布比较成团(近似正态分布), 另一类  则比较均匀地分布在第一类附近，这种情况下只要对第一类求解其二次判别函数即可,即：  决策规则 : 若则 可以用  来调整决策的偏向。即当样本到  类均值的 Mahalanobis 距离的平方小于  时 则决策为  类，否则决策为  类.\n多层感知器神经网络\n阀值逻辑单元  用和表示要区分的两类，并用  代表训练样本  的正确分类，则权值迭代训练与感知器算法是等价的：  对任意复杂形状的分类区域，可用多个神经元组成一定的层次结构来实现非线性分类面。  采用反向传播算法的多层感知器，用 Sigmoid 函数替代阶跃函数作为神经元的传递函数：  为了方便分析，把常数项  作为一个固定输人 1 的权值合并到加权求和项中  在单位超立方体  内的任意连续函数 , 都可以通过选择适当的  和  表示成  多层感知器神经网络能够实现任意复杂的函数映射。即任意一个从  到  的非线性映射,都存在一个适当结构的 3 层前馈神经网络能够以任意的精度来逼近它。\nBP 算法：\n在给定多层感知器结构的情况下训练其权值的反向传播算法 输入向量 ，输入层记 ，第一个隐层记 ，以此类推。第  层第  个神经元的输出记作 ，输出向量 , 第  个隐层的神经元个数为  第  层的权值都用  表示,  表示第  层的节点  连接到第  层的节点  的权值。用  表示在第  步迭代时权值  的取值。 (1) 确定神经网络的结构, 用小随机数进行权值初始化,置  。 (2) 从训练集中得到一个训练样本 , 记它期望的输出是  。 (3) 计算在  输入下当前神经网络的实际输出  (4)  层权值修正  权值修正项  平方误差  对输出层 , 计算梯度项   \n对中间层，计算梯度项  $$\n\n {j}^{l}=-  =-=x{j}{l}(1-x_{j}{l}) {k=1}^{n{l+1}} {k}^{l+1} w{j k}^{l+1}(t), j=1, , n_{l} $$\n\n更新全部权值后对所有训练样本重新计算输出，计算更新后的网络输出与期望输出的误差。检查算法终止条件,如果 条件已达到则停止，否则 , 返回 (2)。此外，这里给出的 BP 算法是针对神经元节点为 Sigmoid 函数的。这时  的梯度函数是：  如果采用其他形式的 Sigmoid 函数或其他函数, 需要根据其梯度函数修正算法第 (4) 步。 收敛结果有时受初始权值的影响很大，算法不能保证收敛到全局最优点。对于多层感知器神经网络，各个初始权值不能为 0, 也不能都相同，而是应该采用较小的随机数。如果算法很难收敛，可以尝试改变初值重新试算。如果步长太大，收敛速度可能一开始会较快,但可能会容易导致算法出现振荡而不能收剑或收敛很慢；如果步长太小，则权值调整可能会非常慢，导致算法收敛太慢，而且一旦陷于局部极小点就容易停在那里。试算过程中观察不同步长下得到的误差收敛曲线有助于找到针对特定问题的较合理的步长。为了兼顾训练过程和训练的精度，有时采用变步长的办法。 为使算法有更好的收敛性能，可在权值更新过程中引人 “记忆项\"或 “惯性项”,把权值更新项改为：  #### 支持向量机：\n\n新空间中的内积也是原特征的函数称作核函数：  对特征  进行非线性变换 , 新特征空间里构造的支持向量机决策函数是：  其中,系数  是下列优化问题的解 $$\n\n通过支持向量求得 y_{i}({i=1}^{n} a{i} K(x_{i} x)+b)-1=0 条件对称函数它是某个特征空间中的内积运算的充要条件是对于任意的且有 K(x, x^{}) (x) (x^{})  x  x^{}&gt;0 选择一个满足条件的核函数，就可以构建非线性的支持向量机，且不用设计变换。条件还可放松为满足如下正定核：是定义在空间上的对称函数且对任意训练数据和任意实系数都有 {i, j} a{i} a_{j} K(x_{i}, x_{j})  $$ 对于满足正定条件的核函数，肯定存在一个从  空间到内积空间  的变换 ，使得  。\n常用核函数形式 一, 多项式核函数  采用这种核函数的支持向量机实现的是  阶的多项式判别函数。 二, 径向基(RBF)核函数  采用它的支持向量机实现与径向基网络形式相同的决策函数。 三, Sigmoid 函数  采用这种核函数的支持向量机在  和  满足一定条件的情况下等价于包含一个隐层的多层感知器神经网络。 采用径向基核函数时，支持向量机能够实现一个径向基函数神经网络的功能。径向基函数神经网络通常需要靠启发式的经验来选择径向基的个数、每个径向基的中心位置、径向基函数的宽度等，只有权系数是通过学习算法得到的，而在支持向量机中，每一个支持向量构成一个径向基函数的中心, 其位置、宽度、个数以及连接权值都是可以通过训练过程确定的。\n核函数及其参数的选择 一般先尝试简单的选择，比如线性核，当结果不满意时才考虑非线性核；如果选择 RBF 核函数，则先选用宽度比较大的核，宽度越大越接近线性,，然后再尝试减小宽度，增加非线性程度。 3.核函数与相似性度量 通过非线性变换将输入空间变换到一个高维空间, 然后在这个新空间中求最优分类面，这种非线性变换是通过定义适当的内积核函数实现的。支持向量机求得的分类函数，形式上类似于一个神经网络，其输出是若干中间层节点的线性组合，而每一个中间层节点对应于输人样本与一个支持向量的内积, 因此早期也被叫做支持向量网络。 决策过程可看作一种相似性比较的过程。输入样本与一系列模板样本（支持向量）进行相似性比较, 采用的相似性度量是核函数。样本与各支持向量比较后的得分进行加权后求和，权值就是训练时得到的各支持向量的系数  与类别标号的乘积。最后根据和值大小进行决策。采用不同的核函数, 可以看作是选择不同的相似性度量, 线性支持向量机就 是采用欧式空间中的内积作为相似性度量。根据这一思想, 除了可以选择常用的核函数形式外, 还可以根据相关领域的专门知识定义一些特殊的核函数。 4.维数与推广能力 支持向量机通过采用核函数作为内积, 间接地实现了对特征的非线性变换, 避开了在高维空间进行计算。支持向 量机通过最大化分类间隔来控制函数集的 VC 维，使得在高维空间里的函数集的 VC 维可以大大低于空间的维数, 从而保证好的推广能力。支持向量机需要求解的是关于  的二次优化函数。这是一个有线性约束的二次优化问题, 有唯一的最优解, 这与多 层感知器神经网络相比是一个优势。而且, 问题的计算复杂度是由样本数目决定的, 计算复杂度不取决于样本的特征维 数和所采用的核函数形式。\n多类支持向量机： 支持向量机用正则化(regularization) 的框架重新表述如下: 设有训练样本集  是样本的特征,  是样本的类别标号。待求函数  是由核函数  定义的可再生希尔伯特空间。决策规则是  。 支持向量机求解的是这样的 , 它最小化以下的目标函数  如果样本的类别标号  和待求的函数  都从标量变为向量,则上述表述就可以用于多类分类问题。 对于  类问题,  是一个  维向量,如果样本  属于第  类,则  的第  个分量为 1 , 其余分里为 , 这样,  的各分量值总和为 0 。如 , 则 若若若 待求函数为 , 它的各分量之和须为 0 , 即 , 且每一个分量都定义在核函 数可再生希尔伯特空间中  把多个类别编码成这样的向量标签后，多类支持向量机就是求 , 使下列目标函数达到 最小  其中,  是损失矩阵  与样本类别  相对的行向量。损失矩阵  是一个  维的矩阵,例如：  得到函数  后,类别决策规则是 , 即决策为  各分量中取值最大的分量对应的类 别。\n用于回归的支持向量机：  目标函数:  对偶问题：  回归函数权值  回归函数  核函数变换：  系数  是以下优化问题的解 \n核 Fisher 判别：\n对样本  进行非线性变换   这里的  是  空间里的权值向量,  和  分别是  空间里的类间离散度矩阵和类内离散度矩阵   是  空间里各类样本的均值  根据表示定理，上述问题的任何解  都处在  空间中所有训练样本张成的子空间中,即  因此  变换空间里的目标函数  其中：  最大化目标函数的解是  的最大本征值对应的本征向量，且最优解的方向是  原空间任意一个样本到 Fisher 判别的方向上的投影, 即只需要计算  通常，上述问题可能是病态的,因为矩阵  可能非正定,这是由于变换后样本维数升高导致的。一种补偿办法是,引入 一个新的矩阵  来代替原来的矩阵 (  是一个常数),使矩阵正定。这样做同时还实现了对  的正则化控 制,类似于支持向量机中控制间隔的作用。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第四章：线性分类器","url":"/2022/04/28/Pattern%20Recognition/%E5%9B%9B%EF%BC%8C%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","content":"\nThe project will be configured before the first of June\n\n\n第四章：线性分类器\n省去了对概率密度函数的估计，直接基于样本直接进行分类器设计：如果知道判别函数的形式，设法从数据直接估计这种判别函数中的参数。即使不知道最优的判别函数是什么形式，仍然可以根据需要或对问题的理解设定判别函数类型，从数据直接求解判别函数。基于样本直接设计分类器需要确定三个基本要素：\n\n分类器的类型，也就是从什么样的判别函数类 (函数集) 中去求解；\n分类器设计的目标或准则，根据样本从函数集中选择在该准则下最优的函数，通常就是确定函数类中的某些待定参数；\n设计算法利用样本数据捜索到最优的函数参数 (即选择函数集中的函数)。\n\n1. 线性判别函数的基本概念\n一般表达式：   是个常数，称为阈值权。对于两类问题的线性分类器可以采用下述决策规则： 则则快策则快策可将任意分到某一类或拒绝\n\n\n\n 定义了一个决策面，  为线性函数时，这个决策面便是超平面。假设  和  都在决策面  上，则  即  判别函数  可以看成是特征空间中某点  到超平面的距离的一种代数度量   是  在  上的射影向量;  若  为原点，则  得到从原点到超平面  的距离  超平面的正方向由权向量  指向确定，它的位置由阈值权  确定。判别函数  正比于  点到超平面的距离。当  在  正侧时 ，在负侧时  。\n2. Fisher线性判别分析（LDA）\n讨论两类线性判别问题：可看作把所有样本都投影到一个方向上，然后在一维空间中确定一个分类的阀值，并使投影后两类相隔尽可能远，而同时每一类内部的样本又尽可能聚集。\n\n\n\n寻找一个投影方向  ，投影后的样本变成  投影后的一维空间，类均值向量： 𝕪 类间离散度：  类内离散度： 𝕪 Fisher 准则函数：  用原样本空间类间离散度矩阵和类内离散度矩阵表示：  这一表达式被称作广义 Rayleigh 商。对  幅值的调节并不会影响  的方向，因此可把优化问题转化为：  拉格朗日函数  极值处  可得极值解  满足  假定  是非奇异的(样本数大于维数时通常是非奇异的)，可得 是矩阵  的本征向量。  代入 \n可知  的方向由  决定，因此有最优投影方向  FDA投影方向也直接求微分求得：  即  由于  是标量，非奇异的条件下解满足  至此，要得到分类面，需要在投影后方向上确定一个分类阀值并采取决策规则 则 当样本是正态分布且两类协方差矩阵相同时最优贝叶斯分类器是线性函数   如果用  估计 , 用  估计 ，则 FDA 所得的方向实际就是最优贝叶斯决策的方向，可以用  来作为分类阀值。  决策规则为： ，则 如果先验概率相同，则可以采用阈值:  ​ 在样本不是正态分布时，这种投影方向和阈值并不能保证是最优的，但通常仍可以取得较好的分类结果。在先验概率不同时，分界点向先验概率小的一侧偏移。FDA本身并不对样本的分布作任何假设。但在很多情况下，当样本维数比较高且样本数也比较多时，投影到一维空间后样本接近正态分布。这时可以在一维空间中用样本拟合正态分布，用得到的参数来确定分类阈值。\n\n3. 感知器(perceptron)\nFDA把分类器设计分为两部分，先确定方向再确定阀值。感知器可直接得到完整的线性判别函数  。为了方便，定义增广的样本向量  增广的权向量  则线性判别函数变为 \n决策规则 则 定义规范化增广样本向量  若若 决策规则变为  对于线性可分的一组样本  ，如果一个权向量  满足  则称  为一个解向量。在权值空间中所有解向量组成的区域称作解区。\n\nimage-20220424092115429\n\n考虑噪声，引入余量  ：  定义惩罚项。  解向量  梯度下降方法迭代求解  即  即在每一步迭代把错分样本按照某个系数加到权向量上。通常每次只修正一个样本的固定增量法效率更高。即随机梯度下降： (1) 任意选择初始的权向量 ，置 ； (2) 考查样本  ，若 , 则 ，否则继续； (3) 考査另一个样本，重复 (2)，直至对所有样本都有 , 即  。\n如果考虑余量 ，只需将错分判断条件变成  即可。对于线性可分的样本集，采用这种梯度下降的迭代算法，经过有限次修正后一定会收敛到一个解向量  。图示这种单步的固定增量法采用的修正步长  。为了减少迭代步数可以使用可变的步长，比如绝对修正法采用步长:  样本不可分但多数可分时，让步长按照一定的启发式规则逐渐缩小，可以强制算法收敛。\n4. 最小平方误差判别\n求解一个  使不满足不等式的样本尽可能少，为简化计算引入待定常数  矩阵形式  其中   是增广的样本向量的维数,  。通常情况下, ，方程属于矛盾方程组，无法求得精确解，方程组的误差为 。可求方程组的最小平方误差解，即 :  最小平方误差( MSE )准则函数：  * 伪逆法求解：  在极值处对  的梯度为零\n 其中  是长方矩阵  的伪逆。可得  * 梯度下降法求解： (1) 任意选择初始的权向量 , 置 ; (2) 按照梯度下降的方向迭代更新权向量\n 直到满足  或者  时为止，其中  是事先确定的误差灵敏度。参照感知器算法中的单步修正法，对最小平方误差准则，也可以采用单样本修正法来调整权向量  其中,  是使得  的样本。这种算法称作 Widrow-Hoff 算法，也称LMS 算法 选择不同的  会带来不同的结果。\n\n当同一类样本的  选择为相同的值，解等价于 Fisher 线性判别的解,把样本和权向量都还原成增广以前的形式后有:\n\n 其中, 、 是两类各自的均值向每,  是总类内离散度矩阵。\n\n当  的选择为第一类样本对应的  都是 , 第二类样本对应的  都是  时, 阀值  为样本均值在所得一维判别函数方向的投影, 即\n\n 其中 、 分别是第一类和第二类的样本数,  是样本总数,  是全部样本的均值, 即  * 当对所有样本都取 , 那么当  时, MSE 算法的解是贝叶斯判别函数\n 的最小平方误差逼近。即,下面定义的均方逼近误差  在  时取得最小值,其中  表示由  个 1 组成的列向量。\n5. 最优分类超平面与线性支持向量机\n\n\n\n最优超平面能将训练样本没有错误地分开，并且分类间隔最大。最优超平面定义的分类决策函数：  确定  和  的尺度  即  有待求解问题  用拉格朗日法求解，对每个样本引人一个拉格朗日系数。优化问题转化为  最优解在  的鞍点上取得。由拉格朗日对偶性，解上式等价于式对  和  求最小，再对  求最大。  对求偏导得最优解处  且  代入拉格朗日泛函可得最优超平面的对偶问题  这是一个对  的二次优化问题，通过对偶问题的解 , 可以求出原问题的解  即，最优超平面的权值向量等于训练样本以一定的系数加权后进行线性组合。根据 KKT 条件，拉格朗日泛函的鞍点处满足：  实际只有  的样本参与加权求和，这些样本被称作支持向量，对于这些支持向量来说有  理论上  可以根据任何一个支持向量求得。在实际的数值计算中，人们通常采用所有  非零的样本用式求解  后再取平均。对比感知器算法，也可以把最优超平面等价地看作是在限制权值尺度的条件下求余量的最大化。\n大间隔与推广能力：\n在某个  下对所有训练样本的分类决策损失称作经验风险，线性可分情况下，通过感知器算法，已经能使经验风险达到零。  在权值  下末来所有可能出现的样本的错误率或风险称作期望风险。  表示所有可能出现的样本及其类别的联合概率模型。  统计学习理论指出, 有限样本下, 经验风险与期望风险是有差别的, 期望风险可能大经验风险,但它们之间满足   称作置信范围，它与样本数  成反比，与参数  成正比。参数  是依赖于模式识别算法的设计的，称作 VC 维，它反映了所设计的学习机器(函数集)的复杂性，上式给出了有限样本下期望风险的上界。在训练误差相同的情况下,学习机器的复杂度越低(VC 维越低),则期望风险与经验风险的差别就越小，因而学习机器的推广能力就越好。在线性可分的问题中，我们能得到很多使  为 0 的解,要使方法 有最好的推广能力，就应使  最小。由于训练样本集是给定的，即  固定，能够调整的是算法的  维。 ​统计学习理论指出，对于规范化的分类超平面， 时, VC 维有上界：  其中，  是样本特征空间中能包含所有训练样本的最小超球体的半径，  是样本特征的维数。对于给定的样本集，这两项均是确定的。再求最大间隔分类超平面时，最大化分类间隔也就等价于最小化  ，实际上是使 VC 维上界最小。这样就是试图使期望风险的置信范围尽可能小，即在经验风险都最小化为 0 的情况下追求期望风险的上界的最小化。因此,支持向量机中最大分类间隔的准则，是为了通过控制算法的 VC 维实现最好的推广能力。在这个意义下,所得的分类超平面是最优的。\n线性不可分情况 线性不可分的情况下最优超平面不存在。 引入松弛变量 ，不等式约束条件变为  在该线性可分情况下，对目标函数增加对错误的惩罚项，定义广义最优分类面的目标函数：  且  同理可将问题转化为以下拉格朗日泛函的鞍点问题，其中,  是拉格朗日乘子：  可得对偶问题：  其中：  得解向量 :  广义最优分类面的判别函数：  根据KKT条件, 鞍点满足以下两套条件：  多数  为 0 , 只有 的样本才会使  。这些样本分为两种,一种是分类正确但处在分类边界面上的样本, 它们  ; 另外一种则是分类错误的样本, 它们  。通过  的样本可求得  。这两部分  的样本都是支持向量，有时也把   的支持向量叫做边界问量。由于广义最优分类面可以兼容线性可分情况下的最优分类面,所以人们通常采用的支持向量机都是考虑广义最优分类面的形式。 目标函数式中只有  的二次项和一次项，这是一个对  在等式和不等式约束下的二次优化问题，具有唯一的极值点。\n6. 多线性分类器\n多个两类分类器的组合 : 1，“一对多\" ，  个两类分类器就可以实现  个类的分类。会有训练样本不均衡的问题。一些区域内的分类也可能会出现歧义。 2，“逐对\"分类， 个类别, 共需要  个两类分类器。不会出现两类样本数过于不均衡的问题, 决策歧义的区域通常要比“一对多\"分类器小。 3， 如果对所研究的类别有较好的认识, 能够根据类别间的内在关系把它们分级合并成多个两类分类问题, 则可以用二叉树来构建多个两类分类器。\n很多分类器在最后的分类决策前得到的是一个连续的量,分类是对这个量用某个阀值划分的结果, 比如所有线性分类器都是最后转化为一个线性判别函数  与某一阈值(通常是 0) 比较的问 题。SVM 也是这样一种分类器。在很多线性分类器中,一个正确分类的样本,如果它离分类面越远,则往往对它的类别判断就更确定,因此可以把分类器的输出值看作是对样本属于某一类别的一种打分。利用这种分类器, 可以用  个一对多的两类分类器来构造多类分类系统， 即每个类别对应一个分类器，其输出是对样本是否属于  类给出一个判断。在多类决策时，如果只有一个两类分类器给出了大于阈值的输出, 而其余分类器输出均小于阈值,则把这个样本分到该类。更进一步,如果各个分类器的输出是可比的,那么可以在决策时直接比 较各个分类器的输出, 把样本赋予输出值最大的分类器所对应的类别。(但是需要注意, 对很多分类器来说,如果它们是分别训练的，其输出值之间并不一定能保证可比性)\n7. 多类线性判别函数：\n对  类设计  个判别函数  决策规则： 若则 增广形式：  其中： 为增广权向量。 多类线性机器不会出现有决策歧义的区域。考虑多类线性可分情况，可用与感知器算法类似的单样本修正法来求解线性机器。 (1) 任意选捀初始的权向量 , 置  。 (2) 考查某个样本 ，若 , 则所有权向量不变；若存在某个类, 使  , 则选择  最大的类别 , 对各类的权值进行如下的修正   是步长,必要时可以随着  而改变。 (3) 如果所有样本都分类正确, 则停止; 否则考査另一个样本, 重复(2)。 如果样本集线性可分,则该算法可以在有限步内收敛于一组解向量。与感知器算法一样, 当样本不是线性可分时, 这种逐步修正法不能收敛，人们可以对算法作适当的调整而使算法能够停止在一个可以接受的解上，比如通过逐渐减小步长而强制使算法收敛。 同样, 也可以像在感知器算法中那样引人余量, 即把  变为 \n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第三章：概率密度函数的估计","url":"/2022/04/28/Pattern%20Recognition/%E4%B8%89%EF%BC%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1/","content":"\nThe project will be configured before the first of June\n\n\n概率密度函数的估计\n1. 引言\n\n基于样本的两步贝叶斯决策：先通过训练样本估计概率密度函数，再通过统计决策进行类别判定。\n参数估计：最大似然估计；贝叶斯估计。\n非参数估计：直方图法，近邻法，Parzen窗法。\n统计量：针对不同要求构造出样本的某种函数，这种函数在统计学中称为统计量。\n参数空间：总体分布未知参数的全部可容许值组成的集合。\n点估计，估计量，估计值： 点估计构造一个统计量  作为参数  的估计 ，  称为  的估计量。如果  是属于类别  的几个样本观察值，代人统计量  就得到对于第  类的  的具体数值，这个数值称为  的估计值。\n区间估计：要求用区间  作为  可能取值范围的一种估计，这个区间称为置信区间。\n无偏性、有效性：对于多次估计来说，估计量能以较小的方差平均地表示其真实值，并不能保证具体的一次估计的性能。\n一致性：保证当样本数无穷多时，每一次的估计量都将在概率意义上任意地接近其真实值。\n\n2. 最大似然估计\n假设：样本独立同分布采样得到且概率分布函数形式已知。\n参数  相对于样本集  的似然函数：  最大似然估计量  对数似然函数  当  有梯度算子  对梯度求导并令其等于零  即  得到  个方程,方程组的解就是对数似然函数的极值点。有时上述方程组会有多个解，其中使得似然函数最大的那个解才是最大似然估计量。此外，并不是所有的概率密度形式都可以用上面的方法求得最大似然估计。\n正态分布下的最大似然估计\n单变量正态分布:\n 要估计的参数 ，用于估计的样本  。  分别对两个末知参数求偏导：  最大似然估计是以下方程组的解：  解得：  对于多元正态分布,均值和方差的最大似然估计是：  最大似然估计量是平方误差一致估计量,不一定是无偏估计量。上例中  是无偏的,而  的无偏估计为： \n3. 贝叶斯估计与贝叶斯学习\n3.1 贝叶斯估计：\n把待估计的参数本身也看作随机变量，除了观测数据外, 还可以考虑参数的先验分布。 设样本的取值空间是  ，参数的取值空间是 ,那么，当用  来作为估计时总期望风险就是  在有限样本集合  的情况下,对所有的样本求条件风险最小,即  定义损失函数常用平方误差:  此时有在样本  条件下  在给定样本集  条件下  最小平方误差损失函数下，贝叶斯估计步骤 (1) 根据对问题的认识或者猜测确定  的先验分布密度  。 (2) 由于样本是独立同分布的, 而且已知样本密度函数的形式 , 可以形式上求出样本集的联合分布为  (3) 利用贝叶斯公式求  的后验概率分布  (4)  的贝叶斯估计量  共轭：  为正态分布时,  也为正态分布。 也可由后验概率分布直接得到样本的概率密度函数：  如果完全没有先验知识, 即认为  为均匀分布,则  完全取决于  。如果先验知识非常强 ，除非  的似然函数为0，否则最后的估计就是，样本不再起作用。\n3.2 贝叶斯学习：\n贝叶斯学习则把贝叶斯估计的原理用于直接从数据对概率密度函数进行迭代估计。\n样本集，贝叶斯估计量：  其中：  当  时, 有  可得递推公式：  先验概率记作 , 表示在没有样本情況下的概率密度估计。根据上式, 随着样本数的增加, 可以得到一系列对概率密度函数参数的估计  称作递推的贝叶斯估计。如果随着样本数的增加, 后验概率序列逐渐尖锐, 逐步趋向于以  的真实值为中心的一个尖峰, 当样本无穷多时收敛于在参数真实值上的脉冲函数, 则这一过程称作贝叶斯学习。此时，估计的样本概率密度函数也逼近真实的密度函数 \n3.3 正态分布的贝叶斯估计：\n一维正态分布模型，假设均值  是待估计的参数, 方差  已知，分布密度为：  假定均值  的先验分布是正态分布，其均值为 , 方差为  ，即  对均值  进行估计：  分子部分：  即：  可见  也是一个正态分布, 可得  其中的参数满足  整理后得  其中,  ; 贝叶斯估计值：  也可由后验分布直接求出样本的密度函数  贝叶斯估计不但使用样本中提供的信息进行估计，还能把待估计参数的先验知识融合进来，并且能够根据数据量大小和先验知识的确定程度来调和两部分信息的相对贡献。\n4. 概率密度估计的非参数方法\n对样本的分布并没有充分的了解，难以给出密度函数形式的情况下，需要非参数估计。即不对概率密度函数的形式作任何假设，而是直接用样本估计出整个函数。这种估计只能是用数值方法取得，无法得到完美的封闭函数形式。从另外的角度来看，概率密度函数的参数估计实际是在指定的一类函数中选择一个函数作为对末知函数的估计，而非参数估计则可以看作是从所有可能的函数中进行的一种选择。\n4.1 直方图法\n\n把 维向量样本  的每个分量在其取值范围内分成  个等间隔的小窗。则会得到  个小舱，每个小舱的体积记作  。\n统计落人每个小舱内的样本数目  。\n把每个小舱内的概率密度看作是常数，并用  作为其估计值,  为样本总数。\n\n\n\n\n基本原理 : 样本集  是从服从密度函数  的总体中独立抽取出来的，求  的估计  。与参数估计时一样，这里不考虑类别，即假设样本都是来自同一个类别，对不同类别只需要分别进行估计即可。考虑在样本所在空间的某个小区域 ，某个随机向量落入这个小区域的概率是：  当小区域中实际落人了  个样本时，  的一个很好的估计是  当  连续、且小区域  的体积  足够小时，可以假定在该小区域范围内  是常数，则  可得, 在小区域  的范围内, 概率密度的估计。  小舱的选择应该与样本总数相适应。理论上,假定样本总数是 , 小舱的体积 为 , 在  附近位置上落人小 舱的样本个数是 , 那么当样本趋于无穷多时  收敛于  的条件是： (1)  (2)  (3) \n在有限数目的样本下, 如果所有小舱的体积相同, 那么就有可能在样本密度大的地方一个小舱里有很多样本, 而在密度小的地方则可能一个小舱里只有很少甚至没有样本，这样就可能导致密度的估计在样本密度不同的地方表现不一致。因此, 要想得到更好的估计，需要采用能够根据样本分布情况调整小舱体积的方法。\n4.2 Kn 近邻法 :\n根据总样本确定一个参数  。在求  处的密度估计  时, 调整小舱体积, 直到小舱恰好落入  个样本  为了取得好的估计效果,需要根据条件来选择  与  的关系,比如可以选取为 ， 为某个常数  近邻法在  的取值范围内以每一点为小舱中心用上式进行估计，而非把  的取值范围划分为若干个区域。\n\n\n\n4.3 Parzen 窗法 :\n固定小舱体积的情况下，像  近邻法用滑动的小舱来估计每个点上的概率密度，而不是像直方图中那样仅在每个小舱内估计平均密度。 假设  是  维特征向量, 并假设每个小舱是一个超立方体, 它在每一维的棱长都为 , 则小舱的体积是：  定义维单位方窗函数： 若其他 共有  个观测样本 , 则落入以  为中心的超立方体内的样本数为：  对于任意一点  的密度估计的表达式：  定义核函数（也称窗函数）：  它反映了一个观测样本  对在  处的概率密度估计的贡献, 与样本  与  的距离有关, 可记作  。概率密度估计就是在每一点上把所有观测样本的贡献进行平均  核函数本身满足密度函数的要求，则估计函数满足密度函数的要求，即函数值非负且积分为 1 。 且 以上定义的立方体核函数满足这一条件。 Parzen 窗估计也可以看作是用核函数对样本在取值空间中进行揷值。多种核函数：\n\n方窗\n\n若其他 其中, 为超立方体的棱长。\n\n高斯窗（正态窗）\n\n 即以样本  为均值、协方差矩阵为  的正态分布函数。一维情况下则为 \n\n超球窗\n\n若其他 其中  是超球体的体积,  是超球体半径。 这些窗函数都有一个表示窗口宽度的参数(平滑参数）, 反映了一个样本对多大范围内的密度估计产生影响。当被估计的密度函数连续时，在核函数及其参数满足一定的条件下，Parzen 窗估计是渐近无偏和平方误差一致的。这些条件主要是: 对称且满足密度函数条件、有界、核函数取值随着距离的减小而迅速减小、对应小舱的体积随着样本数的增加而趋于零，但需慢于  趋于零的速度。以下为两种不同分布情况下用不同参数和高斯窗进行估计的结果，其中，为可调节的参量。\n\n\n\n作为非参数方法的共同问题是对样本数目需求较大，只要样本数目足够大，总可以保证收敛于任何复杂的未知密度，但是计算量和存储量都比较大。当样本数很少时，如果能够对密度函数有先验认识，则参数估计方法能取得更好的估计效果。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第二章：统计决策方法","url":"/2022/04/28/Pattern%20Recognition/%E4%BA%8C%EF%BC%8C%E7%BB%9F%E8%AE%A1%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/%E7%BB%9F%E8%AE%A1%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/","content":"\nThe project will be configured before the first of June\n\n\n统计决策方法\n1. 贝叶斯决策\n在类条件概率密度和先验概率已知或可以估计的情况下，通过贝叶斯公式比较样本属于两类的后验概率，将类别决策为后验概率大的一类，这样做的目的是使总体错误率最小。  两类问题，错误率定义为所有服从同样分布的独立样本上错误概率的期望,正确率 如果决策如果决策\n2. 最小错误率贝叶斯决策\n2.1 两类最小错误率贝叶斯决策：\n则\n后验概率可由贝叶斯公式求得  引入似然比阀值， 则 引入对数似然比 则\n2.2 决策线\n\n\n\n2.3 错误率\n 第一类样本决策为第二类的错误率：  第二类样本决策为第一类的错误率： \n2.4 多类情况下，决策规则\n若则\n错误率： \n3. 最小风险贝叶斯决策\n考虑各种错误造成损失不同时的一种最优决策。\n\n\n\n采取决策的期望损失  最小化期望风险  决策 则 在实际是两类且决策也是两类时，决策为 则 引入似然比 ，则\n4. 两类错误率，Neyman-Pearson决策与ROC曲线\n4.1 混淆矩阵\n\n\n\n灵敏度：\n特异度：\n假阳性/第一类错误率  \n假阴性/第二类错误率  \n4.2 Neyman-Pearson决策\n固定一类错误率，使另一类错误率尽可能小  Lagrange乘子法  有  分别对和决策边界求导得必要条件  考虑到  ，有决策规则 则  常通过数值方法求得，通过到的映射关系，由， 求得 ，并调节  大小得到合适的错误率。 \n4.3 ROC 曲线\n通过设定不同的阀值画出单独用一个特征作为指标划分两类时的ROC曲线\n4.4 AUC\n计算AUC并通过比较不同特征间的AUC来得知哪个特征包含更多的分类信息。此外，从整体上看AUC越接近1.0，方法的性能越好。\n\n\n\n利用有限个测试样例绘图：给定 个正例和 个反例，根据学习器预测结果对样例进行排序，先把分类阈值设为最大，此时真正例率和假正例率均为 0，在坐标  处标记一个点，然后调节分类阈值依次将排序的每个样例划分为正例。设前一个标记点坐标为 ， 划分个样例后新增真正样例，假正样例，则对应标记点的 坐标为 ，并用线段连接相邻点，则  可估算为 \n5. 正态分布下的统计决策\n5.1 多元正态分布\n 期望   边缘分布   协方差矩阵总是对称非负定阵，现仅考虑为正定阵的情况。 \n5,2 多元正态分布的性质\n\n参数 和对分布的决定性\n\n\n\n\n\n等密度点的轨迹为一超椭球面 主轴方向由的本征向量所决定，主轴的长度与的本征值成正比。区域中心由确定，大小由确定。等密度点轨迹是由 到的 Mahalanobis 距离为常数的超椭球面。其大小是样本对于均值向量的离散度度量。\n\n\n\n不相关性等价于独立性 如果多元正态随机向量的协方差阵是对角阵，则的分量是相互独立的正态分布随机变量。\n边缘分布和条件分布的正态性 多元正态分布的边缘分布和条件分布仍然是正态分布。\n线性变换的正态性\n\n 由于为对称阵，则总可以找到非奇异阵使得各随机变量在新的坐标系中是独立的\n\n线性组合的正态性\n\n\n5.3 正态分布概率模型下的最小错误率贝叶斯决策\n多元正态概率型下，判别函数为  决策面方程 \n\n一， 时：每类的协⽅差矩阵都相等,⽽且类内各特征间相互独⽴，具有相等的⽅差\n\n，从几何上看,相当于各类样本落人以  为中心的同样大小的一些超球体内，决策方程  判别函数为线性函数，决策面是由线性方程  所确定的一个超平面(如果决策域  与  相毗邻)  其中： \n ，超平面通过  与  连线中点并与连线正交。当  不相等时，决策面与先验概率相等时的决策面平行，只是向先验概率小的方向偏移，即先验概率大的一类要占据更大的决策空间。\n\n\n\n\n决策面  此时决策规则为最小距离分类器 则 二， 时：\n\n，几何上各类样本集中于以该类均值  点为中心的同样大小和形状的超椭球内。 判别式  其中  决策面仍是一个超平面，如果决策域  和  毗邻，则决策面方程应满足 即  其中 \n，决策规则为计算出  到每类的均值点  的 Mahalanobis 距离平方 ，最后把  归于  最小的类别  此时决策面通过  与  连线的中点 。若先验概率不相等，  则在  与  连线上向先验概率小的均值点偏移。判别式的正向方向为指向的方向，判别式大于零则归类到所指向的的类别。\n\n三， 时：\n判别式  其中 矩阵维列向量 这时判别函数式将  表示为  的二次型。如果决策域  和  毗邻，则决策面方程即  决策面为超二次曲面，随着  的不同而呈现为某种超二次曲面，即超球面、超椭球面、超抛物面、超双曲面或超平面。\n6. 错误率的计算\n最小错误率贝叶斯决策的错误率是  类条件概率密度函数解析表达式较复杂时，计算错误率过于复杂。在处理实际问题时对错误率的计算或估计的方法可概括为： (1) 按理论公式计算； (2) 计算错误率上界 ; (3) 实验估计。\n6.1 正态分布下错误率的计算\n最小错误率贝叶斯决策规则的负对数似然比形式: \n决策面是的二次型，当各协方差阵相等时，决策面就变成的线性函数,其决策规则简化为：  易知服从一维正态分布，对于 ，可计算出决定一维正态分布的参数均值  及方差  :  同样对于  :  错误率计算 其中\n\n\n\n6.2 高维独立随机变量错误率估计\n当维随机向量的分量间相互独立时:  负对数似然比   其中  根据独立性假设和中心极限定理，  较大时无论  密度函数如何，  的密度函数总是趋于正态分布，由此可得  的均值  及方差  ：  由于  和  都是一维随机变量  的函数，在大多数情况下，计算这些参数相对比较容易，即使非正态情况亦是如此，所以可以把  近似看成是服从  的一维正态分布的随机变量，再由近似算出错误率。\n6.3 离散概率模型下的统计决策\n一阶马尔可夫链：第  时刻上的取值仅依赖于第  时刻的取值  转移概率  对一个长度为  的序列，我们观察到这个序列的概率是  DNA 序列每一位置有四种状态,转移概率就是一个  的矩阵,称作状态转移矩阵。\n\n\n\n岛的识别中岛记作 “  \"，非岛一类记作\"\"，马尔可夫转移概率分别记作 ，。计算通常采用对数似然比 \n\n\n\n两类的转移概率密度可由下式估计 和\n阈值选取： 阚值的选取可以根据先验概率，也可以根据最小风险的原则确定，或者根据对两类错误率的特殊要求决定。如果两类的先验概率相同且两类错误的损失相同，则对数似然比决策的阈值就是0。在这里，由于概率模型是用数值方法估计的，很难从理论上计算错误率。\n\n\n\n在实际应用中，常把训练数据代到中，统计所有训练样本的似然比取值的分布。选用不同的阈值来做决策就会导致不同的错误情况，可从直方图上确定满意的阈值，或者通过变动不同阈值画出ROC曲线来决定阈值选择。此外，以上没有考虑起始和终止状态的问题。一般情况下起始和结束的概率就可以用基因组上出现各个核苷酸的概率来代替。如果在某些场景下需要更精确地控制片段的起始和结束，那么用同样的方法可以估计起始和终止的转移概率。\n6.4 隐马尔可夫模型\n\n\n\n对前  个位置，  表示当第  位置对应隐状态为  时能得到的最大概率。  递归形式:  最大似然路径 可通过回溯的方法求得，  并且  这种做法叫Viterbi算法\n教材扩展：\n待补充。。。。。。\n本章概要：先验概率/后验概率/类条件密度，马尔可夫模型\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"第一章：概论","url":"/2022/04/28/Pattern%20Recognition/%E4%B8%80%EF%BC%8C%E6%A6%82%E8%AE%BA/%E6%A6%82%E8%AE%BA/","content":"\nThe project will be configured before the first of June\n\n\n概论:\n1. 模式与模式识别\n通过以往对特定事物的认知来识别目标中的特定事物，例如从心电图中各波的形状判断病人的健康情况。\n2. 模式识别的主要方法\n\n基于知识的方法：根据样本特征与类别间关系的认知建立推理系统，对未知样本类别决策。\n基于数据的方法：依据训练样本建立不完全确定内部机理的表示与关系的系统，对未知样本类别决策。\n\n3. 监督模式识别与非监督模式识别\n\n监督模式识别：利用了有标签的训练样本。\n非监督模式识别：没有利用有标签的训练样本，对训练样本采用不同的划分方法可能导致不同的结果。\n\n4. 模式识别系统举例\n\n语音识别：对一系列连续的音素进行分类，需考虑音素之间的相互影响。例如利用多阶隐马尔可夫模型。\n说人话识别：与语言识别基本原理相同，只是分类目标由语音变成了说话人。\n字符与文字识别：OCR（detection-classification）等。单字识别是OCR的基础，将图片向多方向投影得到像素密度即数量特征；根据对汉字结构的认知提取有效特征点并编码成数字特征。特征提取后每个字就是一个特征向量代表的样本，接下来涉及到多分类问题。分类器设定通常需要结合对文字结构的认知（旋转和尺度不变性）。\n复杂图像中特定目标的识别：目标检测方法判断每个子图像是汽车还是背景，检测出汽车后可追踪其在连续图像中的运动轨迹来识别是否有违章行为等；可根据汽车图像识别出车牌位置再利用数字识别识别车牌号等；路人检测再进行人脸识别，行为识别等。\n根据地震勘探数据对地下储层性质的识别：在探井处利用地震信号提取特征并结合地下储层性质类别建立分类器；探井数不足以用来训练时可利用非监督学习方法对地震勘测信号聚类划分，由地质学家分析划分来实现对储层性质的识别。\n利用基因表达数据进行癌症的分类：利用基因表达作为病例特征研究病例之间的分类和聚类；利用病例表达作为基因特征研究基因之间的分类和聚类等。\n\n5. 模式识别系统的典型构成\n\n处理监督模式识别的一般步骤 分析问题，原始特征提取，特征提取与选择，分类器设计，分类决策；\n处理非监督模式识别的一般步骤 分析问题，原始特征提取，特征提取与选择，聚类分析，结果解释；\n\n本章概要：模式，样本，样本集，类或类别，特征，已知样本，未知样本。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"Test","url":"/2022/04/26/Test/","content":"主要内容\n\nMarkdown是什么？\n谁创造了它？\n为什么要使用它？\n怎么使用？\n谁在用？\n尝试一下\n\n\n本文来自于网络，仅用作测试\n正文\n1. Markdown是什么？\nMarkdown是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，并最终以HTML格式发布。\nMarkdown也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。\n2. 谁创造了它？\n它由Aaron Swartz和John Gruber共同设计，Aaron Swartz就是那位于去年（2013年1月11日）自杀,有着开挂一般人生经历的程序员。维基百科对他的介绍是：软件工程师、作家、政治组织者、互联网活动家、维基百科人。\n他有着足以让你跪拜的人生经历：\n+ 14岁参与RSS 1.0规格标准的制订。\n+ 2004年入读斯坦福，之后退学。\n+ 2005年创建Infogami，之后与Reddit合并成为其合伙人。\n+ 2010年创立求进会（Demand Progress），积极参与禁止网络盗版法案（SOPA）活动，最终该提案被撤回。\n+ 2011年7月19日，因被控从MIT和JSTOR下载480万篇学术论文并以免费形式上传于网络被捕。\n+ 2013年1月自杀身亡。\n\nAaron Swartz\n\n天才都有早逝的归途。\n3. 为什么要使用它？\n\n它是易读（看起来舒服）、易写（语法简单）、易更改纯文本。处处体现着极简主义的影子。\n兼容HTML，可以转换为HTML格式发布。\n跨平台使用。\n越来越多的网站支持Markdown。\n更方便清晰地组织你的电子邮件。（Markdown-here, Airmail）\n摆脱Word（我不是认真的）。\n\n4. 怎么使用？\n如果不算扩展，Markdown的语法绝对简单到让你爱不释手。\nMarkdown语法主要分为如下几大部分： 标题，段落，区块引用，代码区块，强调，列表，分割线，链接，图片，反斜杠 \\，符号'`'。\n4.1 标题\n两种形式：\n1）使用=和-标记一级和二级标题。 &gt; 一级标题\n&gt; =========\n&gt; 二级标题\n&gt; ---------\n效果： &gt; 一级标题\n&gt; =========\n&gt; 二级标题 &gt; ---------\n2）使用#，可表示1-6级标题。 &gt; # 一级标题\n&gt; ## 二级标题\n&gt; ### 三级标题\n&gt; #### 四级标题\n&gt; ##### 五级标题\n&gt; ###### 六级标题\n效果： &gt; # 一级标题\n&gt; ## 二级标题\n&gt; ### 三级标题\n&gt; #### 四级标题\n&gt; ##### 五级标题\n&gt; ###### 六级标题\n4.2 段落\n段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个以上空格加上回车（引用中换行省略回车）。\n4.3 区块引用\n在段落的每行或者只在第一行使用符号&gt;,还可使用多个嵌套引用，如： &gt; &gt; 区块引用\n&gt; &gt;&gt; 嵌套引用\n效果： &gt; 区块引用\n&gt;&gt; 嵌套引用\n4.4 代码区块\n代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如\n普通段落：\nvoid main()\n{\nprintf(\"Hello, Markdown.\");\n}\n代码区块：\nvoid main()\n{\n    printf(\"Hello, Markdown.\");\n}\n注意:需要和普通段落之间存在空行。\n4.5 强调\n在强调内容两侧分别加上*或者_，如： &gt; *斜体*，_斜体_\n&gt; **粗体**，__粗体__\n效果： &gt; 斜体，斜体\n&gt; 粗体，粗体\n4.6 列表\n使用·、+、或-标记无序列表，如： &gt; -（+*） 第一项 &gt; -（+*） 第二项 &gt; - （+*）第三项\n注意：标记后面最少有一个_空格_或_制表符_。若不在引用区块中，必须和前方段落之间存在空行。\n效果： &gt; + 第一项 &gt; + 第二项 &gt; + 第三项\n有序列表的标记方式是将上述的符号换成数字,并辅以.，如： &gt; 1 . 第一项\n&gt; 2 . 第二项\n&gt; 3 . 第三项\n效果： &gt; 1. 第一项 &gt; 2. 第二项 &gt; 3. 第三项\n4.7 分割线\n分割线最常使用就是三个或以上*，还可以使用-和_。\n4.8 链接\n链接可以由两种形式生成：行内式和参考式。\n行内式： &gt; [younghz的Markdown库](https:://github.com/younghz/Markdown \"Markdown\")。\n效果： &gt; younghz的Markdown库。\n参考式： &gt; [younghz的Markdown库1][1]\n&gt; [younghz的Markdown库2][2]\n&gt; [1]:https:://github.com/younghz/Markdown \"Markdown\"\n&gt; [2]:https:://github.com/younghz/Markdown \"Markdown\"\n效果： &gt; younghz的Markdown库1\n&gt; younghz的Markdown库2\n注意：上述的[1]:https:://github.com/younghz/Markdown \"Markdown\"不出现在区块中。\n4.9 图片\n添加图片的形式和链接相似，只需在链接的基础上前方加一个！。 #### 4.10 反斜杠\\ 相当于反转义作用。使符号成为普通符号。 #### 4.11 符号'' 起到标记作用。如： &gt;\\ctrl+a`\n效果： &gt;ctrl+a\n5. 谁在用？\nMarkdown的使用者： + GitHub + 简书 + Stack Overflow + Apollo + Moodle + Reddit + 等等\n6. 尝试一下\n\nChrome下的插件诸如stackedit与markdown-here等非常方便，也不用担心平台受限。\n在线的dillinger.io评价也不错\n\nWindowns下的MarkdownPad也用过，不过免费版的体验不是很好。\n\nMac下的Mou是国人贡献的，口碑很好。\nLinux下的ReText不错。\n\n当然，最终境界永远都是笔下是语法，心中格式化 :)。\n\n注意：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。 虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber] (http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/ )。 **** 以上基本是所有traditonal markdown的语法。\n其它：\n列表的使用(非traditonal markdown)：\n用|表示表格纵向边界，表头和表内容用-隔开，并可用:进行对齐设置，两边都有:则表示居中，若不加:则默认左对齐。\n\n\n\n代码库\n链接\n\n\n\n\nMarkDown\nhttps://github.com/younghz/Markdown\n\n\nMarkDownCopy\nhttps://github.com/younghz/Markdown\n\n\n\n关于其它扩展语法可参见具体工具的使用说明。\n","categories":["Test"],"tags":["Test"]}]