[{"title":"README","url":"/2022/06/01/README/","content":"\n\n\n封面图片出自哪我忘了，作为一个懒人懒得引用链接了\n\n本站说明\n\n啊，还没想好，至少建站目标是🌟 （似乎，貌似，可能，应该，9成达不到。。。至少是一大段时间内。\n博主可能23年4月份才会开始认真写blog吧，到时重新部署一下Blog，Hexo 还是差点格调。\n\n更新日志\n第 0 期更新\n\n搬运了以前模式识别相关的一些笔记，参考 / 抄 的是（模式识别.第四版，机器学习西瓜书，统计学习方法，凸优化，wiki，Pattern Recognition and Machine Learning，Pattern Classification）。如果读者没有相关基础的话还是不建议看了，自我评价是写的挺糟糕的（本来就是笔记可以理解！！\n话说：模式识别和西瓜书看起来就像是最后两者的摘记再格外缝合了一些热点内容（没有黑的意思，作者还是挺用心的），上面硬核内容的基本还得看原书才能真正看明白。\n\n第 1 期更新（23年4月开始，但也不排除博主心血来潮在22年暑假结束前就更完了（也有提桶跑路的可能\n包括但不限于以下内容\ndeep learning（花书）\n22年cs231n\n即便是AI领域的一个萌新 researcher 也应该有所了解的论文：\nLeNet-5：GradientBased Learning Applied to Document Recognition\nImageNet： ImageNet: A large-scale hierarchical image database\nAlexNet：ImageNet Classification with Deep Convolutional Neural Networks\nNIN：Network In Network\nVGG：Very Deep Convolutional Networks for Large-Scale Image Recognition\nGoogLeNet：Going Deeper with Convolutions\nResNet：Deep Residual Learning for Image Recognition\nSENet：Squeeze-and-Excitation Networks\nDenseNet：Densely Connected Convolutional Networks\nRNN：Finding Structure in Time\nLSTM： LONG SHORT-TERM MEMORY\nGRU：Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks\nSeq2Seq：Sequence to Sequence Learning with Neural Networks\nLearning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation：\nLearning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\n首次将带有注意力机制的神经网络应用于机器翻译：\nNEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\nBP：Learning representations by back-propagating errors\nKnowledge Distilling ：Distilling the Knowledge in a Neural Network\nAdversarial Examples：EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES\nDeep Q-Learning：Human-level control through deep reinforcement learning\nNeural Architecture Search with Reinforcement Learning： Neural Architecture Search with Reinforcement Learning\nGNN：The Graph Neural Network Model\nGCN：Semi-Supervised Classification with Graph Convolutional Networks\nA survey of transfer learning：A survey of transfer learning\nBatch Normalization： Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\nGroup Normalization：Group Normalization\nAdam：Adam: A Method for Stochastic Optimization\nStyle Transfer：Image Style Transfer Using Convolutional Neural Networks\nPerceptual Losses for Real-Time Style Transfer and Super-Resolution： Perceptual Losses for Real-Time Style Transfer and Super-Resolution\ndeep dream：deep dream\nShow, Attend and Tell：Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\nVisualizing and Understanding Convolutional Networks\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\nUnderstanding Neural Networks Through Deep Visualization\nA Learned Representation For Artistic Style\n\nTransformers：其重要性可用 Pay Attention to MLPs 正文的第一句话来概括 ! Transformers [1] have enabled many breakthroughs in natural language processing (e.g., [2, 3, 4, 5, 6]) and have been shown to work well for computer vision (e.g., [7, 8, 9, 10]). Thanks to this success, Transformers have largely replaced LSTM-RNN [11] as the default architecture in NLP, and have become an appealing alternative to ConvNets [12, 13, 14, 15, 16, 17] in computer vision.\nTransformers：Attention Is All You Need\nTransformers are RNNs：Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\nVision transformer（ViT）：An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nSwitch Transformers：Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\nSwin Transformer：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\nSwinIR：SwinIR: Image restoration using swin transformer\nBEiT： BERT Pre-Training of Image Transformers 视觉BERT预训练模型\niBOT：Image BERT Pre-Training with Online Tokenizer\nMAE：Masked Autoencoders Are Scalable Vision Learners （CV 版本的Bert？）通向CV大模型\nSimMIM： SimMIM：a Simple Framework for Masked Image Modeling\nSimCLR：A Simple Framework for Contrastive Learning of Visual Representations\nMoCo：Momentum Contrast for Unsupervised Visual Representation Learning\nMoCo v2：Improved Baselines with Momentum Contrastive Learning\nMoCo v3：An Empirical Study of Training Self-Supervised Vision Transformers\nConvMAE：ConvMAE: Masked Convolution Meets Masked Autoencoders\nContrastive Predictive Coding (CPC)：Representation Learning with Contrastive Predictive Coding\nContrastive Language Image Pre-training (CLIP) CLIP: Connecting Text and Images\n\nGPT-1：Improving Language Understanding by Generative Pre-Training\nGPT-2：Language Models are Unsupervised Multitask Learners\nGPT-3：Language Models are Few-Shot Learners\niGPT ：Image GPT\nGenerative Pretraining from Pixels：Generative Pretraining from Pixels\nBERT：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nword2vec：Efficient Estimation of Word Representations in Vector Space\nGlove：GloVe: Global Vectors for Word Representation\nELMo：Deep contextualized word representations\n\nPretext task:\npredict rotations：Unsupervised Representation Learning by Predicting Image Rotations\npredict relative patch locations：Unsupervised Visual Representation Learning by Context Prediction\nsolving “jigsaw puzzles”：Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\npredict missing pixels (inpainting)：Context Encoders: Feature Learning by Inpainting\nimage coloring：Colorful Image Colorization\nvideo coloring：Tracking Emerges by Colorizing Videos\nTransfer learned features to supervised learning：Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction\n\nVariational Inference：Variational Inference: A Review for Statisticians\nVAE ：Auto-Encoding Variational Bayes\nGAN：Generative Adversarial Networks\nCGAN：Conditional GANs\nDCGAN：Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\nImproved Techniques for Training GANs：improved Techniques for Training GANs\nPix2Pix：Image-to-Image Translation with Conditional Adversarial Networks\nWGAN：Wasserstein GAN\nCycleGAN：Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\nStyleGAN：A Style-Based Generator Architecture for Generative Adversarial Networks\nBigGAN：Large Scale GAN Training for High Fidelity Natural Image Synthesis\nSAGAN：Self-Attention Generative Adversarial Networks\n\nSemantic Segmentation Idea: Fully Convolutional\nFCN：Fully Convolutional Networks for Semantic Segmentation\nDeconvNet： Learning Deconvolution Network for Semantic Segmentation\nobject instance segmentation：Mask R-CNN\n\nObject Detection\nMeasuring the objectness of image windows\nSelective Search for Object Recognition\nBinarized normed gradients for objectness estimation at 300fps\nEdge Boxes: Locating Object Proposals from Edges\nOverFeat：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\nR-CNN：Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5)\nFast R-CNN：Fast R-CNN\nFaster R-CNN：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\nYOLO 1：You Only Look Once: Uniﬁed, Real-Time Object Detection\nYOLO 2：YOLO9000: Better, Faster, Stronger\nYOLO 3：YOLOv3: An Incremental Improvement\nYOLO 4：YOLOv4: Optimal Speed and Accuracy of Object Detection\nYOLO 5：A Deep Learning Object Detection Method for an Efficient Clusters Initialization\n\nVideo Understanding\nTwo-Stream：Two-Stream Convolutional Networks for Action Recognition in Videos 视频理解开山之作\nLarge-scale Video Classification with Convolutional Neural Networks\nLarge-scale Video Classification with Convolutional Neural Networks\nConvolutional Two-Stream Network Fusion for Video Action Recognition\nConvolutional Two-Stream Network Fusion for Video Action Recognition\nLearning Spatiotemporal Features with 3D Convolutional Networks\nTran et al, “Learning Spatiotemporal Features with 3D Convolutional Networks”, ICCV 2015\ninflated 3D network（I3D）： Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\nA Comprehensive Study of Deep Video Action Recognition\nA Comprehensive Study of Deep Video Action Recognition\n\n\nAlphago：Mastering the game of Go with deep neural networks and tree search\nAlphaCode：Competition-Level Code Generation with AlphaCode\nCopilot背后的功臣：OpenAI Codex：Evaluating Large Language Models Trained on Code\nCLIP ：Contrastive Language–Image Pre-training)\nAlphaFold（突破性研究）：AlphaFold: Improved protein structure prediction using potentials from deep learning\nALphaFold 2：Highly accurate protein structure prediction with AlphaFold\nAdvancing mathematics by guiding human intuition with AI：\nAdvancing mathematics by guiding human intuition with AI\nSkillful Precipitation Nowcasting using Deep Generative Models of Radar：\nSkillful Precipitation Nowcasting using Deep Generative Models of Radar\n\n","categories":["README"],"tags":["README"]},{"title":"十三，模式识别系统的评价","url":"/2021/10/13/Pattern%20Recognition/%E5%8D%81%E4%B8%89%EF%BC%8C%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BB%B7/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BB%B7/","content":"\n\n\n\n1. 错误率估计\n通常采用实验方法估计分类器的错误率：训练错误率，测试错误率。\n1.1 交叉验证\n总样本不变，随机选用一部分样本作为临时的训练集，用剩余样本作为临时的测试集，得到一个错误率估计；然后随机选用另外一部分样本作为临时训练集，其余样本作为临时测试集，再得到一个错误率估计，如此反复多次，最后将各个错误率求平均，得到交叉验证错误率。一般让临时训练集较大，临时测试集较小，这样得到的错误率估计就更接近用全部样本作为训练样本时的错误率。而测试集过小带来的错误率估计方差大的问题通过多轮实验的平均可以得到一定的缓解。\nk轮n倍交叉验证\n总样本随机划分为n个等份，在一轮实验中轮流抽出其中的1份样本作为测试样本，用其余（n-1）份作为训练样本，得到n个错误率后进行平均，作为一轮交叉验证的错误率；由于对样本的一次划分是随意的，往往进行多轮这样的划分（比如k轮），得到多个交叉验证错误率估计，最后将多个估计再求平均。\n留一法交叉验证\n样本数较少时最常用的方法。每轮实验拿出一个样本来作为测试样本，用其余的N一1个样本作为训练样本集，训练分类器。下一轮把之前测试的样本放回，拿出另外一个样本作为测试样本，用剩余的N一1个样本作训练，再对留出的样本作测试，依次类推。全部N轮实验完成后，统计总共出现的测试错误数占总样本数的比例就是留一法交叉验证错误率。\n1.2 自举法与.632估计\n自举样本集包含原样本集中63.2%左右的样本。训练错误率是对真实错误率偏乐观的估计，而自举错误率是偏保守的估计。将这两种估计按照一定的方式结合起来有.632 估计。是在全部样本上的训练错误率， 是自举错误率。 是对错误率更好的估计。\n\n事实上，考虑样本随机性，如果仅基于交叉验证，不存在错误率估计量方差的无偏估计。\n2. 用扰动重采样估计SVM错误率的置信区间\n之所以单纯靠样本划分或重采样无法获得对分类器错误率变换范围的无偏估计，是因为在划分或重采样得到的数据集之间存在不可避免的相关性。这一问题可以通过适当引人扰动的方法来解决。\n3. 特征提取与选择对分类性能的影响\n\nCV1\n把所有样本都用来进行特征选择与提取，而后把所选择和提取的特征固定下来，再把样本分成训练集和测试集。\n\n\nCV2\n在未作任何特征选择与提取前把测试样本和训练样本分开，在每一轮里只用训练样本选择和提取特征。采用CV2策略进行交叉验证，可以得到对包括特征选择与提取部分在内的模式识别系统性能的真实估计，但是却没有得到一组唯一的用于分类的特征。这时，一种做法是利用所有样本重新进行一次特征选择与提取，得到唯一的特征组合，用所有样本设计分类器；另外一种做法是，将CV2交叉验证中得到的各个特征组合方案进行综合，从中选择在各轮交叉验证中被选中次数最多的若干特征组成最后的特征集，用这些特征在所有样本上设计分类器。\n\n4. 从分类显著性推断特征与类别的关系\n随机置换法：在保持已知样本集中两类样本比例不变的情况下，随机打乱样本的类别标号。再用同样的特征选择、提取和分类方法进行分类，得到的分类性能就反映了这样的模式识别方法在无分类信息的数据上的表现。多次重新随机置换样本类别标号，就可以统计出在没有分类信息情况下模式识别分类性能的空分布，然后把在真实数据上得到的性能估计与这个空分布进行比较，得到分类器性能的随机置换P值。如果该P值很小（比如，通常以小于0.05为参考），则说明在原样本集上得到的分类性能具有统计显著性，初步推断系统S很可能真实存在。\n5. 非监督模式识别系统性能的评价\n\n紧致性(compactness)或一致性(homogeneity)\n最常见的指标是类内方差或者平方误差和。除此还有其他类型的类内一致性度量：类内两两样本之间的平均或最大距离、平均或最大的基于质心的相似度，基于图理论的紧致性度量等。比如,可用指标： \n\n\n连接性质(connectedness)\n衡量了聚类是否遵循了样本的局部密度分布及相邻的样本是否被划分到同一类。如连接度(connectivity)，即样本中相邻的数据点被划分到同一个聚类中的程度。如果第  个样本与其第  个近邻不在同一个聚类中,则 , 否则为 0 。连接度指标越小越好。 \n\n\n分离度(separation) 可用两类中心距离或两类最近样本之间的距离来计算两类间的距离。分离度指标越大则各类间分离越好。\n\n\n综合性质的评价准则: 紧致性与分离度是两个极端,如果不断增加聚类数目,紧致性将会随之增加,但分离度也会相应的减小。因此,可将这两 个指标组合起来, 定义能同时反映类内距离和类间距离的新指标，比如Silhcuette 值:   代表样本  到和它同类的所有样本的平均距离,  表示样本  到其他聚类中最近一个聚类的所有样本的平均距离。所 有样本的 Silhouette 值的平均称作 Silhouette 宽度, 其取值在  之间。越大则聚类效果越好。\n\n\nDunn 指数(Dunn index)   是聚类  中最大的类内距离,  是  和  两类中相邻最近的样本对间的距离。  的取值 范围是 , 此指数的目标是最大化。\n\n由于没有先验认识，非监督学习的目标是多样的，无论采用什么方法混合多个指标，都不可避免地导致某些方面信息的损失。另外一种不同的策略是，同时评价各个指标， 并且仅当一个方法在某一个指标上超出另一个方法、同时在所有指标上都等同于或超出另一方法时，才断定该方法在聚类性能上胜过另一方法。这是个多目标优化的问题。可以用多目标优化的方法，来寻找在多个指标上都优胜的聚类方法，或确定方法中的可变参数。\n\n稳定性\n即其结果的可重复性。采用重复地随机重采样或者对样本加人随机扰动等方法获得多个不同的样本集，在不同的样本集上实施同样的聚类算法，定义某个统计量来衡量在这些重采样或扰动的样本集上得到的聚类结果的一致性，用它来评价从原始的数据上得到的聚类结果的显著性。\n\n\n预测效力\n将样本随机地划分成两份，两份样本上都各自进行聚类；用其中一份中得到的聚类结果作为临时训练样本，对另外一份中的样本实行最近邻法分类，比较这样的分类与直接在这份样本上的聚类划分之间的重合程度，重合程度越大则聚类结果越稳定。实际应用中，这样的实验通常需要多次重复进行，最后以指标的平均值来作为稳定性的度量。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"十二，深度学习","url":"/2021/10/12/Pattern%20Recognition/%E5%8D%81%E4%BA%8C%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","content":"\n\n\n\n\n书上这章讲的太浅了，以后单独开个专题写深度学习（事实上模式识别这个专题都是些以前的笔记，没啥好看的\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"十一，非监督学习与聚类","url":"/2021/10/11/Pattern%20Recognition/%E5%8D%81%E4%B8%80%EF%BC%8C%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%81%9A%E7%B1%BB/%E9%9D%9E%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/","content":"\n\n\n\n1. 混合模型的估计\n1.1 假设条件\n定义混合密度  对数似然函数  最大似然估计 \n\n1.2 可识别问题\n设 ，如果对于混合分布中每个  都有 ，则称密度  是可识别的。常见连续随机变量的分布密度函数都是可识别的，而离散随机变量的混合概率函数则往往是不可识别的，这是由于混合分布中末知参数  的分量数目多于独立方程的数目而造成的。\n1.3 计算问题\n假设似然函数可识别且对  可微  如果当  时  和  的元素在函数上是独立的，引进后验概率  则有  得出最大似然估计  满足  解这个微分方程组就可以得到参数  的最大似然估计 \n\n先验概率 末知时，对  的最大似然值的搜索应有以下约束 且\n假设似然函数可微，利用求条件极值的 Lagrange 乘子法。 对数似然函数：  Lagrange 函数  对  求导并使导数为零，即可解出  的最大似然解   利用贝叶斯公式  将以上  个方程相加有  得  的最大似然估计  即对式（1）有  其中  最大似然解  和 ，原则上可以从以上微分方程组中解出，但通常采用迭代法求解。\n\n2. 高斯混合模型\n模型中的各个分布是多维正态分布的 ,\n2.1 均值向量  末知时\n这时参数  就是  有必要条件。  即  的最大似然估计  应满足  两边左乘  可得  它表明  的最大似然估计是样本加权平均。其中对第  个样本  的权就是属于第  类有多大可能性的一个估计。如果  对某些样本为 1，而对其余样本为零，那么  是被估计为属于第  类样本的平均。但遗惐的是并没有明显地给出  的值,这是因为其中  是末知的。利用贝叶斯公式  并将  代入，将变成一组十分复杂的非线性联立方程组，求解是相当困难的，而且一般没有唯一解，必须对得到的解进行检验以获得一个实际上使似然函数为最大的解。为解决这一困难，可以应用迭代法。如果有某种方法能得到末知均值的一个较好的初始估计 ，可以得到一种迭代算法来改进估计。  这基本上是一种使对数似然函数极大化的梯度法，分量密度之间的重叠较时收敛较快。显然，这个算法也存在一般梯度法的缺点，即算法得到的不是全局最优解，而是局部最优解，其结果将受到初值  的影响，甚至可能收敛到鞍点，对运算的结果应注意分析和检验。\n2.2  均末知\n只有类别数目  已知。在限制条件  下构造出 Lagrange 函数  对  分别相对于  及  求导,并令导数为零。可得  其中 :  当  来自  类时，即 。此时有:  上式说明  是来自  的样本的百分比，  为  类的样本均值， 为  类的样本协方差阵。一般情况下  是介于 0 与 1 之间的数，而且所有的样本都对估计值起某种作用，这些估计基本上仍然是加权的频数比、样本均值和样本协方差阵。\n\n解这些方程一般是很困难的，有效的方法还是采用迭代法，即用一个初始估计值计算式 , 然后反复迭代。如果初始估计较好，比如它是用一些标有类别的样本求出的，那么收敛就比较快。但所得结果与初值选择有关，因而所得的解仍是局部最优解。另一个问题是迭代算法和样本协方差阵求逆都需要很多运算时间。要是有理由假定样本协方差矩阵是对角阵，那么运算可大大简化，而且还减少了末知参数的个数。在样本数不多的情况下，减少末知参数的个数是十分重要的。如果对角阵假设太勉强, 但可以假定 c 个协方差矩阵都相同的话, 也能减少计算时间。 不论是参数化的混合模型估计方法, 还是非参数化的单峰子集分离方法，由于涉及样本概率密度估计的问题，都需要较多的样本数或对样本分布的先验知识。这在很多非监督学习问题中是不易满足的。除了这类基于模型的方法，人们还发展了很多直接基于数据进行聚类的方法。\n\n3. 动态聚类算法 \n(1)选定某种距离度量作为样本间的相似性度量。 (2)确定某个评价聚类结果质量的准则函数。 (3)给定某个初始分类，然后用迭代算法找出使准则函数取极值的最好聚类结果。\n3.1  均值算法 \n\nc 均值算法就是 k 均值算法。书上这里夹带私货，但说错其实也没错。\n\n3.2 ISODATA 方法\n与  均值算法相比： 第一，它把全部样本调整完后才重新计算各类的均值。 第二，聚类过程中引入了对类别的评判准则，可以根据这些准则自动地将某些类别合并或分裂。\n\n\n确定  个初始中心 \n把所有样本分到距离中心最近的类  中\n若，则去掉类，并根据到其他类中心的距离合入其他类， 。\n重新计算均值 \n计算第  类样本与其中心的平均距离和总平均距离 \n若 ，转(7) ； 若 , 转(8)； 若是偶数次迭代转(8)，否则转(7)。\n(分裂)\n\n\n\n对每个类，求各维标准偏差 \n对每个类，求出标准偏差最大的分量 \n对各类的 ，若存在某个类的 ，且  和 , 或 ，则将  分裂为两类，中心分别为 和 ，置   分裂项可以为  为常数)，也可以是  。\n\n\n\n(合并)\n\n\n\n计算各类中心两两之间的距离 \n比较  与  (合并参数)，对小于  者排序 \n从  开始，把每个  对应的  和  合并，组成新类，新的中心为  并置  。每次迭代中避免同一类被合并两次。\n\n\n\n若是最后一次迭代则终止；否则迭代次数加 1 , 转 (2)。 (必要时可调整算法参数)\n\n3.3 基于样本与核相似度量的动态聚类算法\n准则函数 \n\n定义一个样本  到核的距离 \n\n步骤 1 确定每类的初始核  。 步骤 2 若  则  。 步骤 3 重新修正核  。若核  保持不变，则算法终止； 否则转 2 。\n\n\n正态核函数  参数集为 , 样本到核的相似性度量为 \n主轴核函数样本的主轴可通过  变换得到。  这里  是和  矩阵的  个最大本征值相对应的本征向量系统。 任何一个样本  与  之间的相似性程度可以用  与  类主轴之间的欧氏距离的平方来度量。 \n\n\n4. 模糊聚类方法 \n\n隶属度函数表示一个对象  隶属于集合  的程度。自变量范围是所有可能属于集合  的对象，且  。 对于有限个对象  ，模糊集合  可以表示为:  或 \n\n模糊  均值 损失函数   是一个可以控制聚类结果模糊程度的常数。通常选择  取值在 2 左右。在不同的隶属度定义下最小化损失函数就得到不同的模糊聚类方法。FCM 要求一个样本对于各个聚类的隶属度之和为 1, 即  在上条件下求  的极小值，令  对  和  的偏导数为 0 ，可得必要条件：  和 \n\n用迭代方法求解式上式:\n\n设定聚类数目  和参数 \n初始化各个聚类中心 \n重复下面的运算,直到各个样本的隶属度值稳定： ① 用当前的聚类中心根据计算隶属度函数; ② 用当前的隶属度函数更新计算各类聚类中心。 当算法收敛时,就得到了各类的聚类中心和各个样本对于各类的隶属度值,从而完成了模糊聚类划分。 还可以将模糊聚类结果进行去模糊化,即用一定的规则把模糊聚类划分转化为确定性分类。\n\n\n改进的模糊 C 均值算法\n\n在模糊  均值算法中，由于引人了归一化条件，在样本集不理想的情况下可能导致结果不好。野值的存在将影响迭代的最终结果。为此，提出了放松的归一化条件，使所有样本对各类的隶属度总和为 , 即  在这个新的条件下,计算  仍不变,而  变成 \n\n5. 分级聚类方法\n\n初始化， 每个样本形成一个类;\n合并: 计算任意两个类之间的距离(或相似性)，把距离最小(或相似性最大)的两个类合并为一类，记录下这两个类之间的距离(或相似性),其余类不变;\n重复 (2), 直到所有样本被合并到两个类中。 算法核心问题是如何度量样本之间以及类之间的距离或相似性度量。样本之间采用何种距离或相似性度量， 取决于所面对的问题中特征的物理意义及相互之间的关系。如果特征是欧式空间中的向量, 通常可以用欧氏距离作为距离度量, 或用相关系数作为相似性度量。在两个样本之间距离或相似性度量确定后，有三种方法定义两个类  之间的距离或相似性度量，也称作类间的连接\n\n\n\n最近距离 \n最远距离 \n均值距离 \n\n\n6. 自组织映射神经网络\n\nSOM网络的神经元节点都在同一层上，在一个平面上呈规则排列。常见的排列形式包括方形网格排列或蜂窝状排列。样本特征向量的每一维都通过一定的权值输入到SOM网络的每一个节点上。神经元节点之间并没有直接的连接，但在神经元平面上相邻的节点间在学习（训练）过程中有一定的相互影响，构成邻域相互作用。神经元节点的计算功能是对输入的样本给出响应。输入向量连接到某个节点的权值组成的向量称作该节点的权值向量。一个节点对输入样本的响应强度，就是该节点的权值向量与输入向量的匹配程度，可以用欧氏距离或者内积来计算，如果距离小或内积大则响应强度大。对一个输人样本，在神经元平面上所有的节点中响应最大的节点称作获胜节点。\n\n学习算法  是  维样本向量集合，记所有神经元集合为 ，第  个每个神经元的权值为  (1) 权值初始化: 用小随机数初始化权值向量。注意各个节点的初始权值不能相等。 (2) 在时刻 ，按照给定的顺序或随机顺序加入一个样本，记为  。 (3) 计算神经元响应，找到当前获胜节点  如用欧氏距离作为匹配准则，则获胜节点为  (4) 权值竞争学习。对所有神经元节点,用下述准则更新各自的权值  其中,  是学习的步长,  是两个向量间的欧氏距离，  是节点  与  间的近邻函数值，如果采用方形网格结构，则相当于在节点  的周围定义一个矩形邻域范围  。  (5) 更新步长  和邻域 ，达到终止条件，则算法停止; 否则置 , 继续（2） 这是一个自学习的过程，在学习过程中没有已知的类别标号做引导，也无法定义类似训练误差之类的收敛目标。这个算法终止条件一般是事先确定的迭代次数。为了网络能够更有效地达到自组织状态，步长  和邻域  通常在算法开始时可以设置得大一些，而随着时间  的增加单调减小（Warm up），到算法终止时邻域缩小到只包含最佳节点本身。除了矩形邻域外，还可以使用其他形式的邻域函数，如高斯函数等。在经过了适当的自学习后，SOM 网络会表现出自组织现象: 对某个输入样本 , 对应获胜节点  会逐渐趋于固定。把固定下来的获胜节点  称作样本  的像, 把样本  称作柛经元节点  的原像。一个样本只能有一个像，而一个神经元可能有多个原像, 也可能没有原像。当学习过程终止后，可以统计在每个神经元节点上有多少个原像，即有多少个样本映射到该节点，把这个量叫做像密度。如果把各个节点的像密度按照神经元本来的排列图示出来，就得到一张像密度图。SOM 网络的自组织现象，就是在对样本经过了适当的学习后，每个样本固定映射到一个像节点，在原样本空间中距离相近的样本趋向于映射到同一个像节点或者在神经元平面上排列。 对于有  个样本的样本集，如果邻域函数固定(不随学习时间改变)，则 SOM 学习算法实际是通过梯度下降算法最小化：  ​ 事实上，如果在 SOM 学习算法中取消邻域作用，即只对获胜节点自身做权值修正，SOM就退化为  均值算法的一种随机迭代实现，其中的聚类数  就是神经元结点的数目。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"十，特征提取与降维表示","url":"/2021/10/10/Pattern%20Recognition/%E5%8D%81%EF%BC%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E9%99%8D%E7%BB%B4%E8%A1%A8%E7%A4%BA/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/","content":"\n\n\n\n1. 基于类别可分判据的特征提取\n求最优的  使 \n\n基于类内类间距离的可分性判据 ，求 使下列准则最优  这些准则得到的最优变换矩阵是相同的： 设矩阵  的本征值为 ，按大小顺序排列$ {1} {2} _{D}$，选前  个本征值对应的本征向量构成的变换阵就是在这些准则下的最优变换阵  。（LDA基于  ）\n\n也可以采用基于概率距离的判据或基于熵的判据作为准则来进行特征提取。但一般情况下只能靠数值求解，在数据服从正态分布并满足某些特殊条件时可以得到形式化的解。\n2. 主成分分析法 \n设新特征  是原始特征  的线性组合，并且相互之间不相关。  统一尺度  矩阵形式  求解最优的正交变换  使新特征  的方差达到极大值。正交变换保证了新特征间不相关，而新特征的方差越大，特征也就越重要。  拉格朗日函数  求导得最优解  满足  即  是矩阵  的本征向量，  是对应的本征值。  最优的  是  的最大本征值对应的本征向量。  称作第一主成分。对第二个新特征，它除了满足和第一个特征同样的要求（方差最大、模为 1），还必须与第一主成分不相关，即：  可得：  考虑到 ，不相关的要求等价于要求  和  正交。在  和  的约束条件下最大化  的方差，可以得到  是  的第二大本征值对应的本征向量，  称作第二主成分。以此类推可得，协方差矩阵  共有  个本征值 ，从大到小排序为  。对应这些本征值的本征向量为  个主成分  。  它等于各个原始特征的方差之和。且  是正交矩阵。从  到  的逆变换是  取前  个主成分，可知这  个主成分所代表的数据全部方差的比例是  数据中的大部分信息常集中在较少的几个主成分上。可根据本征值谱图来决定选择几个主成分来代表全部数据。可以事先确定希望新特征所能代表的数据总方差的比例，根据上式来试算出适当的  。计算中常把主成分进行零均值化，这种平移并不影响主成分的方向。即： \n3. K - L 变换 \n 变换能够考虑到不同的分类信息实现监督的特征提取，其是从  展开引出的。对  维随机向量 ，可以用一个完备的正交归一向量系  来展开  其中  有  用有限的  项  来逼近   与原向量的均方误差是  记 ，即  的二阶矩阵  要在正交归一的向量系中最小化这一均方误差，就是求解下列优化问题  采用拉格朗日法，得到无约束的目标函数  对各个向量求偏导并令其为零得  即  是矩阵  的本征向量满足   是矩阵  的本征值，均方误差为  把矩阵  的本征值按从大到小排列，选前  个本征值对应的本征向量，即可使表示样本均方误差最小。  组成了新的特征空间，样本  在这个新空间上的展开系数  就组成了样本的新的特征向量。\n\n矩阵  称作  变换的产生矩阵。这里得到的  个新特征与主成分分析中的  个主成分很相似，当原特征为零均值或者对原特征进行去均值处理时，二者等价。\n\n\n样本集  的 K-L 坐标系是由数据的二阶统计量决定的。 当样本集中的样本没有类别信息时， 坐标系的产生矩阵是  。如果去掉均值信息，也可以用数据的协方差矩阵  作为 K-L 坐标系的产生矩阵，这时 K-L变换就等同于主成分分析。\n\n\n当样本的类别已知时，可以有各种方法在计算二阶矩阵时考虑到类别信息，从而得到不同的 K-L坐标系。例如：如果  是有类别标签  的样本集，各类的先验概率是 ，均值是 ，协方差矩阵是 ，则可以用总类内离散度矩阵  作为  展开的产生矩阵，其中  是第  类样本的协方差矩阵  另一种简单的方法是先分别对各类样本进行 K-L 变换，再把所得到的坐标组合起来。 显然这样得到的 K-L 坐标系只是对本类的样本来说具有 K-L 变换的最优性质。如果对样本的分类信息有特定的认识或要求, 可以设计出一些专门的 K-L变换特征提取方法。\n\n\n有监督\n\n4. 多维尺度法\n根据样本之间的距离关系或不相似度关系在低维空间里生成对样本的一种表示。MDS分为度量型和非度量型两种类型。\n4.1 古典尺度法\n古典尺度法是度量型 MDS 的一种特殊形式。给定一个两两点之间距离的矩阵，确定这些点在空间里的坐标。 假定给定的距离矩阵是欧氏距离。样本矩阵 ，内积矩阵  。欧氏距离  所有两两点之间的欧氏距离组成的矩阵为   是矩阵  的对角线元素组成的向量，即 ，现已知矩阵 求  。 对坐标的平移不会影响样本间的距离，假设所有样本的质心为坐标原点, 即  定义中心化矩阵 \n\n 是单位对角阵。显然  由式 (1) 有 \n\n对  两边乘以中心化矩阵，得  可得内积矩阵  这种做法也称作双中心化(double centering)。\n\n如果  是由欧氏距离组成的矩阵，则  是对称矩阵，可以用奇异值分解的方法来求解   其中  是由矩阵  的本征向量组成的矩阵， 是以  的本征值为对角元素的对角阵，有  如果样本不是中心化的，则只要知道样本的均值向量  就可以求得各个样本原来的坐标  如果要用  维空间来表示这些样本, 则可以按照本征值从大到小排序  用  组成 ，只用这些本征值对应的本征向量组成  。如果已知样本集，从中计算出 ，再用古典尺度法得到  的低维表示，结果与主成分分析相同。\n\n4.2 度量型MDS\n用  作为目标函数，则当  是欧氏距离时，得到的低维空间表示就是样本在主成分上的投影。很多 MDS 压力函数可以统一为：   是对样本对的加权, 比如   是预先定义的函数。比如，如果希望  与  之间是线性关系，则可以选  另外一种常用的压力函数形式是  scale 是一个尺度因子，可取为 scale , 此时压力函数称作 Kruskal 压力。\n4.3 非度量型 MDS \n非度量型 MDS 就是追求样本的坐标能反映出定性的顺序信息。也需要最小化上述形式的目标函数但其中的函数  或  只需要是某种单调函数或弱单调函数即可。这种单调函数可以通过所谓 “单调回归\"来实现。目标是，用低维空间坐标表示的样本点之间的距离关系，尽可能接近地反映原相异度矩阵所表示的顺序关系。\n4.4 MDS 在模式识别中的应用 \n通常用 MDS 在二维或三维上可视化地显示一组复杂样本之间的关系。如果样本间的距离/相异度矩阵是定义在某一特征空间中的，那么 MDS 也可以看作是样本的一种特征变换。\n5. 非线性变换方法\n5.1 核主成分分析\n\n通过核函数计算矩阵 , 其元素为\n解矩阵  的特征方程 \n解矩阵  的特征方程  得到归一化本征向量  按照对应的本征值从大到小排列。本征向量的维数是 , 向量的元素记作   。由于引人了非线性变换,这里得到的非零本征值数目可能超过样本原来的维数。根据需要选择前若干个 本征值对应的本征向量作为非线性主成分。第  个非线生主成分是 \n计算样本在非线性主成分上的投影。对样本 , 它在第  个非线性主成分上的投影是  样本  在前  个非线性主成分上的坐标就构成样本在新空间的表示  。\n\n5.2 IsoMap\n当样本在高维空间中按照某种复杂结构分布时，直接计算两个样本点之间的欧氏距离，就损失了样本分布的结构信息。如果样本分布较密集，可以假定样本集的复杂结构在每个小的局部都可以用欧式空间来近似。计算每个样本与相邻样本之间的欧氏距离；对两个不相邻的样本，寻找一系列两两相邻的样本构成连接这两个样本的路径，用两个样本间最短路径上的局部距离之和作为两个样本间的距离。这种距离称作测地距离。有了样本间的距离矩阵，就可以用度量型 MDS 等方法映射到低维空间。\n5.3 LLE\n\n在原空间中，对样本  选择一组领域样本；\n用这一组邻域样本的线性加权组合重构  ，得到一组使重构误差  最小的权值  ；\n在低维空间里求向量  及其邻域的映射，使对所有样本用同样的权值进行重构得到的误差  最小。\n\n5.4 t-SNE\n\n待更新\n\n\n\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"九，特征选择","url":"/2021/10/09/Pattern%20Recognition/%E4%B9%9D%EF%BC%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","content":"\n\n\n\n1. 类别可分性准则\n定义与错误率有关又便于计算的类别可分性准则 ，用来衡量在一组特征下第  类和第  类之间的可分程度，\n\n其应该满足：\n\n判据应该与错误率(或错误率的上界)有单调关系。\n当特征独立时,判据对特征具有可加性,即 \n应具有度量特性 当时当时\n对特征具有单调性, 即加入新的特征不会使判据减小 \n\n\n1.1 基于类内类间距离的可分性判据\n\n\n基于类内类间距离的判据 \n\n1.2 基于概率分布的可分性判据\n\n基于类内类间距离的可分性判据没有直接考虑样本的分布情况，很难与错误率建立直接的联系。分布密度的交叠程度可用  及  这两个分布密度函数之间的距离  来度量。\n\n概率距离度量函数应满足： (1) ; (2) 两类完全不交叠时  取最大值, 即对所有  有  时 , 则  (3) 当两类分布密度相同时  应为零，即若  ，则  。\n\nBhattacharyya 距离   理论上的错误率 \nChernoff 界限   散度   当两类样本都服从正态分布  当两类协方差矩阵相等时,  。这等于两类均值之间的 Mahalanobis 距离。\n\n概率相关性判据 : 可以用  与  之间的函数距离作为特征对分类贡献的判据  只需把  换成  换成  即可。\n1.3 基于熵的可分性判据\n\nShannon 熵  平方熵 \n\n对特征的所有取值积分，得到基于熵的可分性判据，  越小可分性越好。 \n1.4 统计检验作为可分性判据\n计算在空假设下有多大的概率会得到所观察统计量的取值, 这个概率小于  值，则拒绝空假设，接受备择假设。\n\n 检验 : 假设两类样本服从方差相同的正态分布，在同一特征上的观测。总体样本方差：  统计量：  服从自由度为  的  分布。双边 -检验空假设 : , 备择假设是  。单边  检验空假设 : , 备 择假设是  。  检验属于参数化检验方法,对数据分布有一定假设,必要时需要检验样本分布是否符合该假设。\n\n\n秩和检验: 把两类样本混合在一起,对所有样本按照所考査的特征从小到大排序。在两类样本中分别计算所得排序序号之和  和 , 称作秩和, 考查某一类的秩和是否显著小于或大于另一类的秩和。两类的样本数分别为  和  。 不同样本数目情况下  的空分布是不同的。对于小的样本数,人们预先计算出了  的分布。当  和  较大时(比如都大于10), 可用正态分布  来近似秩和  的空分布, 其中 \n\n过滤方法 指依据一定的统计量来过滤出与所研究的分类问题密切相关的特征,再采用一定的分类方法进行分类。这种方法实现起来比较简单,但是，所采用的过滤准则与后期分类器所采用的准则并不一定有很好的联系。\n2. 特征选择的最优算法\n分枝定界法：从所有候选特征中逐步去掉不被选中的特征。这种方法具有回溯的过程，能够考虑到所有可能的组合：\n\n同一层按照去掉单个特征后的准则函数值来对各个结点排序，如果去掉某个特征后准则函数的损失量最大，则认为这个特征是最不可能被去掉的，把它放在该层的最左侧节点，依次类推。搜索：从最右侧开始向下搜索，当到达叶节点时计算当前达到的准则函数值，记作界限  。算法向上回溯，每回溯一步回收相应节点上舍弃的特征。遇到最近的分枝节点时停止回溯，从这个分枝节点向下搜索左侧最近的一个分枝。当搜索到某一个节点时，准则函数值已经小于界限  ，则说明最优解已不可能在本节点之下的叶节点上，因此停止沿本树枝的搜索，从此节点重新向上回溯。如果搜索到一个新的叶节点， 则更新界限  值，向上回溯。如果回溯过程一直到了根节点，而且根据界限  不能再向下搜索其他树枝，则算法停止，最后一次更新  时取得的特征组合就是特征选择的结果。\n\n3. 特征选择的次优算法\n\n单独最优特征的组合 假设单独作用时性能最优的特征，它们组合起来也是性能最优的。但即使是特征间统计独立时，单独最优特征的组合也不一定是最优的，这还与所采用的特征选择的准则函数有关, 只有当所采用的判据是每个特征上的判据之和或之积时, 这种做法选择出的才是最优的特征。\n顺序前进法\n顺序后退法\n增  减  法 与广义顺序前进法和广义顺序后退法类似。可以每次选择或剔除多个特征，这种做法称作  法。这样与  法相比能够既考虑到特征间的相关性又保持适当的计算量。\n\n4. 特征选择的遗传算法\n把候选对象编码为一条染色体  。把所有特征表述为一条由  个  字符组成的字符串，求一条仅有  个 1 的染 色体,这样的染色体共有  种。优化的目标为适应度函数, 每一条染色体对应一个适应度值  。可用类别可分性判 据作为适应度。针对不同的适应度有不同的选择概率  。\n\n遗传算法： (1) 初始化, , 随机地产生一个包含  条不同染色体的种群 ; (2) 计算当前种群  中每一条染色体的适应度 ; (3) 按选择概率  对种群中的染色体采样,由采样出的染色体繁殖出下一代染色体,组成种群 ; (4) 回到 (2), 直到达到终止条件（常是某染色体的适应度达到设定國值）,输出适应度最大的染色体作为找到的最优解。 在第(3)步产生后代的过程中, 有两个最基本的操作 : 一个是重组也称交叉, 两条染色体配对, 并在某个随机的位 置上以一定的重组概率  进行交叉, 互换部分染色体。另一个是突变, 每条染色体的每一个位置都有一定的概率  发生突变 (从 0 变成 1 或从 1 变成 0 ) 。算法有很多可调节参数 : 种群大小  、选择概率、重组概率、突变概率等。\n\n5. 以分类性能为准则的特征选择方法\n包裹法：把分类器与特征选择集成在一起利用分类器进行特征选择的方法。\n过滤法：利用单独的可分性准则来选择特征再进行分类的方法。\n\n包裹法对分类器的基本要求：一是分类器能处理高维的特征向量；二是分类器能在特征维数很高但样本数有限时仍能得到较好的效果。 支持向量机能较好地满足这两个要求。递归支持向量机和支持向量机递归特征剔除的核心是线性的支持向量机，特征选择与分类采用的是同样的算法步骤。两种算法的不同在于它们评估特征在分类器中贡献的方法不同。\n\n两种算法的基本步骤都是\n\n用当前所有候选特征训练线性支持向量机；\n评估当前所有特征在支持向量机中的相对贡献，按照相对贡献大小排序；\n根据事先确定的递归选择特征的数目选择出的排序在前面的特征（SVM-RFE中描述为剔除排序在后面的特征），用这组特征构成新的候选特征，转（1），直到达到所规定的特征选择数目。\n\n支持向量机的输出函数  R-SVM 定义两类在当前特征上的分离程度为  写成各个特征之和的形式   是当前候选特征的维数, 、分别是两类样本在第  维特征上的均值。每个特征的贡献是  SVM-RFE 采用灵敏度的方法来推导各个特征在 SVM 分类器中的贡献。它把 SVM 输出与正确类别标号  之间平均平方误差作为 SVM 分类的损失函数  考查各个权值对这个损失函数的影响，各个特征的贡献应该用下式衡量。  在很多实际应用中,R-SVM 与 SVM-RFE 从分类上看性能基本相同,但 R-SVM 在选择特征的稳定性和在对末来样本的推 广能力方面有一定优势,尤其是当训练样本中存在较大的噪声和野值时优势更明显。 包裏法递归进行特征选择与分类的做法可推广到 SVM 采用非线性核的情况。 SVM 对偶问题的目标函数  用  表示去掉第  维特征后的样本, 去掉第  个特征对这一目标函数的影响是  可以用这个量作为特征在非线性 SVM 分类器中的贡献，并利用递归方法来进行包裹法特征选择。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"八，非参数学习与集成学习","url":"/2021/10/08/Pattern%20Recognition/%E5%85%AB%EF%BC%8C%E9%9D%9E%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E5%85%B6%E4%BB%96%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95/","content":"\n\n\n\n1. 最近邻决策\n 最近邻法的渐近错误率最坏不会超出两倍的贝叶斯错误率，而最好可能接近贝叶斯错误率。\n\n设  个样本下最近邻法的平均错误率为 , 样本  的最近邻为 , 平均错误率可以写成   为贝叶斯错误率, 存在关系 \n\n2. k 近邻法\n决策规则 则 当  趋于无穷大时，  近邻法错误率渐近贝叶斯错误率。\n\n近邻法只是确定一种决策原则，并不需要利用已知数据事先训练出一个判别函数，但需要始终存储所有的已知样本，并将每一个新样本与所有已知样本进行比较和排序，其计算和存储成本都很大。\n\n3. 近邻法的快速算法\n : 样本集；  : 节点  对应的样本子集；  中的样本数；  : 样本子集  中的样本均值；  : 从  到  的最大距离；\n\n一, 样本集划分\n将  分为  个子集。每个子集再分成  个子集得到树结构。对每个节点计算  和  求得 。\n\n二, 搜索\n（1）置  是当前水平,  是当前节点。\n（2）将当前节点所有直接后继节点放入当前水平一个目录表中, 并对节点计算  。\n（3）如果有 ，则从目录表中去掉  。\n（4）如果目录表没有节点，则  则停止）步骤 3。否则转步骤 5 。\n（5）在目录表中选择使  最小化的最近节点 ，并从目录表中去掉  。如果当前的水平  是最终水平, 则转步骤 6 。否则置  转步骤 2 。\n（6）对当前执行节点  中的每个 ，如果  则计算  。若  置  和  。当前执行节点中所有  被检验之后，转步骤 3\n\n算法结束：输出  的最近邻  和  与  的距离  。\n\n4. 剪辑近邻法\n落在最优分类面错误一侧的训练样本会误导决策。重叠区域内两类已知样本都存在，可能会使分类面的形状变得 非常复杂，去掉重叠区域可使近邻法的决策面更接近最优分类面。\n\n将已知样本集划分为考试集  和训练集  ，用训练集  中的样本对考试集  中的样本进行近邻法分类, 从  中除去被错误分类的样本，剩余样本构成剪辑样本集 ,用  对末来样本进行近邻法分类。\n\n如果在剪辑阶段和分类阶段都用最近邻法，则剪辑近邻法得到的渐近错误率与近邻法错误率的关系是  其中,  是近邻法的错误率，剪辑后错误率减小。如果近邻法的错误率不大，则有  &gt;近邻法的渐近错误率上界是两倍的贝叶斯错误率，，可知剪辑近邻法的渐近错误率近似等于贝叶斯错误率。如果在剪辑阶段用  近邻法，分类阶段用最近邻法，则当 、 但  时，剪辑近邻法的渐近错误率收敛于贝叶斯错误率。同样的方法应用到多类问题上，剪辑近邻法对性能的改善比两类情况下更显著。\n\nMULTIEDIT： 当样本数较多时、为了消除考试集、训练集划分中的偶然性造成的影响。 (1) 划分：把样本集随机划分为  个子集,  。 (2) 分类：用  对  中的样本分类,  。比如, 如果  ，则用  对  分类,用  对  分类，用  对  分类。 (3) 剪辑：从各个子集中去掉在 (2) 中被分错的样本。 (4) 混合：把剩下的样本合在一起, 形成新的样本集  (5) 迭代：用新的样本集  替代原样本集，转(1)。如果在最近的  次迭代中都没有样本被剪掉，则终止迭代，用最后的  作为剪辑后的样本集。 经过多重剪辑之后的近邻法分类面在数据分布的主要区域内已经非常接近贝叶斯分类面。\n\n5. 压缩近邻法\n将样本集  分为  和  两个活动子集。算法开始时，  中只有一个样本，其余样本均在  中。依次考查  中的每一个样本 ，若用  中的样本能够对它正确分类，则该样本保留在 ，否则移到  中, 依次类推，直到没有样本需要搬移为止。最后用  中的样本作为代表样本，对末来样本进行近邻法分类。\n\n可多重剪辑后再使用压缩近邻法\n\n6. 决策树与随机森林\n6.1 ID3 方法\n熵不纯度 (信息熵)  如果特征把  个样本划分成  组，则不纯度减少量为  其中， 。\n\nGini 不纯度度量，也称方差不纯度  误差不纯度   是当前节点上的  个样本中属于第  类的样本数占总样本数的比例。多数情况下,采用不同的不纯度度量对 分类结果的影响不大。\n\n二类分类中基尼指数、熵之半和分类误差率的关系：\n\n\n6.2 C4.5 算法\n采用信息增益率代替信息增益 \n\nC4.5 增加了处理连续数值的功能\n\n6.3 CART算法\n每一个节点上都采用二分法，最后构成二叉树。\n\n分类和回归树算法 (CART) 既可以用于分类问题，也可以用于构造回归树对连续变量进行回归。\n\n\n6.4 过学习与决策树的剪枝\n对以把有限的样本全部正确划分为准则建立的决策规则，控制决策树生成算法的终止条件和对决策树进行剪枝是防止出现overfitting的主要手段。\n\n先剪枝：在决策树生长过程中决定某节点是否需要继续分枝还是直接作为叶节点。\n（1）数据划分法。将数据分成训练样本和测试样本，首先基于训练样本对决策树进行生长，直到在测试样本上的分类错误率达到最小时停止生长。通常采用多次的交叉验证方法以充分利用数据信息。\n（2）阈值法。预先设定一个信息增益阈值，当从某节点往下生长时得到的信息增益小于设定阈值时停止树的生长。但是此阈值往往不容易设定。\n（3）信息增益的统计显著性分析。统计已有节点获得的信息增益其分布，如果继续生长得到的信息增益与该分布相比不显著，则停止树的生长，通常可以用卡方检验来考查这个显著性。\n\n\n\n后剪枝：消除有相同父节点的叶节点后不会导致不纯度的明显增加则以其父节点作为新的叶节点。\n（1）减少分类错误修剪法。通过独立的剪枝集估计剪枝前后分类错误率的改变。\n（2）最小代价与复杂性的折中。对合并分枝后错误率增加与复杂性减少进行折中考虑。\n（3）最小描述长度准则。最简单的树就是最好的树。对决策树进行编码再剪枝得到编码最短的决策树\n\n7. Bootstrap\n随机森林， Bagging、adaboost、随机划分选择法等。\n7.1 随机森林\n建立很多决策树，组成一个决策树的“森林”，通过多棵树投票来进行决策。 1，对样本数据进行自举重采样，得到多个样本集。 2，用每个重采样样本集作为训练样本构造一个决策树。在构造过程中每次从所有候选特征中随机地抽取m个特征，作为当前节点下决策的备选特征，从这些特征中选择最好地划分训练样本的特征。 3，得到所需数目的决策树后，以这些树的输出为投票，以得票最多的类作为随机森林的决策。\n\n这对训练样本和特征进行了采样，保证了所构建的每棵树之间的独立性，使投票结果更无偏。\n\nBoosting 方法：通过一个迭代过程对分类器的输入和输出进行加权处理，而非简单地对多个分类器的输出进行投票决策。\n7.2 AdaBoost 算法\n\n 表示  个弱分类器在样本  上的输出\n1，初始化训练样本  的权重  。\n2，对 , 重复以下过程 :\n\n\n利用  加权后的训练样本构造分类器  ；\n计算样本用  加权后的分类错误率 , 并令  ；\n令 , 并归一化使  。\n\n\n3，对于待分类样本 ，分类器的输出为  。\n\n用加权后的训练样本构造分类器，是指对分类器算法目标函数中各个样本所对应的项进行加权。对于最小平方误差判别，加权后的最小平方误差(MSE)准则函数为：  而对于决策树或一些其他方法，则可以根据每个样本的权值调整重采样的概率，用重采样得到的样本集构造新的弱分类器。在很多情况下，迭代次数（所采用的弱分类器数）较大时，Boosting 方法不会导致严重的过学习问题。\n\n待更新\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"七，统计学习理论概要","url":"/2021/10/07/Pattern%20Recognition/%E4%B8%83%EF%BC%8C%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E6%A6%82%E8%A6%81/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E6%A6%82%E8%A6%81/","content":"\n\n\n\n统计学习理论包括以下四分内容\n\n经验风险最小化学习过程一致的概念及充分必要条件。它回答了在样本趋向于无穷多的情况下，什么样的函数集可以采样经验风险最小化原则进行学习。\n采样经验风险最小化的学习过程，随着样本数目增加的收敛速度有多快。\n如何控制学习过程的收敛速度，有限样本下机器学习的结构风险最小化原则。\n在结构风险最小化原则下设计机器学习算法，包括支持向量机推广能力的理论依据。\n\n1. 机器学习问题的提法\n\n2. 学习过程中的一致性\n\n如果在函数集上期望风险  和经验风险  在样本趋向于无穷多时都收敛于最小可能的期望风险 ，则在这个函数集上采用期望风险最小化原则的学习过程具有一致性。  为保证一致性不是由于函数集中的个别函数导致的，统计学习理论提出了非平凡一致性的概念，即要求上式对函数集的所有子集都成立。 只有非平凡一致性才是实际上有意义的。\n\n传统上要判断一个函数与另一个函数充分接近，常用准则  统计学习理论指明，在上式的意义下经验风险收敛于期望风险，仍无法保证学习过程具有一致性。学习过程一致性需要满足：对于有界的㧹失函数，经验风险最小化学习一致的充分必要条件是，经验风险在如下意义上一致地收敛于真实风险；  可以看到，经验风险最小化学习是否具有一致性，不是取决于平均情况，而是取决于最坏情况。统计学习理论是最坏情况分析。可以解读为，如果一个机器学习方法在最坏猜况下仍能表现良好，则我们对它的推广能力才有信心。显然，这种思路有些偏悲观，所以有人指出统计学习理论对学习机器推广性的判断偏保守。\n3. 函数容量与VC维\n3.1 函数容量\n\n为了研究函数集在经验风险最小化原则下的学习一致性问题和一致性收敛的速度，统计学习理论定义了一系列有关函数集学习侏能的指标，这些指标多是从两类分类函数 (即指示函数)提出的，后又推广到一般函数。现只针对指示函数集进行讨论\n\n统计学习理论用函数集在一组样本集上可能实现的分类方案数来度量函数集的容量，把这个容量的对数在符合同一分布的样本集上的期望称作函数集的熵，而把容量对数在所有可能样本集上的上界定义为函数集的生长函数，记作 。其反映了函数集在所有可能的  个样本上的最大能力或容量。显然  。关于学习过程的一致性，有结论：\n\n【定理】函数集学习过程一致收敛的充分必要条件是, 对任意的样本分布，都有  而且，这时学习过程收敛速度一定是快的，也就是满足  其中， 是常数。\n\n直观上理解，这个定理说明一个采用经验风险最小化原则的学习过程要一致，函数集的能力不能随着样本数无限增长。经验上，人们知道机器学习模型的复杂程度要与样本数目相适应，过于复杂的分类模型容易导致过学习，这个定理给出了其理论依据。\n3.2 VC维\n\n指示函数集的VC维：1968 年，Vapnik 和 Chervonenkis 发现了生长函数的一个重要规律，即一个函数集的生长函数，如果不是一直满足 ， 则一定在样本数增加到某个值  时有：  这个特殊的样本数  被定义为函数集的 VC 维。如果这个值是无穷大，即不论样本数多大，总有 ， 则称函数集的  维为无穷大。直观理解，函数集的 VC维度量了当样本数目增加到多少之后函数集的能力就不会继续跟随样本数等比例增长。因此VC 维有限是学习过程一致性的充分必要条件，而且这时学习过程是比较快的。生长函数和  维为我们选择什么样的函数集来设计学习机器提供了原理上的指导，但这两个度量都不直观，Vapnik 和 Chervonenkis 又为 VC 维给出了下面的直观定义\n\n假如一个有  个样本的样本集能被一个函数集中的函数按照所有可能的  种形式分为两类，则称函数集能把样本数为 的样本集打散。指示函数集的VC维，就是用这个函数集中的函数所能够打散的最大样本集的样本数目。在指示函数集的VC维的基础上，可以定义一般实值函数集的VC维，其基本思想是通过一系列阈值把实值函数集转化为指示函数集，所以VC维的理论不但适用于机器学习中的分类问题，也适合于实函数映射的机器学习。\n4. 推广能力的界与结构风险最小化原则\n【定理】对于两类分类问题，对指示函数集中的所有函数 (当然也包括使经验风险最小的函数)，经验风险和实际风险之间至少以概率  满足如下关系 \n\n当函数集中包含无穷多个元素时  而当函数集中包含有限  个元素时 \n\n这个定理对在有限样本下期望风险的上界给出了度量估计。通常我们面对的学习机器都是包含 50 多个可能的函数，因此这个上界可以写成  或者进一步简写为  其中，  是样本数  的单调减函数，VC 维  的单调增函数。在统计学习理论的文献中，超出部分的上界被称作置信范围。设计一个机器学习模型就意味着选择了一定的函数集，用样本训练的过程是寻求经验风险  最小化。一个学习机器的推广能力不是取决于经验风险最小能有多小，而是在于期望风险与经验风险有多大差距，这个差距越小则推广能力越好。所以反映期望风险与经验风险差距的上界  被称作推广性的界。\n\n当  较小时 (如小于 20)，置信范围  较大，用经验风险最小化取得的最优解可能会有较大的期望风险，即可能推广性差; 如果群本数较多， 较大，则置信范围就会很小，经验风险最小化的最优解就接近实际的最优解。另一方面，对于一个特定的问题，样本数  是固定的，此时学习机器的 VC 维越高 (即复杂性越高)，则置信范围就越大，导致真实风险与经验风险之间可能的差就越大，推广能力就可能越差。人们在实验中认识到对于有限样本应该尽可能选择相对简单的分类器，其背后的原因就在于此。因此, 在设计分类器时，不但要考虑函数集中的函数是否能使经验风险有效减小，还要使函数集的 VC 维尽量小，从而缩小置信范围，以期获得尽可能好的推广能力。\n需要指出的是，如学习理论关键定理一样，推广性的界也是对最坏情况的结论，所给出的界在很多情况下是很松的，尤其当 VC 维比较高时更是如此。而且，这种界往往只在对同一类学习函数进行比较时是有效的，可以指导我们从函数集中选择最优的函数，但在不同函数集之间比较却不一定成立，因为界的松紧程度可能有较大差别。\n\n既然学习的目标是最小化式期望风险上界，有没有可能直接根据这个原则来设计学习机器，而不是进行上述的试错性设计?  维是函数集的性质而并非单个函数的性质，因此式右边的两项并无法直接通过优化算法来最小化。统计学习理论提出了一种一般性的策略来解决这个问题，做法是\n\n首先把函数集  分解为一个函数子集序列 (或叫子集结构)  使各个子集能够按照置信范围  的大小排列，也就是按照  维的大小悱列，即  在划分了这样的函数子集结构后，学习的目标就变成在函数集中同时进行子集的选择和子集中最优函数的选择。选捀最小经验风险与置信范围之和最小的子集，就可以达到期望风险的最小，这个子集中使经验风险最小的函数就是要求的最优函数。这种思想称作结构风险最小化(structural risk minimization)，简称 SRM 原则。\n\n一个合理的函数子集结构应满足两个基本条件\n\n每个子集的 VC 维是有限的且满足式 (1) 的关系；\n每个子集中的函数对应的损失函数或是有界的非负函数，或是无界但能量有限的函数。这样的函数子集结构被称作容许结构。\n\n统计学习理论的一个基本结论是，在有限样本下，设计和训练学习机器不应该采用经验风险最小化原则，而应该采用结构风险最小化原则。对于多层感知器神经网络，不同的网络结构对应着不同的VC维，因此，可以按隐节点数目把多层感知器实现的函数集划分为若干个子集。结构风险最小化机器学习就是要在这一系列结构中选择能使经验风险和置信范围之和最小的函数子集和其中的函数。另外，神经网络反向传播算法中对权值加约束以改善网络学习性能的做法，也等价于由权值正则化项引入的函数子集结构，通过正则化目标函数实现结构风险最小化。（例如：通过正则化使部分权重为零）\n5. 支持向量机的理论分析\n根据结构风险最小化原则，学习机器需要同时最小化经验风险和取得经验风险最小的函数子集的置信范围。支持向量机从线性可分这样的最简单情况人手来实现这个日标。\n对于样本集线性可分的情况，存在很多线性判别函数能够得到零经验风险, 这种情况下，什么样的判别函数具有最好的推广能力，取决于判别函数来自什么样的函数子集。支持向量机用分类间隔对函数集进行子集的划分，其依据是下面的关于分类间隔与 VC 维关系的理论。\n\n定义  间隔超平面的概念:  维空间中权值归一化的超平面  如果它把样本用以下的形式分开  则称为  间隔超平面  。具有间隔  的超平面构成函数子集，它的  维有下面的界\n\n\n【定理】设样本集中在空间中属于一个半径为  的超球范围内， 间隔超平面的 VC 维  满足  其中 [] 为取整。前面我们看到，  维空间中不加约束的线性函数集的  维是 ，而对于间隔为  的线性函数子集来说，如果这个间隔足够大，则函数子集的  维将主要由间隔决定，有可能小于甚至远小于空间维数。\n\n把这个定理转述为支持向量机中采用的规范化超平面的形式，结论是：若  维空间中规范化分类超平面权值的模为 ，则函数子集的  维满足  其中  是空间中包含全部训练样本的最小超球的半径。所以，支持向量机中最大化分类间隔，就是通过最小化  以实现最小化函数子集 VC 维的上界。在高维空间中，尤其是经过核函数变换后的高维空间中，空间维数很大甚至是无穷大，但通过控制分类间隔，可以有效控制函数子集的  维，从而保证在函数子集中求得经验风险最小的解具有好的推广能力。\n\n【定理】如果包含  个样本的训练集被最大间隔超平面分开,那么超平面在末来独立 测试集上测试错误率的期望有如下的界  其中,  是支持向量个数,  是包含数据的超球半径,  是分类间隔， 是空间的维数。\n\n错误率上界则不再由原空间的维数决定，可以大大降低。同时也看到，支持向量机训练后得到的支持向量数目在全部训练样本中所占的比例，也体现了学习后的机器的推广能力，比例越小则期望的测试错误率上界越小。 正是由于这些理论性质，保证了在高维小样本问题上支持向量机表现出色，而且使它在引入核函数进行等效的升维后仍然能保证良好的推广能力。\n\n统计学习理论也存在其局限性。一方面，对于大部分常见非线性机器学习模型，对应的函数集的 VC 维难以估计，这使得很多定量的结论难以直接用于指导其他机器学习模型和算法的设计。另一方面，在深度学习中，很多场景下面临的训练数据虽然有限，但已经超出了统计学习理论所主要针对的小样本情形，导致在VC维基础上得到的各种界都比较松弛。深度学习采用了多种复杂的深度神经网络模型来获得更高的表示能力，一定意义上是增加函数集的能力，但同时在模型设计和算法设计中又采用多种技巧和策略来降低自由参数的数目、缩小参数的取值空间，这与结构风险最小化中一方面追求经验风险最小、另一方面对函数子集进行约束的原理是一致的，只是结构风险最小化理论的定量结果尚无法直接用于解释和指导深度学习模型和算法。如何与深度学习相结合，拓展统计学习理论，发展新的关于学习机器推广性的理论，是机器学习和模式识别未来研究的重要方向。\n\n6. 不适定问题和正则化方法简介\n6.1 不适定问题\n有些研究者把机器学习问题抽象为一个求解反演问题（inverse problem）的任务来进行数学上的研究。例如，假设研究对象具有某种我们感兴趣但不易观测的特性乙，但可以观测到它经过了一定映射后的另外的特性，例如最简单情况下它和z有如下的关系  如果方程的解存在、唯一且稳定，这样的问题就称作适定问题。稳定是指方程的解对参数或输入数据的依赖是连续的，微小变化带来的影响也是微小的。但实际上，例如对问题，即使解存在且唯一，问题也不一定是适定的。因为我们得到的观测通常是带有噪声的，逆算子经常是不连续的，导致有噪声的观测有微小变化时， 可能会有很大变化。这时问题就是不适定问题。在很多情况下，求解算子方程  的问题是不适定的。即使方程存在唯一解，方程右边的微小扰动  会带来解的很大变化。在无法得到准确的观测  的情况下，对带有噪声的观测  ，常最小化下面的目标泛函来得到对解  的好的估计，即使扰动  趋向于零也如此。 \n6.2 正则化方法\n将支持向量机用正则化框架表示  在这里，目标函数的第一项对应着支持向量机原问题中用松弛因子表示的分类错误惩罚，第二项对应着支持向量机原问题中最大化间隔的项，两项之间折中的系数变成了这里的正则化系数  。\n\n 范数就是对参数向量中非零参数个数的计数，把它放到目标函数中进行最小化，就是在要求经验风险最小化的同时希望函数中非零参数的个数尽可能少，实现在减小训练误差的同时实现特征选择的功能，也就是常说的学习对样本特征的稀疏表示，这也是所谓“压缩感知”的基本思想，但范数的优化计算很难。\n 范数即参数向量各元素的绝对值之和也可以用来作为对非零参数个数的一种惩罚，所以比较广泛地被果用。\n 范数由于采用了平方和，在计算上有很大的方便性，L2范数能够有效地防止参数变得过大，可以较有效地避免过拟合，但平方惩罚对于强制小的参数变成 0 的作用不大。采用L2范数的线性回归方法也称作铃回归（ridgeregression）。支持向量机中的最大化分类间隔就等价于采用  范数作为正则化项，但与  正则化回归方法不同，支持向量机中对错误的度量不是采用平方误差函数，而是对分类错误采用了线性的惩罚。\n\n弹性网方法采用了范数与范数相结合的方式，它既发挥  范数的作用防止参数值过大带来的过学习风险，也利用范数有效减少非零参数个数，两个目标通过人为确定的常数来进行权衡。弹性网 (亦称混合正则化)： \n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"六，典型非线性分类器","url":"/2021/10/06/Pattern%20Recognition/%E5%85%AD%EF%BC%8C%E5%85%B8%E5%9E%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","content":"\n\n\n\n1. 分段线性判别函数\n把各类划分成适当的子类，在两类的多个子类之间构建线性判别函数，再合并成分段线性判别函数。\n分段线性距离分类器\n\n最小距离分类器\n当两类的类条件概率密度为正态分布，两类先验概率相等，而且各维特征独立且方差相等时，最小错误率贝叶斯决策就是直观的最小距离分类器。在很多情况下，只要每一类数据的分布是单峰的、在各维上的分布基本对称且各类先验概率基本相同，则最小距离分类器都不失为一种简单有效的分类方法。\n分段线性距离分类器\n\n ​ 决策规则 则 * 一般的分段线性判别函数\n ​ 判别函数  ​ 决策规则 ，则决策 ​ 两个相邻的类的决策面方程 \n\n子类的划分：\n第一种情况：视情况人工划分；或者用非监督学习方法对同一类的样本聚类，得到子类的划分。\n\n第二种情况： 已知各类的子类数目，每个类都有一定数量训练样本，但是不知道子类的划分。\n\n任意给定各类各子类的权值 。\n在时刻 ，考虑某个训练样本 ，找出  类的各子类中判别函数最大的子类，记为 \n\n\n​ 考査当前权值对样本  的分类情㑆 ​ (1) 若 ， ​ 则   ； ​ (2) 若对某个 ，存在子类  使得 ； ​ 则选取  中最大的子类，不妨记作  类的第  个子类，做以下权值修正  ​ 其余权值不变。\n\n，考查下一个样本，回到第(2)步。迭代至收敛。\n\n\n第三种情况：子类数目无法事先确定。可以采用分类树的思想来分级划分子类和设计分段线性判别函数。\n\n\n\n\n2. 二次判别函数\n一般的正态分布情况下，贝叶斯决策面是二次函数。二次判别函数的一般形式是 \n\n其中,  是  维实对称矩阵,  为  维向量；判别函数中包含  个参数。\n\n==参数化方法==（对函数形式做一个假设。非参数方法：支持向量机和kNN）求判别函数。这时有二次判别函数： \n\n 是一个阈值项，它受协方差矩阵和先验概率的影响。样本的均值和方差可以用估计 \n\n决策面方程  决策规则 若则 其中，可以通过调整两类的阈值  和  来调整两类错误率情况。\n\n另一种情况： 近似正态分布， 比较均匀地分布在其附近。可用二次判别函数：  决策规则 若则 其中：可用  来调整两类错误率情况。\n\n3. 多层感知器神经网络\n阀值逻辑单元  用  和 表示要区分的两类，  代表正确分类，则有权值迭代：  对任意复杂形状的分类区域，可用多个神经元组成一定的层次结构来实现非线性分类面。 \n\n采用反向传播算法的多层感知器，用 Sigmoid 作为传递函数  把常数项  作为一个固定输人 1 的权值合并到加权求和项 \n\n单位超立方体  内的任意连续函数 ，都可以通过选择适当的  和  表示成  即：多层感知器神经网络能够实现任意复杂的函数映射。对任意一个从  到  的映射，都存在一个适当结构的 3 层前馈神经网络能够以任意的精度来逼近它。\n\nBP 算法：在给定多层感知器结构的情况下训练其权值的反向传播算法 输入向量 ，输入层记 ，第一个隐层记 ，以此类推。第  层第  个神经元的输出记作 ，输出向量 ，第  个隐层的神经元个数为  第  层的权值都用  表示， 表示第  层的节点  连接到第  层的节点  的权值。用  表示在第  步迭代时权值  的取值。\n\n\n确定神经网络的结构，用小随机数进行权值初始化，置  。\n从训练集中得到一个训练样本 ，记它期望的输出是  。\n计算在  输入下当前神经网络的实际输出  其中,  是 Sigmoid 函数  平方误差 \n 层权值修正  对输出层，计算梯度项  \n\n\n对中间层，计算梯度项  \n\n\n更新全部权值后对所有训练样本重新计算输出，计算更新后的网络输出与期望输出的误差。检查算法终止条件，如果条件已达到则停止，否则 , 返回 (2)。\n\n\n此外，这里给出的 BP 算法是针对神经元节点为 Sigmoid 函数的。这时  的梯度函数是：  如果采用其他形式的 Sigmoid 函数或其他函数，需要根据其梯度函数修正算法第 (4) 步。\n\n收敛结果有时受初始权值的影响很大，算法不能保证收敛到全局最优点。对于多层感知器神经网络，各个初始权值不能为 0，也不能都相同，而是应该采用较小的随机数。如果算法很难收敛，可以尝试改变初值重新试算。如果步长太大，收敛速度可能一开始会较快，但可能会容易导致算法出现振荡而不能收剑或收敛很慢；如果步长太小，则权值调整可能会非常慢，导致算法收敛太慢，而且一旦陷于局部极小点就容易停在那里。试算过程中观察不同步长下得到的误差收敛曲线有助于找到针对特定问题的较合理的步长。为了兼顾训练过程和训练的精度，有时采用变步长的办法。 为使算法有更好的收敛性能，可在权值更新过程中引人 “记忆项\"或 “惯性项”，把权值更新项改为 \n\n4. 支持向量机\n定义核函数  对特征  进行非线性变换 ，新特征空间决策函数  系数  是下列优化问题的解 \n 通过支持向量求得 \n\nMercer 条件 一个对称函数 ，它是某个特征空间中的内积运算的充要条件是：对于任意的  且 , 有  选择一个满足 Mercer 条件的核函数，就可以构建非线性的支持向量机，且不用设计变换  。\n\n 是定义在空间  上的对称函数，且对任意训练数据  和任意实系数 , 都有  则 是一个正定核。\n\n对于正定核，肯定存在一个从  空间到内积空间  的变换 ，使得  。\n\n\n一, 多项式核函数  采用这种核函数的支持向量机实现的是  阶的多项式判别函数。 二, 径向基(RBF)核函数  采用它的支持向量机实现与径向基网络形式相同的决策函数。 三, Sigmoid 函数  采用这种核函数的支持向量机在  和  满足一定条件的情况下等价于包含一个隐层的多层感知器神经网络。\n\n核函数及其参数的选择 一般先尝试简单的选择，比如线性核，当结果不满意时才考虑非线性核；如果选择 RBF 核函数，则先选用宽度比较大的核，宽度越大越接近线性，然后再尝试减小宽度，增加非线性程度。\n\n核函数与相似性度量 通过非线性变换将输入空间变换到一个高维空间，然后在这个新空间中求最优分类面，这种非线性变换是通过定义适当的内积核函数实现的。支持向量机求得的分类函数，形式上类似于一个神经网络，其输出是若干中间层节点的线性组合，而每一个中间层节点对应于输人样本与一个支持向量的内积，因此早期也被叫做支持向量网络。 决策过程可看作一种相似性比较的过程。输入样本与一系列模板样本（支持向量）进行相似性比较，采用的相似性度量是核函数。样本与各支持向量比较后的得分进行加权后求和，权值就是训练时得到的各支持向量的系数  与类别标号的乘积。最后根据和值大小进行决策。采用不同的核函数, 可以看作是选择不同的相似性度量, 线性支持向量机就是采用欧式空间中的内积作为相似性度量。根据这一思想，除了可以选择常用的核函数形式外，还可以根据相关领域的专门知识定义一些特殊的核函数。 维数与推广能力 支持向量机通过采用核函数作为内积，间接地实现了对特征的非线性变换，避开了在高维空间进行计算。支持向量机通过最大化分类间隔来控制函数集的 VC 维，使得在高维空间里的函数集的 VC 维可以大大低于空间的维数，从而保证好的推广能力。支持向量机需要求解的是关于  的二次优化函数。这是一个有线性约束的二次优化问题，==有唯一的最优解==，这与多层感知器神经网络相比是一个优势。而且，问题的计算复杂度是由样本数目决定的，计算复杂度不取决于样本的特征维数和所采用的核函数形式。\n\n多类支持向量机\n\n支持向量机用正则化(regularization) 的框架重新表述如下 设有训练样本集  是样本的特征,  是样本的类别标号。待求函数  是由核函数  定义的可再生希尔伯特空间。决策规则是  。 支持向量机求解的是这样的 ，它最小化以下的目标函数  如果样本的类别标号  和待求的函数  都从标量变为向量，则上述表述就可以用于多类分类问题。 对于  类问题， 是一个  维向量，如果样本  属于第  类，则  的第  个分量为 1 , 其余分量为 ，这样,  的各分量值总和为 0 。如  则 若若若 待求函数为 ，它的各分量之和须为 0，即 ，且每一个分量都定义在核函数可再生希尔伯特空间中 \n\n\n（one-versus-the-rest）\n\n多类支持向量机就是求 ，使下列目标函数达到最小  其中,  是损失矩阵  与样本类别  相对的行向量。损失矩阵  是一个  维的矩阵，例如：  得到函数  后，类别决策为  各分量中取值最大的分量对应的类别。 \n\n用于回归的支持向量机  \n目标函数  对偶问题（PRML） \n\n回归函数权值  回归函数  核函数变换 \n\n系数  是(1)的解 ### 5. 核 Fisher 判别\n对样本  进行非线性变换  \n\n这里的  是  空间里的权值向量,  和  分别是  空间里的类间离散度矩阵和类内离散度矩阵   是  空间里各类样本的均值 \n\n根据表示定理，上述问题的任何解  都处在  空间中所有训练样本张成的子空间中,即 \n\n因此 \n\n变换空间里的目标函数 \n\n其中 \n\n最大化目标函数的解是  的最大本征值对应的本征向量，且最优解的方向是  原空间任意一个样本到 Fisher 判别的方向上的投影，即只需要计算  通常，上述问题可能是病态的，因为矩阵  可能非正定,这是由于变换后样本维数升高导致的。一种补偿办法是，引入 一个新的矩阵  来代替原来的矩阵 (  是一个常数)，使矩阵正定。这样做同时还实现了对  的正则化控制，类似于支持向量机中控制间隔的作用。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"五，线性学习机器与线性分类器","url":"/2021/10/05/Pattern%20Recognition/%E4%BA%94%EF%BC%8C%E7%BA%BF%E6%80%A7%E5%AD%A6%E4%B9%A0%E6%9C%BA%E5%99%A8%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/","content":"\n\n\n\n省去了对概率密度函数的估计，直接基于样本直接进行分类器设计，其需要确定三个基本要素：\n\n分类器的类型，也就是从什么样的判别函数类 (函数集) 中去求解；\n分类器设计的准则，根据样本从函数集中选择在该准则下最优的函数（通常是确定某些待定参数）；\n设计算法利用样本数据捜索到最优的函数参数。\n\n1. 线性回归\n目标函数  使目标函数最小化的参数  应该满足  即  因此，当矩阵  可逆时，最优参数的解为  这就是经典的 “最小二乘法” 线性回归，其中的矩阵  也被称作  的伪逆 (Pseudo-inverse) 矩阵, 记作 。 线性回归给出了在最小平方误差意义下对解释变量与响应变量之间线性关系的最好的估计。此外线性回归也可以通过迭代的方法求解，并且可以用来解决分类问题。\n2. 线性判别函数\n\n线性判别函数一般表达式   是个常数，称为阈值权。对于两类问题的线性分类器可以采用下述决策规则： 则则快策则快策可将任意分到某一类或拒绝  定义了一个决策面，判别函数  可以看成是特征空间中某点  到超平面的距离的一种代数度量  得到从原点到超平面  的距离 \n\n注：超平面的正方向由权向量  指向确定，它的位置由阈值权  确定。判别函数  正比于  点到超平面的距离。判别式  大于零则  归类到  所指向的类别。\n\n3. Fisher线性判别分析（LDA）\n把所有样本都投影到一个方向上，然后在一维空间中确定一个分类的阀值，并使投影后两类相隔尽可能远，而同时每一类内部的样本又尽可能聚集。\n\n寻找一个投影方向  ，投影后的样本变成  投影后的一维空间，类均值向量 𝕪𝕩 类间离散度  类内离散度 𝕪 用原样本空间类间离散度矩阵和类内离散度矩阵表示 \n\n 是类间协方差矩阵   是类内协方差矩阵 𝕩𝕩\n\n这一表达式被称作广义 Rayleigh 商。对  幅值的调节并不会影响  的方向，因此可把优化问题转化为：  拉格朗日函数  极值处  可得极值解  满足  假定  是非奇异的(样本数大于维数时通常是非奇异的)，可得  是矩阵  的本征向量。  代入  有 \n可知  的方向由  决定，因此有最优投影方向 \n\nFDA投影方向也直接求微分求得：  即  由于  是标量，非奇异的条件下解满足 \n\n当样本是正态分布且两类协方差矩阵相同时最优贝叶斯分类器是线性函数   如果用  估计 , 用  估计 ，则 FDA 所得的方向实际就是最优贝叶斯决策的方向，\n\n其中 \n\n可以用  来作为分类阀值。  决策规则为 ，则\n\n注意⚠️：这里阀值的尺度确定了， 的尺度也就确定了。\n\n\n如果先验概率相同，则可以采用阈值:  或者  其中， 是所有样本投影后的均值。\n\n注意到  是⼀组随机变量的和，因此根据中⼼极限定理，我们可以做出⾼斯分布的假设。在样本不是正态分布时，这种投影方向和阈值并不能保证是最优的，但通常仍可以取得较好的分类结果。在先验概率不同时，分界点向先验概率小的一侧偏移。FDA本身并不对样本的分布作任何假设，但在很多情况下，当样本维数比较高且样本数也比较多时，投影到一维空间后样本接近正态分布。这时可以在一维空间中用样本拟合正态分布，用得到的参数来确定分类阈值。\n\n4. 感知器(perceptron)\nFDA通过先确定方向再确定阀值的方法设计分类器。感知器可直接得到完整的线性判别函数。 定义增广的样本向量  增广的权向量  则线性判别函数变为 \n决策规则 则 定义规范化增广样本向量  若若 决策规则变为  &gt;如考虑噪声，可引入余量  &gt;\n\n定义惩罚项  解向量   梯度下降方法迭代求解  即  即在每一步迭代把错分样本按照某个系数加到权向量上。\n\n通常每次只修正一个样本的固定增量法效率更高。即随机梯度下降\n\n任意选择初始的权向量 ，置 ；\n考查样本  ，若 , 则 ，否则继续；\n考査另一个样本，重复 (2)，直至对所有样本都有 ，即  。\n\n\n如果考虑余量 ，只需将错分判断条件变成  即可。对于线性可分的样本集，采用这种梯度下降的迭代算法，经过有限次修正后一定会收敛到一个解向量  。图示这种单步的固定增量法采用的修正步长  。为了减少迭代步数可以使用可变的步长，比如绝对修正法采用步长  样本不可分但多数可分时，让步长按照一定的启发式规则逐渐缩小，可以强制算法收敛。\n5. 最小平方误差判别\n求解一个  使不满足不等式的样本尽可能少，为简化计算引入待定常数   矩阵形式  其中   是增广的样本向量的维数，  。通常情况下 ，方程属于矛盾方程组，无法求得精确解，方程组的误差为 。可求方程组的最小平方误差解，即  最小平方误差( MSE )准则函数 \n\n伪逆法求解：  在极值处对  的梯度为零  其中  是长方矩阵  的伪逆。可得 \n\n\n梯度下降法求解： (1) 任意选择初始的权向量 , 置 ; (2) 按照梯度下降的方向迭代更新权向量  直到满足  或者  时为止，其中  是事先确定的误差灵敏度。参照感知器算法中的单步修正法，对最小平方误差准则，也可以采用单样本修正法来调整权向量  其中,  是使得  的样本。这种算法称作 Widrow-Hoff 算法，也称LMS 算法\n\n选择不同的  会带来不同的结果\n\n同一类样本的  选相同的值，解等价于 Fisher 线性判别的解，把样本和权向量都还原成增广以前的形式后有:\n\n\n\n第一类样本对应的  都是 ，第二类样本对应的  都是  时，阀值  为样本均值在所得一维判别函数方向的投影 \n\n\n\n对所有样本都取 ，那么当  时，MSE 算法的解是贝叶斯判别函数\n\n\n的最小平方误差逼近。即以下面均方逼近误差在  时取得最小值。 \n6. logistic 回归（待更新）\n\n多元线性回归问题   为残差。特征  可以是连续变量，也可以是离散变量。求解线性回归常用最小二乘法，即求使各样本残差的平方和达到最小的系数  。这实际是在变量  服从正态分布假设下的最大似然估计。\n\nlogistic 函数 \n\n几率(odds)  对数几率 (log odds) \n\n 的 logit 函数  样本属于  类的概率是  决策函数 若则\n\n最大似然法：设共有  个独立的训练样本，把两类的输出分别编码为  和    为样本的总体概率密度函数。  个独立样本出现的似然函数为  最大化上式等价于最大化  取对数 \n\n\n在似然函数满足连续、可微的条件下，最大似然估计量就是以下微分方程的解  即  如果  是  维特征, 则  代入  得到一组关于  的非线性方程。其一般无法解析求解，可以采用迭代的策略求解。\n\n\n定义目标函数为似然函数的负对数，，把两类的输出分别编码为  和 有  梯度下降法求解\n（1）记时刻为 ，初始化化参数  。\n（2）目标函数的梯度方向  按步长  更新下一时刻参数  检查是否达到终止条件, 如末达到，令 , 重新进行 (2)。\n（3）算法停止，输出得到的参数  。 其中终止条件可以是似然函数的梯度已经小于某个预设值，训练过程不再有显著更新，或者是迭代达到预设的上限，等等。\n\n7. 最优分类超平面与线性支持向量机\n最优超平面能将训练样本没有错误地分开，并且分类间隔最大。最优超平面定义的分类决策函数：  \n确定  的尺度，即  优化问题  对每个样本引入一个拉格朗日系数 。（KKT条件）优化问题变为  最优解在  的鞍点上取得。由拉格朗日对偶性，上式等价于： \n\n对求偏导得最优解处  且 \n\n代入拉格朗日泛函 (1) 可得最优超平面的对偶问题  这是一个对  的凸二次优化问题，通过对偶问题的解  可以求出原问题的解\n\n维数较大时常采用SMO算法\n\n 即最优超平面的权值向量等于训练样本以一定的系数加权后进行线性组合。根据 KKT 条件，拉格朗日泛函的鞍点处满足：  实际只有  的样本参与加权求和，这些样本被称作支持向量，对于这些支持向量来说有  理论上  可以根据任何一个支持向量求得。在实际的数值计算中，通常采用所有  非零的样本用式求解  后再取平均。对比感知器算法，也可以把最优超平面等价看作是在限制权值尺度的条件下求余量的最大化。\n\nKKT 条件  \n拉格朗日得到的是一个必要条件\n\n\n期望风险和经验风险之间满足   称作置信范围，它与样本数  成反比，与参数  成正比。参数  称作 VC 维，对于规范化的分类超平面， 时, VC 维有上界：   是样本特征空间中能包含所有训练样本的最小超球体的半径，  是样本特征的维数。最大化分类间隔也就等价于最小化  ，因此，支持向量机中最大分类间隔的准则，是为了通过控制算法的 VC 维实现最好的推广能力。在这个意义下，所得的分类超平面是最优的。\n\n线性不可分时引入松弛变量   增加对错误的惩罚项，定义广义最优分类面的目标函数 \n\n正则化表述  其中  称为 “结构风险” (structural risk)，用于描述模型  的某些性质；第二项  称为 “经验风险” (empirical risk)，用于描述模型与训练数据的契合程度， 称为正则化项，  则称为正则化常数。  范数 (norm) 是常用的正则化项，其中  范数  倾向于  的分量取值 尽量均衡, 即非零分量个数尽量稠密, 而  范数  和  范数  则倾向 于  的分量尽量稀疏，即非零分量个数尽量少。\n\n\n转化为以下拉格朗日泛函的鞍点问题，其中,  是拉格朗日乘子  对偶问题  其中 \n\n令  对  的偏导为零可得 \n\n广义最优分类面的判别函数  根据KKT条件, 鞍点满足以下两套条件 \n\n多数  为 0 ，只有 的样本才会使  。其分为两种，一种是分类正确但处在分类边界面上的样本，它们  ；另一种是分类错误的样本，它们  。通过  的样本可求得  。这两部分  的样本都是支持向量，有时也把   的支持向量叫做边界问量。由于广义最优分类面可以兼容线性可分情况下的最优分类面，所以人们通常采用的支持向量机都是考虑广义最优分类面的形式。目标函数式中只有  的二次项和一次项，这是一个对  在等式和不等式约束下的二次优化问题，具有唯一的极值点。\n\n8. 多类线性分类器\n8.1 多个两类分类器的组合\n\n“一对多\" ，  个两类分类器就可以实现  个类的分类。会有训练样本不均衡的问题。一些区域内的分类也可能会出现歧义。\n“逐对\"分类， 个类别, 共需要  个两类分类器。不会出现两类样本数过于不均衡的问题, 决策歧义的区域通常要比“一对多\"分类器小。\n如果对所研究的类别有较好的认识, 能够根据类别间的内在关系把它们分级合并成多个两类分类问题, 则可以用二叉树来构建多个两类分类器。\n\n\n很多分类器在最后的分类决策前得到的是一个连续的量,分类是对这个量用某个阀值划分的结果, 比如所有线性分类器都是最后转化为一个线性判别函数  与某一阈值(通常是 0) 比较的问 题。SVM 也是这样一种分类器。在很多线性分类器中,一个正确分类的样本,如果它离分类面越远,则往往对它的类别判断就更确定,因此可以把分类器的输出值看作是对样本属于某一类别的一种打分。利用这种分类器, 可以用  个一对多的两类分类器来构造多类分类系统， 即每个类别对应一个分类器，其输出是对样本是否属于  类给出一个判断。在多类决策时，如果只有一个两类分类器给出了大于阈值的输出, 而其余分类器输出均小于阈值,则把这个样本分到该类。更进一步,如果各个分类器的输出是可比的,那么可以在决策时直接比 较各个分类器的输出, 把样本赋予输出值最大的分类器所对应的类别。(但是需要注意, 对很多分类器来说,如果它们是分别训练的，其输出值之间并不一定能保证可比性)\n\n8.2 多类线性判别函数\n对  类设计  个判别函数  决策规则 若则 增广形式  其中： 为增广权向量\n多类线性机器不会出现有决策歧义的区域。考虑多类线性可分情况，可用与感知器算法类似的单样本修正法来求解线性机器。\n\n\n任意选捀初始的权向量 , 置  。\n考查某个样本 ，若 ，则所有权向量不变； 若存在某个类 ，使  ，则对各类的权值进行如下的修正   是步长，必要时可以随着  而改变。\n如果所有样本都分类正确，则停止; 否则考査另一个样本，重复(2)。\n\n\n如果样本集线性可分，则该算法可以在有限步内收敛于一组解向量。与感知器算法一样，当样本不是线性可分时，这种逐步修正法不能收敛，人们可以对算法作适当的调整而使算法能够停止在一个可以接受的解上，比如通过逐渐减小步长而强制使算法收敛。 同样，也可以像在感知器算法中那样引人余量，即把  变为 \n8.3 多类logistic 回归\n前文讨论的罗杰斯特回归问题，所考虑的实际上是样本属于所关心的类和不属于所关心的类的问題。这个思路可以推广到多类情况，对每一类考虑样本是否属于它。在这个视角下，罗杰斯特函数的分子可以看作是对样本属于该类的可能性的度量，而分母的作用则是把这个可能性归一化为概率。 \n把这个思路推广到多类情况我们可以把模型设为样本属于每一类  都与一个参数为  的指数判别函数成正比  归一化得  这个归一化指数函数称作软最大(Softmax) 函数，就是对样本的多类罗杰斯特回归。可以采用与两类罗杰斯特回归类似的思路用最大似然法求解。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"四，隐马尔可夫模型与贝叶斯网络","url":"/2021/10/04/Pattern%20Recognition/%E5%9B%9B%EF%BC%8C%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/","content":"\n\n\n\n1. 贝叶斯网络的基本概念\n贝叶斯网络，又称信念网络 (belief network)，它是一种用有向无环图（DAG）表示的概率模型。贝叶斯网络是概率论与图论的结合，它提供了一种表示联合概率分布的紧凑方法，在机器学习和概率推断 (probability inference) 中有重要应用。概率推断问题可以抽象为在一个联合概率分布模型中，已知部分随机变量的取值 ，希望推断末知变量取值  的概率分布。\n2. 隐马尔可夫模型\n隐马尔可夫模型由初始状态概率向量  、状态转移概率矩阵  和观测概率矩阵  决定。  和  决定不可观测的状态序列， 决定观测序列。因此，隐马尔可夫模型  可以用三元符号表示，即   称为隐马尔可夫模型的三要素。从定义可知，隐马尔可夫模型作了两个基本假设\n（1）齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻  的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关：  （2）观测独立性假设,，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态, 与其他观测及状态无关: \n\n隐马尔可夫模型有 3 个基本问题\n\n概率计算问题：已知模型  和观测序列 ，计算在模型  下观测序列  出现的概率  。\n解码问题：已知模型  和观测序列 ，求使条件概率 最大的状态序列 \n学习问题：已知观测序列 ，估计模型  参数，使得在该模型下观测序列概率  最大。即用极大似然估计的方法估计参数。\n\n\n2.2 HMM评估问题\n本节介绍计算观测序列概率  的前向（forward）与后向（backward）算法。 先介绍概念上可行但计算上不可行的直接计算法。\n2.2.1 直接计算法\n列举所有可能的长度为  的状态序列 ，求各个状态序列  与观测序列 的联合概率 ，然后对所有可能的状态序列求和，得到  。 状态序列为  的概率  对固定的状态序列 ，观测序列  的概率   和  同时出现的联合概率  然后，对所有可能的状态序列  求和，得到观测序列  的概率 ，即  但上式计算量很大，是  阶的，算法不可行。\n2.2.2 前向算法\n前向概率记作 \n\n观测序列概率的前向算法\n输入：隐马尔可夫模型 ，观测序列 ； 输出：观测序列概率  。\n（1）初值：  （2）递推：对 ,  （3）终止： \n\n前向算法可减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果，避免重复计算。这样利用前向概率计算  的计算量是  阶的，而不是直接计算的  阶。\n2.2.3 后向算法\n后向概率记作 \n\n观测序列概率的后向算法\n输入: 隐马尔可夫模型 , 观测序列 ; 输出: 观测序列概率  。\n（1）  （2） 对   （3） \n\n利用前向概率和后向概率的定义可以将观测序列概率  统一写成： \n\n一些概率与期望值的计算\n利用前向概率和后向概率，可以得到关于单个状态和两个状态概率的计算公式。\n\n给定模型  和观测 ，在时刻  处于状态  的概率。记\n\n\n\n给定模型  和观测 ，在时刻  处于状态  且在时刻  处于状态  的概率。记\n\n\n将  和  对各个时刻  求和，可以得到一些有用的期望值：\n（1）在观测  下状态  出现的期望值  （2）在观测  下由状态  转移的期望值  （3）在观测  下由状态  转移到状态  的期望值 \n\n2.3 HMM解码问题\n2.3.1 近似算法\n给定隐马尔可夫模型  和观测序列 ，在时刻  处于状态  的概率  是  在每一时刻  最有可能的状态  是  从而得到状态序列  。近似算法计算简单，但不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列可能有实际不发生的部分。事实上，上述方法得到的状态序列中有可能存在转移概率为 0 的相邻状态，即对某些  。尽管 如此，近似算法仍然是有用的。\n2.3.2 维特比算法\n输入: 模型  和观测 ; 输出: 最优路径  。\n（1）初始化  （2）递推。对   （3）终止  （4）最优路径回溯。对   求得最优路径  。\n2.4 HMM学习问题\n隐马尔可夫模型的学习, 根据训练数据是包括观测序列和对应的状态序列还是只 有观测序列, 可以分别由监督学习与无监督学习实现。本节首先介绍监督学习算法, 而后介绍无监督学习算法—Baum-Welch 算法 (也就是 EM算法)。\n2.4.1 监督学习方法\n假设已给训练数据包含  个长度相同的观测序列和对应的状态序列 , ，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。 具体方法如下。\n\n转移概率  的估计\n\n 2. 观测概率  的估计  3. 初始状态概率  的估计  为  个样本中初始状态为  的频率。 由于监督学习需要使用标注的训练数据，而人工标注训练数据往往代价很高，有时就会利用无监督学习的方法。\n2.4.2 Baum-Welch 算法\n假设给定训练数据只包含  个长度为  的观测序列  ，目标是学习隐马尔可夫模型  的参数。这时隐马尔可夫模型事实上是一个含有隐变量的概率模型：  它的参数学习可以由EM算法实现。\n（1）确定完全数据的对数似然函数 所有观测数据写成 所有隐数据写成, 完全数据是  完全数据的对数似然函数是  。\n（2） 算法的  步: 求  函数  \n\n其中 \n\n（3） 算法的  步: 极大化  函数  求模型参数 。 由于要极大化的参数在式中单独地出现在 3 个项中, 所以只需对各项分别极大化。\n\n第 1 项\n\n 注意到  满足约束条件 ，利用拉格朗日乘子法，写出拉格朗日函数  对其求偏导数并令结果为 0  得  对  求和得到   即得 \n\n第 2 项\n\n 类似第 1 项, 应用具有约束条件  的拉格朗日乘子法可以求出 \n\n第 3 项\n\n\n同样用拉格朗日乘子法，约束条件是  。只有在  时  对  的偏导数才不为 0 , 以  表示。得  Baum-Weich模型参数估计公式\nBaum-Welch 算法是 EM算法在隐马尔可夫模型学习 中的具体实现。将各概率分别用  表示有：  Baum-Welch 算法\n输入: 观测数据 ; 输出: 隐马尔可夫模型参数。\n（1）初始化。对 , 选取 , 得到模型  。\n（2）递推。对 ,  右端各值按观测  和模型  计算。式中  由式 (10.24) 和式 (10.26) 给出。\n（3）终止。得到模型参数  。\n3. 朴素贝叶斯分类器\n朴素贝叶斯假设各个特征的取值只依赖于类别标签且特征之间是互相独立的，该假设下，联合概率可以分解为：  类别的先验概率可通过统计训练样本中第  类样本占总训练样本的比率来进行估计：  对于各个特征的条件概率，可以通过第  类样本在该特征上的取值进行估计：  当训练样本量较少，或者某些特征取值概率较低时, 可能会出现分子的情况。这时如果将  直接设置为 0 可能并不太合理，通常会采用拉普拉斯平滑。  其中,  为类别数， 为第  维特征的可能取值个数。\n4. 在贝叶斯网络上的条件独立性\n在有向无环图(DAG) 中,任意三个节点之间的路径关系可以概括为下面三种形式。\n4.1 形式 1: 头对头\n\n\n\n这时联合概率满足   取值末知时 和  是互相独立的，也称为头对头条件独立。  如果  的取值已知为 ，则  和  的取值对于  条件不独立。 \n4.2 形式 2: 尾对尾\n\n\n\n这时联合概率满足  如果  的取值已知为 ，则  与  关于  条件独立。  这时  的边缘概率分布与观测  无关，即  若  的取值末知，  和  不独立。 \n4.3 形式 3: 头对尾\n\n\n\n这时联合概率满足  在给定取值  的情况下，随机变量  利  关于  独立  而在  取值未知的情况下，显然  和  不独立。\n4.4 D-分离\n一种判断 DAG 概率图中随机变量间条件独立的图形化方法。一条无向路径  被取值已知的节点集合 D-分离，当且仅当至少满足下面一种情况时成立:\n\n 包念  或者 ，且节点  属于集合 ；\n 包含 , 且节点  属于集合 ；\n 包含 , 且  和  的后继节点不属于集合  。\n\n利用D-分离特性可知，一个节点在给定其父节点的情况下，它与它的非子节点之间是条件独立的。基于 D-分离的概念，在贝叶斯网络上可以定义一个节点或节点集合的马尔可夫覆盖 (Markov Blanket): 对于网络中的一个节点 ，如果存在节点集合使得在条件于该节点集合情况下，  与网络中的其他节点条件独立，则这些集合中的最小集合被定义为  节点的马尔可夫覆盖。对于 DAG中的单个变量节点 ，其马尔可夫覆盖就是由该节点的父节点、子节点以及子节点的父节点组成的集合，用 表示。对于不在  中的任意变量 ，均与  条件于  独立，即：  也就是说，要推断  的概率取值，只需要知道  集合中的节点取值就够了。因此，当研究对象背后存在一系列复杂的概率依赖关系时，只要能把概率依赖关系梳理成适当的贝叶斯网络形式，则可以通过马尔可夫覆盖分解计算在已知某些观测情况下所感兴趣的节点上的后验概率，从而可以应用贝叶斯决策的思想进行类别判别或定量预测。这一过程实际上是一种对事件发生可能性的推理过程，因此，贝叶斯网络也被称作信念网络（belief network），意思是在观测到一定证据的情祝下，对各种相关事件发生的可能性进行推断，类似于对各种事件的信念在网络上传播。\n5. 贝叶斯网络模型的学习\n5.1 贝叶斯网络的参数学习\n当知道模型的网络结构，但不知道具体的概率分布时，可以建模为在给定数据的情况下，求解最大后验概率（maximum a posteriori estimation，MAP）的问题。已知数据集  的情况 下, 模型参数  的后验概率密度表示为  即求  如果模型中不同参数的先验概率相同，则变为参数最大似然值估计 (MLE) 问题。 利用贝叶斯网络的条件独立性，有  其中， 表示与  和  的父节点有关的子数据集， 为模型在这一部分中的参数。通常假设参数的先验分布是互相独立的, 即  则后验概率整体上可以分解为  利用这个性质，可以将各个部分的概率密度进行分解后分别进行计算，分别利用  来求解  。在有的学习任务中，会遇到有一部分数据存在特征缺失，或者网络中存在部分不可观测的节点（隐变量）的情况。对于这种情况，通常使用EM算法进行参数的学习。\n5.2 贝叶斯网络的结构学习\n贝叶斯网络能描述复杂数据内在关系。应用中通常会构造一个对网络模型的评分函数来控制模型复杂度。用  表示某个网络结构  如果对模型没有先验知识，可以假设对于各种可能的网终结构先验概率  都相等, 使问题进一步简化为最大似然问题  加入惩罚函数，结构学习的问题变成了最小化以下目标函数的问题。  基于打分函数，可以根据一定的算法对各种可能的网络结构进行逐一分析，在每种结构下利用最大似然方法来估计参数，并计算打分函数值，最终输出为使得该评分最小的模型及其参数。除了基于打分的方法外，另一类常用的网络学习方法是基于约束条件的结构学习。其思想是通过一系列条件独立的假设检验来逐步构建网络。在网络结构比较稀疏的情况下，这类方法表现出较高的学习效率。在所有可能的网络结构空间搜索最优贝叶斯网络结构被证明是一个NP难的问题。很多情况下需采用一些约束假设和启发式方法来降低运算的复杂程度。\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"三，概率密度函数的估计","url":"/2021/10/03/Pattern%20Recognition/%E4%B8%89%EF%BC%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1/","content":"\n\n\n\n1. 概念\n\n基于样本的两步贝叶斯决策：先通过训练样本估计概率密度函数，再通过统计决策进行类别判定。\n参数估计：最大似然估计；贝叶斯估计。\n非参数估计：直方图法，近邻法，Parzen窗法。\n统计量：针对不同要求构造出样本的某种函数，这种函数在统计学中称为统计量。\n参数空间：总体分布未知参数的全部可容许值组成的集合。\n点估计，估计量，估计值： 点估计构造一个统计量  作为参数  的估计 ，  称为  的估计量。如果  是属于类别  的几个样本观察值，代人统计量  就得到对于第  类的  的具体数值，这个数值称为  的估计值。\n区间估计：要求用区间  作为  可能取值范围的一种估计，这个区间称为置信区间。\n无偏性、有效性：对于多次估计来说，估计量能以较小的方差平均地表示其真实值，并不能保证具体的一次估计的性能。\n一致性：保证当样本数无穷多时，每一次的估计量都将在概率意义上任意地接近其真实值。\n\n2. 最大似然估计\n假设：样本独立同分布采样得到且概率分布函数形式已知。有似然函数\n 最大似然估计量  对数似然函数  当  有梯度算子  对梯度求导并令其等于零  即  方程的解就是对数似然函数的极值点。有时上述方程组会有多个解，其中使得似然函数最大的那个解才是最大似然估计量。此外，并不是所有的概率密度形式都可以用上面的方法求得最大似然估计。\n2.1. 正态分布下的最大似然估计\n单变量正态分布:\n 要估计的参数 ，用于估计的样本  。则有：  由（1）解得： \n\n对于多元正态分布，均值和方差的最大似然估计是：  最大似然估计量是平方误差一致估计量，不一定是无偏估计量。上例中  是无偏的，而  的无偏估计为： \n\n3. 贝叶斯估计与贝叶斯学习\n3.1 贝叶斯估计\n把待估计参数本身也看作随机变量。用  作为估计时总期望风险  对所有的样本求条件风险最小，即  定义损失函数常用平方误差 \n\n采用平方误差损失函数时，贝叶斯估计量  是给定  下  的条件期望（求导得必要条件） \n\n最小平方误差损失函数下，贝叶斯估计步骤 (1) 确定先验分布密度  。 (2) 假设样本是独立同分布的，则有联合分布  (3) 利用贝叶斯公式求  的后验概率分布  (4) 得  的贝叶斯估计量  也可由后验概率分布直接得到样本的概率密度函数  上式就是贝叶斯估计中最核心的公式。如果完全没有先验知识，即认为  为均匀分布，则  完全取决于  。如果先验知识非常强 ，除非  的似然函数为0，否则最后的估计就是，样本不再起作用。\n\n共轭：  为正态分布时，  也为正态分布。\n\n3.2 贝叶斯学习\n贝叶斯学习则把贝叶斯估计的原理用于直接从数据对概率密度函数进行迭代估计。已有贝叶斯估计量\n 其中：  当  时, 有  可得递推公式  先验概率记作 。随着样本数的增加，可以得到一系列对概率密度函数参数的估计  称作递推的贝叶斯估计。如果随着样本数的增加，后验概率序列逐渐尖锐，逐步趋向于以  的真实值为中心的一个尖峰，当样本无穷多时收敛于在参数真实值上的脉冲函数，则这一过程称作贝叶斯学习。此时，估计的样本概率密度函数也逼近真实的密度函数，即： \n3.3 正态分布的贝叶斯估计\n一维正态分布模型，假设均值  是待估计的参数，方差  已知  假设均值  的先验分布是正态分布，其均值为 , 方差为  ，即  对均值  进行估计 \n\n分子部分 \n\n可见  也是一个正态分布 \n\n其中的参数满足  整理后得 \n\n可得贝叶斯估计值  也可由后验分布直接求出样本的密度函数  贝叶斯估计不但使用样本中提供的信息进行估计，还能把待估计参数的先验知识融合进来，并且能够根据数据量大小和先验知识的确定程度来调和两部分信息的相对贡献。\n4. 概率密度估计的非参数方法\n在对样本的分布并没有充分的了解，难以给出密度函数形式的情况下，需要非参数估计。即不对概率密度函数的形式作任何假设，而是直接用样本估计出整个函数。这种估计只能是用数值方法取得，无法得到完美的封闭函数形式。从另外的角度来看，概率密度函数的参数估计实际是在指定的一类函数中选择一个函数作为对末知函数的估计，而非参数估计则可以看作是从所有可能的函数中进行的一种选择。\n4.1 直方图法\n\n把  维向量样本  的每个分量在其取值范围内分成  个等间隔的小窗。则会得到  个体积为  的小舱。\n统计落人每个小舱内的样本数目  。\n把每个小舱内的概率密度看作是常数，并用  作为其估计值， 为样本总数。\n\n\n假定  附近位置上落入小舱的样本个数是 ，当样本趋于无穷多时  收敛于  的条件是 (1)  (2)  (3) \n\n直⽅图⽅法，⼀个明显的问题是估计的概率密度具有不连续性，这种不连续性是因为小窗的边缘造成的。直⽅图⽅法的另⼀个主要的局限性是维度上的问题。\n\n4.2 Kn 近邻法\n根据总样本确定一个参数  。在求  处的密度估计  时，调整小舱体积，直到小舱恰好落入  个样本 \n4.3 Parzen 窗法\n假设  是  维特征向量，每个小舱是一个超立方体，它在每一维的棱长都为  ，则小舱的体积是  定义  维单位方窗函数 若其他 落入以  为中心的超立方体内的样本数为  对于任意一点  的密度估计的表达式 \n定义核函数（窗函数)  它反映了一个观测样本  对在  处的概率密度估计的贡献，与样本  与  的距离有关，可记作  。概率密度估计就是在每一点上把所有观测样本的贡献进行平均，即  估计函数需满足密度函数的要求，显然这只需要核函数本身满足要求即可，即： 且 以上定义的立方体核函数满足这一条件。Parzen 窗估计也可以看作是用核函数对样本在取值空间中进行插值。\n\n多种核函数\n\n方窗\n\n若其他\n​  为超立方体的棱长。\n\n高斯窗（正态窗）\n\n\n​ 即以样本  为均值、协方差矩阵为  的正态分布函数。一维情况下则为 \n\n超球窗\n\n若其他\n​  是超球体的体积,  是超球体半径。\n\n高斯窗示例\n\n这些窗函数都有一个表示窗口宽度的参数(平滑参数）, 反映了一个样本对多大范围内的密度估计产生影响。当被估计的密度函数连续时，在核函数及其参数满足下列条件下，Parzen 窗估计是渐近无偏和平方误差一致的。\n\n对称且满足密度函数条件、有界、核函数取值随着距离的减小而迅速减小\n对应小舱的体积随着样本数的增加而趋于零，但需慢于  趋于零的速度。\n\n\n作为非参数方法的共同问题是对样本数目需求较大，只要样本数目足够大，总可以保证收敛于任何复杂的未知密度，但是计算量和存储量都比较大。正如到⽬前为⽌讨论的那样，K 近邻⽅法和核密度估计⽅法都需要存储整个训练数据。如果数据集很⼤的话，这会造成很⼤的计算代价。当样本数很少时，如果能够对密度函数有先验认识，则参数估计方法能取得更好的估计效果。\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"二，统计决策方法","url":"/2021/10/02/Pattern%20Recognition/%E4%BA%8C%EF%BC%8C%E7%BB%9F%E8%AE%A1%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/%E7%BB%9F%E8%AE%A1%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/","content":"\n感觉还是没啥好说的，书上写的挺详细的。\n\n\n1. 贝叶斯决策\n将类别决策为后验概率大的一类，从而使总体错误率最小。两类问题时  错误率  为所有服从同样分布的独立样本错误率的期望 如果决策如果决策\n正确率 \n2. 最小错误率贝叶斯决策\n2.1 两类最小错误率贝叶斯决策\n则\n引入似然比阀值  则 通常采用负对数形式 则\n总错误率 \n\n2.2 多类最小错误率贝叶斯决策\n则\n正确率  错误率 \n3. 最小风险贝叶斯决策\n考虑各种错误造成不同损失时的一种最优决策。采取决策  的期望损失  对决策规则  最小化期望风险   最小风险贝叶斯决策 则\n\n状态是两类且决策也是两类时 则 引入似然比 ，则\n\n4. Neyman-Pearson决策\n4.1 混淆矩阵\n\n4.2 Neyman-Pearson决策\n 由Lagrange乘子法  设 、 分别是两类的决策区，有  为最小化 ， 应由下式确定  由此可得决策规则： 则\n\n由此可见，决策边界上有  其中  的值由决策边界确定，且这个决策边界满足  当  难求得封闭解时，可用数值方法求得 。即通过  到  的映射关系，由 ， 求得 ，并调节  大小得到合适的错误率。 \n\n\n注意⚠️：书上这部分其实写的有点小问题\n\n4.3 ROC 曲线\n上述三种决策规则的区别只是在于决策阈值的不同，采用不同的阈值，就能达到错误率的不同情况\n\n式 (1) 采用先验概率的比作阈值，达到总的错误率最小，即两类错误率之加权和最小。\n式 (2) 相比式 (1) 阈值中格外考虑了对两类错误率不同的惩罚，实现风险最小。\n式 (3) 通过调整阈值，使一类的错误率为指定数值，而另一类的错误率求最小。\n\n反映随着阈值变化两类错误率变化的ROC曲线\n\n\nROC的绘图方法： 给定 个正例和 个反例，先把分类阈值设为最大，此时真阳性率和假阳性率均为 0，然后调节分类阈值依次将样本划分为正例。设前一个标记点坐标为 ， 划分  个样例后新增  真阳样例  假阳样例，则下个对应标记点的坐标为 ，用线段连接两点，以此递推即可画出ROC曲线，步长  可取值为1。曲线下的面积 AUC 通常用来定量地衡量方法的性能，其可估算为 \n对角线的 AUC 是 ，没有任何分类能力。最理想的情况是 ROC 沿纵轴到  点后再沿水平直线到  点，此时  。用 AUC 可以定量地比较两种不同的方法。从整体上看， AUC 越接近 方法的性能越好。除了可以用来比较不同的分类决策方法，ROC 曲线和 AUC 还可以用来评价和选择与分类有关的特征，即通过设定不同的阈值画出单独用一个特征作为指标划分两类时的 ROC 曲线，计算 AUC并通过比较不同特征间的 AUC 来得知哪个特征包含更多的分类信息。\n\n5. 正态分布下的统计决策\n5.1 多元正态分布\n 边缘分布   协方差矩阵  总是对称非负定阵（现仅考虑为正定阵的情况）。 \n\n期望   方差  \n\n5,2 多元正态分布的性质\n\n参数  和  对分布的决定性\n\n\n\n等密度点的轨迹为一超椭球面 主轴方向由的本征向量所决定，主轴的长度与的本征值成正比。区域中心由  确定，大小由  确定。等密度点轨迹是由 到的 Mahalanobis 距离为常数的超椭球面。其大小是样本对于均值向量的离散度度量。\n\n\n\n不相关性等价于独立性 如果多元正态随机向量的协方差阵是对角阵，则  的分量是相互独立的正态分布随机变量。\n边缘分布和条件分布的正态性 多元正态分布的边缘分布和条件分布仍然是正态分布。\n线性变换的正态性\n\n ​ 由于  为对称阵，则总可以找到非奇异阵  使得各随机变量在新的坐标系中是独立的\n\n线性组合的正态性\n\n\n​ 若  为多元正态随机向量，则线性组合  是一维的正态随机变量\n5.3 正态分布概率模型下的最小错误率贝叶斯决策\n多元正态概率型下，判别函数  决策面方程 \n\n\n一， 时：\n每类的协⽅差矩阵都相等，⽽且类内各特征间相互独⽴，具有相等的⽅差。 (1) 时，从几何上看，各类样本落人以  为中心的同样大小的一些超球体内，决策方程  判别函数为线性函数，决策面是由  所确定的一个超平面(  与  相毗邻) \n\n其中： \n\n\n 时，超平面通过  与  连线中点并与连线正交。先验概率不相等时，决策面与先验概率相等时的决策面平行，只是向先验概率小的方向偏移，即先验概率大的一类要占据更大的决策空间。\n\n\n\n决策面简化为：  此时决策规则为最小距离分类器 则\n\n\n二， 时：\n\n，几何上各类样本集中于以该类均值  点为中心的同样大小和形状的超椭球内。 判别式 \n\n\n其中： \n\n判别函数仍为线性函数，决策面是由  所确定的一个超平面(  与  相毗邻)，即 \n\n其中： \n\n\n，决策规则为计算出  到每类的均值点  的 Mahalanobis 距离平方 ，最后把  归于  最小的类别  此时决策面通过  与  连线的中点 。若先验概率不相等，  则在  与  连线上向先验概率小的均值点偏移。\n\n\n注：判别式的正向方向为指向的方向，判别式大于零则归类到所指向的的类别。\n\n\n三， 时：\n判别式 \n\n其中： 矩阵维列向量\n\n判别函数式将  表示为  的二次型。决策面是由  所确定的一个超二次曲面(  与  相毗邻)，即  决策面随着  的不同而呈现为某种超二次曲面，即超球面、超椭球面、超抛物面、超双曲面或超平面。\n6. 错误率的计算\n最小错误率贝叶斯决策的错误率是  类条件概率密度函数解析表达式较复杂时，计算错误率过于复杂。实际常用以下方法估计错误率 (1) 按理论公式计算； (2) 计算错误率上界； (3) 实验估计。\n6.1 正态分布下错误率的计算\n最小错误率贝叶斯决策规则的负对数似然比形式: \n决策面是  的二次型，当各协方差阵相等时，决策面就变成的线性函数，决策规则简化为：  易知服从一维正态分布，对于 ，可计算出决定一维正态分布的参数均值  及方差   同样对于   \n错误率计算 其中\n6.2 高维独立随机变量错误率估计\n 维随机向量  的分量间相互独立时，有：  负对数似然比  \n\n其中 \n\n由中心极限定理，  较大时无论  密度函数如何，  的密度函数总是趋于正态分布，由此可得  的均值  及方差  ：  由于  和  都是一维随机变量  的函数，在大多数情况下，计算这些参数相对比较容易，即使非正态情况亦是如此，所以可以把  近似看成是服从  的一维正态分布的随机变量，再由 近似算出错误率。\n6.3 离散概率模型下的统计决策\n一阶马尔可夫链：第  时刻上的取值仅依赖于第  时刻的取值  转移概率  对一个长度为  的序列，我们观察到这个序列的概率是 \n 岛记作 “  \"，非  岛一类记作 “” 。判别通常采用对数似然比 \n\n两类的转移概率密度可由下式估计 和\n\n判别阈值选取 阚值的选取可以根据先验概率，也可以根据最小风险的原则确定，或者根据对两类错误率的特殊要求决定。如果两类的先验概率相同且两类错误的损失相同，则对数似然比决策的阈值就是0。在这里，由于概率模型是用数值方法估计的，很难从理论上计算错误率。\n\n在实际应用中，常把训练数据代到中，统计所有训练样本的似然比取值的分布。选用不同的阈值来做决策就会导致不同的错误情况，可从直方图上确定满意的阈值，或者通过变动不同阈值画出ROC曲线来决定阈值选择。\n6.4 隐马尔可夫模型\n\n对前  个位置，  表示当第  位置对应隐状态为  时能得到的最大概率。  写成递归形式:  通过回溯求得最大似然路径 ，即  这种做法叫Viterbi算法\n\nHMM详见：待更新\nGibbs采样详见：待更新\n\n","categories":["模式识别"],"tags":["模式识别"]},{"title":"一，概论","url":"/2021/10/01/Pattern%20Recognition/%E4%B8%80%EF%BC%8C%E6%A6%82%E8%AE%BA/%E6%A6%82%E8%AE%BA/","content":"\n这章没啥好说的，写出来完全是为了笔记的完整性。\n\n\n1. 模式与模式识别\n通过以往对特定事物的认知来识别目标中的特定事物，例如从心电图中各波的形状判断病人的健康情况。\n2. 模式识别的主要方法\n\n基于知识的方法：根据样本特征与类别间关系的认知建立推理系统，对未知样本类别决策。\n基于数据的方法：依据训练样本建立不完全确定内部机理的表示与关系的系统，对未知样本类别决策。\n\n3. 监督模式识别与非监督模式识别\n\n监督模式识别：利用了有标签的训练样本。\n非监督模式识别：没有利用有标签的训练样本，对训练样本采用不同的划分方法可能导致不同的结果。\n\n4. 模式识别系统举例\n\n语音识别：对一系列连续的音素进行分类，需考虑音素之间的相互影响。例如利用多阶隐马尔可夫模型。\n说人话识别：与语言识别基本原理相同，只是分类目标由语音变成了说话人。\n字符与文字识别：OCR（detection-classification）等。单字识别是OCR的基础，将图片向多方向投影得到像素密度即数量特征；根据对汉字结构的认知提取有效特征点并编码成数字特征。特征提取后每个字就是一个特征向量代表的样本，接下来涉及到多分类问题。分类器设定通常需要结合对文字结构的认知（旋转和尺度不变性）。\n复杂图像中特定目标的识别：目标检测方法判断每个子图像是汽车还是背景，检测出汽车后可追踪其在连续图像中的运动轨迹来识别是否有违章行为等；可根据汽车图像识别出车牌位置再利用数字识别识别车牌号等；路人检测再进行人脸识别，行为识别等。\n根据地震勘探数据对地下储层性质的识别：在探井处利用地震信号提取特征并结合地下储层性质类别建立分类器；探井数不足以用来训练时可利用非监督学习方法对地震勘测信号聚类划分，由地质学家分析划分来实现对储层性质的识别。\n利用基因表达数据进行癌症的分类：利用基因表达作为病例特征研究病例之间的分类和聚类；利用病例表达作为基因特征研究基因之间的分类和聚类等。\n\n5. 模式识别系统的典型构成\n\n处理监督模式识别的一般步骤 分析问题，原始特征提取，特征提取与选择，分类器设计，分类决策；\n处理非监督模式识别的一般步骤 分析问题，原始特征提取，特征提取与选择，聚类分析，结果解释；\n\n","categories":["模式识别"],"tags":["模式识别"]}]